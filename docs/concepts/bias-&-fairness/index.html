<!doctype html>
<html class="docs-version-current" lang="en" dir="ltr">
<head>
<meta charset="UTF-8">
<meta name="viewport" content="width=device-width,initial-scale=1">
<meta name="generator" content="Docusaurus v2.0.0-beta.17">
<link rel="alternate" type="application/rss+xml" href="/ai-kb/blog/rss.xml" title="AIKB RSS Feed">
<link rel="alternate" type="application/atom+xml" href="/ai-kb/blog/atom.xml" title="AIKB Atom Feed">
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.13.24/dist/katex.min.css" integrity="sha384-odtC+0UGzzFL/6PNoE8rX/SPcQDXBJ+uRepguP4QkPCm2LBxH3FA3y+fKSiJ+AmM" crossorigin="anonymous"><title data-rh="true">Bias &amp; Fairness | AIKB</title><meta data-rh="true" name="twitter:card" content="summary_large_image"><meta data-rh="true" property="og:url" content="https://kb.recohut.com/ai-kb/docs/concepts/bias-&amp;-fairness"><meta data-rh="true" name="docusaurus_locale" content="en"><meta data-rh="true" name="docusaurus_version" content="current"><meta data-rh="true" name="docusaurus_tag" content="docs-default-current"><meta data-rh="true" name="docsearch:language" content="en"><meta data-rh="true" name="docsearch:version" content="current"><meta data-rh="true" name="docsearch:docusaurus_tag" content="docs-default-current"><meta data-rh="true" property="og:title" content="Bias &amp; Fairness | AIKB"><meta data-rh="true" name="description" content="It can’t be denied that there is bias all around us. A bias is a prejudice against a person or group of people, including, but not limited to their gender, race, and beliefs. Many of these biases arise from emergent behavior in social interactions, events in history, and cultural and political views around the world. These biases affect the data that we collect. Because AI algorithms work with this data, it is an inherent problem that the machine will “learn” these biases. From a technical perspective, we can engineer the system perfectly, but at the end of the day, humans interact with these systems, and it’s our responsibility to minimize bias and prejudice as much as possible. The algorithms we use are only as good as the data provided to them. Understanding the data and the context in which it is being used is the first step in battling bias, and this understanding will help you build better solutions—because you will be well versed in the problem space. Providing balanced data with as little bias as possible should result in better solutions."><meta data-rh="true" property="og:description" content="It can’t be denied that there is bias all around us. A bias is a prejudice against a person or group of people, including, but not limited to their gender, race, and beliefs. Many of these biases arise from emergent behavior in social interactions, events in history, and cultural and political views around the world. These biases affect the data that we collect. Because AI algorithms work with this data, it is an inherent problem that the machine will “learn” these biases. From a technical perspective, we can engineer the system perfectly, but at the end of the day, humans interact with these systems, and it’s our responsibility to minimize bias and prejudice as much as possible. The algorithms we use are only as good as the data provided to them. Understanding the data and the context in which it is being used is the first step in battling bias, and this understanding will help you build better solutions—because you will be well versed in the problem space. Providing balanced data with as little bias as possible should result in better solutions."><link data-rh="true" rel="icon" href="/ai-kb/img/logo.svg"><link data-rh="true" rel="canonical" href="https://kb.recohut.com/ai-kb/docs/concepts/bias-&amp;-fairness"><link data-rh="true" rel="alternate" href="https://kb.recohut.com/ai-kb/docs/concepts/bias-&amp;-fairness" hreflang="en"><link data-rh="true" rel="alternate" href="https://kb.recohut.com/ai-kb/docs/concepts/bias-&amp;-fairness" hreflang="x-default"><link rel="stylesheet" href="/ai-kb/assets/css/styles.af9e0313.css">
<link rel="preload" href="/ai-kb/assets/js/runtime~main.65743e6b.js" as="script">
<link rel="preload" href="/ai-kb/assets/js/main.ed9e086a.js" as="script">
</head>
<body class="navigation-with-keyboard" data-theme="light">
<script>!function(){function t(t){document.documentElement.setAttribute("data-theme",t)}var e=function(){var t=null;try{t=localStorage.getItem("theme")}catch(t){}return t}();t(null!==e?e:"light")}()</script><div id="__docusaurus">
<div role="region"><a href="#" class="skipToContent_ZgBM">Skip to main content</a></div><nav class="navbar navbar--fixed-top"><div class="navbar__inner"><div class="navbar__items"><button aria-label="Navigation bar toggle" class="navbar__toggle clean-btn" type="button" tabindex="0"><svg width="30" height="30" viewBox="0 0 30 30" aria-hidden="true"><path stroke="currentColor" stroke-linecap="round" stroke-miterlimit="10" stroke-width="2" d="M4 7h22M4 15h22M4 23h22"></path></svg></button><a class="navbar__brand" href="/ai-kb/"><div class="navbar__logo"><img src="/ai-kb/img/logo.svg" alt="aikb Logo" class="themedImage_W2Cr themedImage--light_TfLj"><img src="/ai-kb/img/logo.svg" alt="aikb Logo" class="themedImage_W2Cr themedImage--dark_oUvU"></div><b class="navbar__title">AIKB</b></a><a class="navbar__item navbar__link navbar__link--active" href="/ai-kb/docs/intro">Docs</a></div><div class="navbar__items navbar__items--right"><a href="https://github.com/sparsh-ai/ai-kb" target="_blank" rel="noopener noreferrer" class="navbar__item navbar__link"><span>GitHub<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_I5OW"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></span></a><div class="toggle_S7eR toggle_TdHA toggleDisabled_f9M3"><div class="toggleButton_rCf9" role="button" tabindex="-1"><svg viewBox="0 0 24 24" width="24" height="24" class="lightToggleIcon_v35p"><path fill="currentColor" d="M12,9c1.65,0,3,1.35,3,3s-1.35,3-3,3s-3-1.35-3-3S10.35,9,12,9 M12,7c-2.76,0-5,2.24-5,5s2.24,5,5,5s5-2.24,5-5 S14.76,7,12,7L12,7z M2,13l2,0c0.55,0,1-0.45,1-1s-0.45-1-1-1l-2,0c-0.55,0-1,0.45-1,1S1.45,13,2,13z M20,13l2,0c0.55,0,1-0.45,1-1 s-0.45-1-1-1l-2,0c-0.55,0-1,0.45-1,1S19.45,13,20,13z M11,2v2c0,0.55,0.45,1,1,1s1-0.45,1-1V2c0-0.55-0.45-1-1-1S11,1.45,11,2z M11,20v2c0,0.55,0.45,1,1,1s1-0.45,1-1v-2c0-0.55-0.45-1-1-1C11.45,19,11,19.45,11,20z M5.99,4.58c-0.39-0.39-1.03-0.39-1.41,0 c-0.39,0.39-0.39,1.03,0,1.41l1.06,1.06c0.39,0.39,1.03,0.39,1.41,0s0.39-1.03,0-1.41L5.99,4.58z M18.36,16.95 c-0.39-0.39-1.03-0.39-1.41,0c-0.39,0.39-0.39,1.03,0,1.41l1.06,1.06c0.39,0.39,1.03,0.39,1.41,0c0.39-0.39,0.39-1.03,0-1.41 L18.36,16.95z M19.42,5.99c0.39-0.39,0.39-1.03,0-1.41c-0.39-0.39-1.03-0.39-1.41,0l-1.06,1.06c-0.39,0.39-0.39,1.03,0,1.41 s1.03,0.39,1.41,0L19.42,5.99z M7.05,18.36c0.39-0.39,0.39-1.03,0-1.41c-0.39-0.39-1.03-0.39-1.41,0l-1.06,1.06 c-0.39,0.39-0.39,1.03,0,1.41s1.03,0.39,1.41,0L7.05,18.36z"></path></svg><svg viewBox="0 0 24 24" width="24" height="24" class="darkToggleIcon_nQuB"><path fill="currentColor" d="M9.37,5.51C9.19,6.15,9.1,6.82,9.1,7.5c0,4.08,3.32,7.4,7.4,7.4c0.68,0,1.35-0.09,1.99-0.27C17.45,17.19,14.93,19,12,19 c-3.86,0-7-3.14-7-7C5,9.07,6.81,6.55,9.37,5.51z M12,3c-4.97,0-9,4.03-9,9s4.03,9,9,9s9-4.03,9-9c0-0.46-0.04-0.92-0.1-1.36 c-0.98,1.37-2.58,2.26-4.4,2.26c-2.98,0-5.4-2.42-5.4-5.4c0-1.81,0.89-3.42,2.26-4.4C12.92,3.04,12.46,3,12,3L12,3z"></path></svg></div><input type="checkbox" class="toggleScreenReader_g2nN" aria-label="Switch between dark and light mode (currently light mode)"></div><div class="dsla-search-wrapper"><div class="dsla-search-field" data-tags="default,docs-default-current"></div></div></div></div><div role="presentation" class="navbar-sidebar__backdrop"></div></nav><div class="main-wrapper docs-wrapper docs-doc-page"><div class="docPage_P2Lg"><button aria-label="Scroll back to top" class="clean-btn theme-back-to-top-button backToTopButton_RiI4" type="button"></button><aside class="theme-doc-sidebar-container docSidebarContainer_rKC_"><div class="sidebar_CW9Y"><nav class="menu thin-scrollbar menu_SkdO"><ul class="theme-doc-sidebar-menu menu__list"><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/ai-kb/docs/intro">Introduction</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/ai-kb/docs/projects/">Projects</a></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--active" href="/ai-kb/docs/concepts/">Concepts</a><button aria-label="Toggle the collapsible sidebar category &#x27;Concepts&#x27;" type="button" class="clean-btn menu__caret"></button></div><ul style="display:block;overflow:visible;height:auto" class="menu__list"><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/ai-kb/docs/concepts/amazon-personalize">Amazon Personalize</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/ai-kb/docs/concepts/apps">Apps</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link menu__link--active" aria-current="page" tabindex="0" href="/ai-kb/docs/concepts/bias-&amp;-fairness">Bias &amp; Fairness</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/ai-kb/docs/concepts/causal-inference">Causal Inference</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/ai-kb/docs/concepts/cross-domain">Cross-domain</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/ai-kb/docs/concepts/data-science">Data Science</a></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link" tabindex="0" href="/ai-kb/docs/concepts/frameworks/">Frameworks</a><button aria-label="Toggle the collapsible sidebar category &#x27;Frameworks&#x27;" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/ai-kb/docs/concepts/graph-embeddings">Graph Embeddings</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/ai-kb/docs/concepts/graph-networks">Graph Networks</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/ai-kb/docs/concepts/incremental-learning">Incremental Learning</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/ai-kb/docs/concepts/ipython">IPython</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/ai-kb/docs/concepts/meta-learning">Meta Learning</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/ai-kb/docs/concepts/mlops">MLOps</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/ai-kb/docs/concepts/model-deployment">Model Deployment</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/ai-kb/docs/concepts/model-retraining">Model Retraining</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/ai-kb/docs/concepts/models">Models</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/ai-kb/docs/concepts/multi-objective-optimization">Multi-Objective Optimization</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/ai-kb/docs/concepts/multi-task-learning">Multi-Task Learning</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/ai-kb/docs/concepts/multitask-learning">Multi-task Learning</a></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist" tabindex="0" href="/ai-kb/docs/concepts/nlp/chatbot">NLP</a></div></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/ai-kb/docs/concepts/offline-learning">Off-Policy Learning</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/ai-kb/docs/concepts/scalarization">Scalarization</a></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist" tabindex="0" href="/ai-kb/docs/concepts/vision/facial-analytics">Computer Vision</a></div></li></ul></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link" href="/ai-kb/docs/tutorials/">Tutorials</a><button aria-label="Toggle the collapsible sidebar category &#x27;Tutorials&#x27;" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link" href="/ai-kb/docs/tools/">Tools</a><button aria-label="Toggle the collapsible sidebar category &#x27;Tools&#x27;" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/ai-kb/docs/notebooks/">Notebooks</a></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link" href="/ai-kb/docs/best-practices/">Best Practices</a><button aria-label="Toggle the collapsible sidebar category &#x27;Best Practices&#x27;" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link" href="/ai-kb/docs/training-courses/">Training Courses</a><button aria-label="Toggle the collapsible sidebar category &#x27;Training Courses&#x27;" type="button" class="clean-btn menu__caret"></button></div></li></ul></nav></div></aside><main class="docMainContainer_TCnq"><div class="container padding-top--md padding-bottom--lg"><div class="row"><div class="col docItemCol_DM6M"><div class="docItemContainer_vinB"><article><nav class="theme-doc-breadcrumbs breadcrumbsContainer_Xlws" aria-label="breadcrumbs"><ul class="breadcrumbs"><li class="breadcrumbs__item"><a class="breadcrumbs__link breadcrumbsItemLink_e5ie" href="/ai-kb/">🏠</a></li><li class="breadcrumbs__item"><a class="breadcrumbs__link breadcrumbsItemLink_e5ie" href="/ai-kb/docs/concepts/">Concepts</a></li><li class="breadcrumbs__item breadcrumbs__item--active"><a class="breadcrumbs__link breadcrumbsItemLink_e5ie" href="/ai-kb/docs/concepts/bias-&amp;-fairness">Bias &amp; Fairness</a></li></ul></nav><div class="tocCollapsible_jdIR theme-doc-toc-mobile tocMobile_TmEX"><button type="button" class="clean-btn tocCollapsibleButton_Fzxq">On this page</button></div><div class="theme-doc-markdown markdown"><h1>Bias &amp; Fairness</h1><p>It can’t be denied that there is bias all around us. A bias is a prejudice against a person or group of people, including, but not limited to their gender, race, and beliefs. Many of these biases arise from emergent behavior in social interactions, events in history, and cultural and political views around the world. These biases affect the data that we collect. Because AI algorithms work with this data, it is an inherent problem that the machine will “learn” these biases. From a technical perspective, we can engineer the system perfectly, but at the end of the day, humans interact with these systems, and it’s our responsibility to minimize bias and prejudice as much as possible. The algorithms we use are only as good as the data provided to them. Understanding the data and the context in which it is being used is the first step in battling bias, and this understanding will help you build better solutions—because you will be well versed in the problem space. Providing balanced data with as little bias as possible should result in better solutions.</p><p>Recommender systems are important for connecting users to the right items. But are items recommended fairly? For example, in a recruiting recommender that recommends job candidates (the items here), are candidates of different genders treated equally? In a news recommender, are news stories with different political ideologies recommended fairly? And even for product recommenders, are products from big companies favored over products from new entrants? The danger of unfair recommendations for items has been recognized in the literature, with potential negative impacts on item providers, user satisfaction, the recommendation platform itself, and ultimately social good.</p><p>In practice, the data is observational rather than experimental, and is often affected by many factors, including but not limited to self-selection of the user (selection bias), exposure mechanism of the system (exposure bias), public opinions (conformity bias) and the display position (position bias). De-biasing the data is an important and critical pre-processing step. Biases cause training data distribution deviate from the ideal unbiased one.</p><p><img loading="lazy" alt="Untitled" src="/ai-kb/assets/images/content-concepts-raw-bias-&amp;-fairness-untitled-64a535748137172862ffe40cdbfeede7.png" width="555" height="418"></p><h2 class="anchor anchorWithStickyNavbar_mojV" id="types-of-biases">Types of biases<a class="hash-link" href="#types-of-biases" title="Direct link to heading">​</a></h2><p><img loading="lazy" alt="Untitled" src="/ai-kb/assets/images/content-concepts-raw-bias-&amp;-fairness-untitled-1-c0494b16fb2ef50b14cce3f94a2991c1.png" width="1290" height="376"></p><h3 class="anchor anchorWithStickyNavbar_mojV" id="selection-bias">Selection bias<a class="hash-link" href="#selection-bias" title="Direct link to heading">​</a></h3><p>Selection bias originates from users’ numerical ratings on items (i.e., explicit feedback), which is defined as - <em>&quot;Selection Bias happens as users are free to choose which items to rate, so that the observed ratings are not a representative sample of all ratings. In other words, the rating data is often missing not at random (MNAR).&quot;</em></p><p><img loading="lazy" alt="Distribution of rating values for randomly selected items and user-selected items, as demonstrated in [this](https://www.notion.so/04c70cf18dbe401980fe9b00bb1a2077) paper." src="/ai-kb/assets/images/content-concepts-raw-bias-&amp;-fairness-untitled-2-783d9b3c06b9edb7a88c0c717f6e8d17.png" width="752" height="402"></p><p>Distribution of rating values for randomly selected items and user-selected items, as demonstrated in <a href="https://www.notion.so/04c70cf18dbe401980fe9b00bb1a2077" target="_blank" rel="noopener noreferrer">this</a> paper.</p><h3 class="anchor anchorWithStickyNavbar_mojV" id="conformity-bias">Conformity bias<a class="hash-link" href="#conformity-bias" title="Direct link to heading">​</a></h3><p>Another bias inherent in the explicit feedback data is conformity bias, which is defined as: <em>&quot;Conformity bias happens as users tend to rate similarly to the others in a group, even if doing so goes against their own judgment, making the rating values do not always signify user true preference&quot;</em>.</p><p>For example, influenced by high ratings of public comments on an item, one user is highly likely to change her low rate, avoiding being too harsh. Such phenomenon of conformity is common and cause biases in user ratings. As shown in <a href="https://www.notion.so/A-methodology-for-learning-analyzing-and-mitigating-social-influence-bias-in-recommender-systems-e304120c16f1415583e396f786cac335" target="_blank" rel="noopener noreferrer">Krishnan et al.</a>, user ratings follow different distributions when users rate items before or after being exposed to the public opinions. Moreover, conformity bias might be caused by social influence, where users tend to behave similarly with their friends. Hence, the observed ratings are skewed and might not reflect users’ real preference on items.</p><h2 class="anchor anchorWithStickyNavbar_mojV" id="debiasing-methods">Debiasing methods<a class="hash-link" href="#debiasing-methods" title="Direct link to heading">​</a></h2><h3 class="anchor anchorWithStickyNavbar_mojV" id="ips">IPS<a class="hash-link" href="#ips" title="Direct link to heading">​</a></h3><p>IPS eliminates popularity bias by re-weighting each instance according to item popularity. Specifically, weight for an instance is set as the inverse of corresponding item popularity value, hence popular items are imposed lower weights, while the importance for long-tail items are boosted.</p><h3 class="anchor anchorWithStickyNavbar_mojV" id="ips-c">IPS-C<a class="hash-link" href="#ips-c" title="Direct link to heading">​</a></h3><p>This method adds max-capping on IPS value to reduce the variance of IPS.</p><h3 class="anchor anchorWithStickyNavbar_mojV" id="ips-cn">IPS-CN<a class="hash-link" href="#ips-cn" title="Direct link to heading">​</a></h3><p>This method further adds normalization which also achieved lower variance than plain IPS, at the expense of introducing a small amount of bias.</p><h3 class="anchor anchorWithStickyNavbar_mojV" id="ips-cnsr">IPS-CNSR<a class="hash-link" href="#ips-cnsr" title="Direct link to heading">​</a></h3><p>Smoothing and re-normalization are added to attain more stable output of IPS.</p><h3 class="anchor anchorWithStickyNavbar_mojV" id="cause">CausE<a class="hash-link" href="#cause" title="Direct link to heading">​</a></h3><p>This method requires a large biased dataset and a small unbiased dataset. Each user or item has two embeddings to perform matrix factorization (MF) on the two datasets respectively, and L1 or L2 regularization is exploited.</p><h2 class="anchor anchorWithStickyNavbar_mojV" id="random">Random<a class="hash-link" href="#random" title="Direct link to heading">​</a></h2><p><img loading="lazy" alt="Aim for ethical and legal applications of technology" src="/ai-kb/assets/images/content-concepts-raw-bias-&amp;-fairness-untitled-3-b27bd256c9ce69e4b2b68cb103f314bc.png" width="695" height="504"></p><p>Aim for ethical and legal applications of technology</p><h2 class="anchor anchorWithStickyNavbar_mojV" id="concepts">Concepts<a class="hash-link" href="#concepts" title="Direct link to heading">​</a></h2><h3 class="anchor anchorWithStickyNavbar_mojV" id="equal-opportunity"><a href="https://arxiv.org/abs/1610.02413" target="_blank" rel="noopener noreferrer">Equal opportunity</a><a class="hash-link" href="#equal-opportunity" title="Direct link to heading">​</a></h3><p>In a classification task, equal opportunity requires a model to produce the same true positive rate (TPR) for all individuals or groups. The goal is to ensure that items from different groups can be equally recommended to matched users during testing (the same true positive rate): for example, candidates of different genders are equally recommended to job openings that they are qualified for. In contrast, demographic parity fairness only focuses on the difference in the amount of exposure to users without considering the ground-truth of user-item matching. However, because only the exposure to matched users (as considered by equal opportunity fairness) can influence the feedback or economic gain of items, in recommendation tasks, equal opportunity is better aligned than demographic parity fairness.</p><h3 class="anchor anchorWithStickyNavbar_mojV" id="rawlsian-max-min-fairness-principle-of-distributive-justice">Rawlsian Max-Min fairness principle of distributive justice<a class="hash-link" href="#rawlsian-max-min-fairness-principle-of-distributive-justice" title="Direct link to heading">​</a></h3><p>Rawlsian Max-Min fairness requires a model to maximize the minimum utility of individuals or groups so that no subject is underserved by the model. Unlike equality (or parity) based notions of fairness aiming to eliminate difference among individuals or groups but neglecting a decrease of utility for betterserved subjects, Rawlsian Max-Min fairness accepts inequalities and thus does not requires decreasing utility of better-served subjects. So, Rawlsian Max-Min fairness is preferred in applications where perfect equality is not necessary, such as recommendation tasks, and it can also better preserve the overall model utility.</p><h2 class="anchor anchorWithStickyNavbar_mojV" id="autodebias">AutoDebias<a class="hash-link" href="#autodebias" title="Direct link to heading">​</a></h2><p>AutoDebias is an automatic debiasing method for recommendation system based on meta learning, exploiting a small amount of uniform data to learn de-biasing parameters and using these parameters to guide the learning of the recommendation model.</p><p><img loading="lazy" alt="The working flow of AutoDebias, consists of three steps: (1) tentatively updating 𝜃 to 𝜃 ′ on the training data 𝐷𝑇 with current 𝜙 (black arrows); (2) updating 𝜙 based on 𝜃 ′ on the uniform data (blue arrows); (3) actually updating 𝜃 with the updated 𝜙 (black arrows)." src="/ai-kb/assets/images/content-concepts-raw-bias-&amp;-fairness-untitled-4-1de344d9d5f6cc9b44f273e17b564c6a.png" width="720" height="441"></p><p>The working flow of AutoDebias, consists of three steps: (1) tentatively updating 𝜃 to 𝜃 ′ on the training data 𝐷𝑇 with current 𝜙 (black arrows); (2) updating 𝜙 based on 𝜃 ′ on the uniform data (blue arrows); (3) actually updating 𝜃 with the updated 𝜙 (black arrows).</p></div><footer class="theme-doc-footer docusaurus-mt-lg"><div class="theme-doc-footer-edit-meta-row row"><div class="col"><a href="https://github.com/sparsh-ai/ai-kb/docs/03-concepts/bias-&amp;-fairness.mdx" target="_blank" rel="noreferrer noopener" class="theme-edit-this-page"><svg fill="currentColor" height="20" width="20" viewBox="0 0 40 40" class="iconEdit_dcUD" aria-hidden="true"><g><path d="m34.5 11.7l-3 3.1-6.3-6.3 3.1-3q0.5-0.5 1.2-0.5t1.1 0.5l3.9 3.9q0.5 0.4 0.5 1.1t-0.5 1.2z m-29.5 17.1l18.4-18.5 6.3 6.3-18.4 18.4h-6.3v-6.2z"></path></g></svg>Edit this page</a></div><div class="col lastUpdated_foO9"></div></div></footer></article><nav class="pagination-nav docusaurus-mt-lg" aria-label="Docs pages navigation"><div class="pagination-nav__item"><a class="pagination-nav__link" href="/ai-kb/docs/concepts/apps"><div class="pagination-nav__sublabel">Previous</div><div class="pagination-nav__label">Apps</div></a></div><div class="pagination-nav__item pagination-nav__item--next"><a class="pagination-nav__link" href="/ai-kb/docs/concepts/causal-inference"><div class="pagination-nav__sublabel">Next</div><div class="pagination-nav__label">Causal Inference</div></a></div></nav></div></div><div class="col col--3"><div class="tableOfContents_cNA8 thin-scrollbar theme-doc-toc-desktop"><ul class="table-of-contents table-of-contents__left-border"><li><a href="#types-of-biases" class="table-of-contents__link toc-highlight">Types of biases</a><ul><li><a href="#selection-bias" class="table-of-contents__link toc-highlight">Selection bias</a></li><li><a href="#conformity-bias" class="table-of-contents__link toc-highlight">Conformity bias</a></li></ul></li><li><a href="#debiasing-methods" class="table-of-contents__link toc-highlight">Debiasing methods</a><ul><li><a href="#ips" class="table-of-contents__link toc-highlight">IPS</a></li><li><a href="#ips-c" class="table-of-contents__link toc-highlight">IPS-C</a></li><li><a href="#ips-cn" class="table-of-contents__link toc-highlight">IPS-CN</a></li><li><a href="#ips-cnsr" class="table-of-contents__link toc-highlight">IPS-CNSR</a></li><li><a href="#cause" class="table-of-contents__link toc-highlight">CausE</a></li></ul></li><li><a href="#random" class="table-of-contents__link toc-highlight">Random</a></li><li><a href="#concepts" class="table-of-contents__link toc-highlight">Concepts</a><ul><li><a href="#equal-opportunity" class="table-of-contents__link toc-highlight">Equal opportunity</a></li><li><a href="#rawlsian-max-min-fairness-principle-of-distributive-justice" class="table-of-contents__link toc-highlight">Rawlsian Max-Min fairness principle of distributive justice</a></li></ul></li><li><a href="#autodebias" class="table-of-contents__link toc-highlight">AutoDebias</a></li></ul></div></div></div></div></main></div></div><footer class="footer footer--dark"><div class="container container-fluid"><div class="footer__bottom text--center"><div class="footer__copyright">Copyright © 2022 AIKB Docs, Inc. Built with Docusaurus.</div></div></div></footer></div>
<script src="/ai-kb/assets/js/runtime~main.65743e6b.js"></script>
<script src="/ai-kb/assets/js/main.ed9e086a.js"></script>
</body>
</html>