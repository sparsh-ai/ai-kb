<!doctype html>
<html lang="en" dir="ltr">
<head>
<meta charset="UTF-8">
<meta name="viewport" content="width=device-width,initial-scale=1">
<meta name="generator" content="Docusaurus v2.0.0-beta.17">
<link rel="alternate" type="application/rss+xml" href="/ai-kb/blog/rss.xml" title="AIKB RSS Feed">
<link rel="alternate" type="application/atom+xml" href="/ai-kb/blog/atom.xml" title="AIKB Atom Feed">
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.13.24/dist/katex.min.css" integrity="sha384-odtC+0UGzzFL/6PNoE8rX/SPcQDXBJ+uRepguP4QkPCm2LBxH3FA3y+fKSiJ+AmM" crossorigin="anonymous"><title data-rh="true">2 posts tagged with &quot;flask&quot; | AIKB</title><meta data-rh="true" property="og:title" content="2 posts tagged with &quot;flask&quot; | AIKB"><meta data-rh="true" name="twitter:card" content="summary_large_image"><meta data-rh="true" property="og:url" content="https://kb.recohut.com/ai-kb/blog/tags/flask"><meta data-rh="true" name="docusaurus_locale" content="en"><meta data-rh="true" name="docusaurus_tag" content="blog_tags_posts"><meta data-rh="true" name="docsearch:language" content="en"><meta data-rh="true" name="docsearch:docusaurus_tag" content="blog_tags_posts"><link data-rh="true" rel="icon" href="/ai-kb/img/logo.svg"><link data-rh="true" rel="canonical" href="https://kb.recohut.com/ai-kb/blog/tags/flask"><link data-rh="true" rel="alternate" href="https://kb.recohut.com/ai-kb/blog/tags/flask" hreflang="en"><link data-rh="true" rel="alternate" href="https://kb.recohut.com/ai-kb/blog/tags/flask" hreflang="x-default"><link rel="stylesheet" href="/ai-kb/assets/css/styles.af9e0313.css">
<link rel="preload" href="/ai-kb/assets/js/runtime~main.4448d8b2.js" as="script">
<link rel="preload" href="/ai-kb/assets/js/main.b26486b4.js" as="script">
</head>
<body class="navigation-with-keyboard" data-theme="light">
<script>!function(){function t(t){document.documentElement.setAttribute("data-theme",t)}var e=function(){var t=null;try{t=localStorage.getItem("theme")}catch(t){}return t}();t(null!==e?e:"light")}()</script><div id="__docusaurus">
<div role="region"><a href="#" class="skipToContent_ZgBM">Skip to main content</a></div><nav class="navbar navbar--fixed-top"><div class="navbar__inner"><div class="navbar__items"><button aria-label="Navigation bar toggle" class="navbar__toggle clean-btn" type="button" tabindex="0"><svg width="30" height="30" viewBox="0 0 30 30" aria-hidden="true"><path stroke="currentColor" stroke-linecap="round" stroke-miterlimit="10" stroke-width="2" d="M4 7h22M4 15h22M4 23h22"></path></svg></button><a class="navbar__brand" href="/ai-kb/"><div class="navbar__logo"><img src="/ai-kb/img/logo.svg" alt="aikb Logo" class="themedImage_W2Cr themedImage--light_TfLj"><img src="/ai-kb/img/logo.svg" alt="aikb Logo" class="themedImage_W2Cr themedImage--dark_oUvU"></div><b class="navbar__title">AIKB</b></a><a class="navbar__item navbar__link" href="/ai-kb/docs/intro">Docs</a></div><div class="navbar__items navbar__items--right"><a href="https://github.com/sparsh-ai/ai-kb" target="_blank" rel="noopener noreferrer" class="navbar__item navbar__link"><span>GitHub<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_I5OW"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></span></a><div class="toggle_S7eR toggle_TdHA toggleDisabled_f9M3"><div class="toggleButton_rCf9" role="button" tabindex="-1"><svg viewBox="0 0 24 24" width="24" height="24" class="lightToggleIcon_v35p"><path fill="currentColor" d="M12,9c1.65,0,3,1.35,3,3s-1.35,3-3,3s-3-1.35-3-3S10.35,9,12,9 M12,7c-2.76,0-5,2.24-5,5s2.24,5,5,5s5-2.24,5-5 S14.76,7,12,7L12,7z M2,13l2,0c0.55,0,1-0.45,1-1s-0.45-1-1-1l-2,0c-0.55,0-1,0.45-1,1S1.45,13,2,13z M20,13l2,0c0.55,0,1-0.45,1-1 s-0.45-1-1-1l-2,0c-0.55,0-1,0.45-1,1S19.45,13,20,13z M11,2v2c0,0.55,0.45,1,1,1s1-0.45,1-1V2c0-0.55-0.45-1-1-1S11,1.45,11,2z M11,20v2c0,0.55,0.45,1,1,1s1-0.45,1-1v-2c0-0.55-0.45-1-1-1C11.45,19,11,19.45,11,20z M5.99,4.58c-0.39-0.39-1.03-0.39-1.41,0 c-0.39,0.39-0.39,1.03,0,1.41l1.06,1.06c0.39,0.39,1.03,0.39,1.41,0s0.39-1.03,0-1.41L5.99,4.58z M18.36,16.95 c-0.39-0.39-1.03-0.39-1.41,0c-0.39,0.39-0.39,1.03,0,1.41l1.06,1.06c0.39,0.39,1.03,0.39,1.41,0c0.39-0.39,0.39-1.03,0-1.41 L18.36,16.95z M19.42,5.99c0.39-0.39,0.39-1.03,0-1.41c-0.39-0.39-1.03-0.39-1.41,0l-1.06,1.06c-0.39,0.39-0.39,1.03,0,1.41 s1.03,0.39,1.41,0L19.42,5.99z M7.05,18.36c0.39-0.39,0.39-1.03,0-1.41c-0.39-0.39-1.03-0.39-1.41,0l-1.06,1.06 c-0.39,0.39-0.39,1.03,0,1.41s1.03,0.39,1.41,0L7.05,18.36z"></path></svg><svg viewBox="0 0 24 24" width="24" height="24" class="darkToggleIcon_nQuB"><path fill="currentColor" d="M9.37,5.51C9.19,6.15,9.1,6.82,9.1,7.5c0,4.08,3.32,7.4,7.4,7.4c0.68,0,1.35-0.09,1.99-0.27C17.45,17.19,14.93,19,12,19 c-3.86,0-7-3.14-7-7C5,9.07,6.81,6.55,9.37,5.51z M12,3c-4.97,0-9,4.03-9,9s4.03,9,9,9s9-4.03,9-9c0-0.46-0.04-0.92-0.1-1.36 c-0.98,1.37-2.58,2.26-4.4,2.26c-2.98,0-5.4-2.42-5.4-5.4c0-1.81,0.89-3.42,2.26-4.4C12.92,3.04,12.46,3,12,3L12,3z"></path></svg></div><input type="checkbox" class="toggleScreenReader_g2nN" aria-label="Switch between dark and light mode (currently light mode)"></div><div class="dsla-search-wrapper"><div class="dsla-search-field" data-tags="default,docs-default-current"></div></div></div></div><div role="presentation" class="navbar-sidebar__backdrop"></div></nav><div class="main-wrapper blog-wrapper blog-tags-post-list-page"><div class="container margin-vert--lg"><div class="row"><aside class="col col--3"><nav class="sidebar_a9qW thin-scrollbar" aria-label="Blog recent posts navigation"><div class="sidebarItemTitle_uKok margin-bottom--md">Recent posts</div><ul class="sidebarItemList_Kvuv"><li class="sidebarItem_CF0Q"><a class="sidebarItemLink_miNk" href="/ai-kb/blog/2021/10/01/clinical-decision-making">Clinical Decision Making</a></li><li class="sidebarItem_CF0Q"><a class="sidebarItemLink_miNk" href="/ai-kb/blog/2021/10/01/detectron-2">Detectron 2</a></li><li class="sidebarItem_CF0Q"><a class="sidebarItemLink_miNk" href="/ai-kb/blog/2021/10/01/distributed-training-of-recommender-systems">Distributed Training of Recommender Systems</a></li><li class="sidebarItem_CF0Q"><a class="sidebarItemLink_miNk" href="/ai-kb/blog/2021/10/01/document-recommendation">Document Recommendation</a></li><li class="sidebarItem_CF0Q"><a class="sidebarItemLink_miNk" href="/ai-kb/blog/2021/10/01/fake-voice-detection">Fake Voice Detection</a></li></ul></nav></aside><main class="col col--7" itemscope="" itemtype="http://schema.org/Blog"><header class="margin-bottom--xl"><h1>2 posts tagged with &quot;flask&quot;</h1><a href="/ai-kb/blog/tags">View All Tags</a></header><article class="margin-bottom--xl" itemprop="blogPost" itemscope="" itemtype="http://schema.org/BlogPosting"><header><h2 class="blogPostTitle_rzP5" itemprop="headline"><a itemprop="url" href="/ai-kb/blog/2021/10/01/image-similarity-system">Image Similarity System</a></h2><div class="blogPostData_Zg1s margin-vert--md"><time datetime="2021-10-01T00:00:00.000Z" itemprop="datePublished">October 1, 2021</time> · <!-- -->4 min read</div><div class="margin-top--md margin-bottom--sm row"><div class="col col--6 authorCol_FlmR"><div class="avatar margin-bottom--sm"><span class="avatar__photo-link avatar__photo"><a href="https://github.com/sparsh-ai" target="_blank" rel="noopener noreferrer"><img class="image_o0gy" src="https://avatars.githubusercontent.com/u/62965911?v=4" alt="Sparsh Agarwal"></a></span><div class="avatar__intro" itemprop="author" itemscope="" itemtype="https://schema.org/Person"><div class="avatar__name"><a href="https://github.com/sparsh-ai" target="_blank" rel="noopener noreferrer" itemprop="url"><span itemprop="name">Sparsh Agarwal</span></a></div><small class="avatar__subtitle" itemprop="description">Principal Developer</small></div></div></div></div></header><div class="markdown" itemprop="articleBody"><p><img loading="lazy" alt="/img/content-blog-raw-blog-image-similarity-system-untitled.png" src="/ai-kb/assets/images/content-blog-raw-blog-image-similarity-system-untitled-59c540a72e9f8afbece0d5a9bd41f513.png" width="1289" height="545"></p><h1>Choice of variables</h1><h3 class="anchor anchorWithStickyNavbar_mojV" id="image-encoder">Image Encoder<a class="hash-link" href="#image-encoder" title="Direct link to heading">​</a></h3><p>We can select any pre-trained image classification model. These models are commonly known as encoders because their job is to encode an image into a feature vector. I analyzed four encoders named 1) MobileNet, 2) EfficientNet, 3) ResNet and 4) <a href="https://tfhub.dev/google/bit/m-r152x4/1" target="_blank" rel="noopener noreferrer">BiT</a>. After basic research, I decided to select BiT model because of its performance and state-of-the-art nature. I selected the BiT-M-50x3 variant of model which is of size 748 MB. More details about this architecture can be found on the official page <a href="https://tfhub.dev/google/bit/m-r50x3/1" target="_blank" rel="noopener noreferrer">here</a>. </p><h3 class="anchor anchorWithStickyNavbar_mojV" id="vector-similarity-system">Vector Similarity System<a class="hash-link" href="#vector-similarity-system" title="Direct link to heading">​</a></h3><p>Images are represented in a fixed-length feature vector format. For the given input vector, we need to find the TopK most similar vectors, keeping the memory efficiency and real-time retrival objective in mind. I explored the most popular techniques and listed down five of them: Annoy, Cosine distance, L1 distance, Locally Sensitive Hashing (LSH) and Image Deep Ranking. I selected Annoy because of its fast and efficient nature. More details about Annoy can be found on the official page <a href="https://github.com/spotify/annoy" target="_blank" rel="noopener noreferrer">here</a>.</p><h3 class="anchor anchorWithStickyNavbar_mojV" id="dataset">Dataset<a class="hash-link" href="#dataset" title="Direct link to heading">​</a></h3><p>I listed down 3 datasets from Kaggle that were best fitting the criteria of this use case: 1) <a href="https://www.kaggle.com/bhaskar2443053/fashion-small?" target="_blank" rel="noopener noreferrer">Fashion Product Images (Small)</a>, 2) <a href="https://www.kaggle.com/trolukovich/food11-image-dataset?" target="_blank" rel="noopener noreferrer">Food-11 image dataset</a> and 3) <a href="https://www.kaggle.com/jessicali9530/caltech256?" target="_blank" rel="noopener noreferrer">Caltech 256 Image Dataset</a>. I selected Fashion dataset and Foods dataset.</p><h1>Literature review</h1><ul><li>Determining Image similarity with Quasi-Euclidean Metric <a href="https://arxiv.org/abs/2006.14644v1" target="_blank" rel="noopener noreferrer">arxiv</a></li><li>CatSIM: A Categorical Image Similarity Metric <a href="https://arxiv.org/abs/2004.09073v1" target="_blank" rel="noopener noreferrer">arxiv</a></li><li>Central Similarity Quantization for Efficient Image and Video Retrieval <a href="https://arxiv.org/abs/1908.00347v5" target="_blank" rel="noopener noreferrer">arxiv</a></li><li>Improved Deep Hashing with Soft Pairwise Similarity for Multi-label Image Retrieval <a href="https://arxiv.org/abs/1803.02987v3" target="_blank" rel="noopener noreferrer">arxiv</a></li><li>Model-based Behavioral Cloning with Future Image Similarity Learning <a href="https://arxiv.org/abs/1910.03157v1" target="_blank" rel="noopener noreferrer">arxiv</a></li><li>Why do These Match? Explaining the Behavior of Image Similarity Models <a href="https://arxiv.org/abs/1905.10797v1" target="_blank" rel="noopener noreferrer">arxiv</a></li><li>Learning Non-Metric Visual Similarity for Image Retrieval <a href="https://arxiv.org/abs/1709.01353v2" target="_blank" rel="noopener noreferrer">arxiv</a></li></ul><h1>Process Flow</h1><h3 class="anchor anchorWithStickyNavbar_mojV" id="step-1-data-acquisition">Step 1: Data Acquisition<a class="hash-link" href="#step-1-data-acquisition" title="Direct link to heading">​</a></h3><p>Download the raw image dataset into a directory. Categorize these images into their respective category directories. Make sure that images are of the same type, JPEG recommended. We will also process the metadata and store it in a serialized file, CSV recommended. </p><h3 class="anchor anchorWithStickyNavbar_mojV" id="step-2-encoder-fine-tuning">Step 2: Encoder Fine-tuning<a class="hash-link" href="#step-2-encoder-fine-tuning" title="Direct link to heading">​</a></h3><p>Download the pre-trained image model and add two additional layers on top of that: the first layer is a feature vector layer and the second layer is the classification layer. We will only train these 2 layers on our data and after training, we will select the feature vector layer as the output of our fine-tuned encoder. After fine-tuning the model, we will save the feature extractor for later use.</p><p><img loading="lazy" alt="Fig: a screenshot of encoder fine-tuning process" src="/ai-kb/assets/images/content-blog-raw-blog-image-similarity-system-untitled-1-66cd649b6c0e03c173f3e0734b2b3312.png" width="1195" height="333"></p><p>Fig: a screenshot of encoder fine-tuning process</p><h3 class="anchor anchorWithStickyNavbar_mojV" id="step-3-image-vectorization">Step 3: Image Vectorization<a class="hash-link" href="#step-3-image-vectorization" title="Direct link to heading">​</a></h3><p>Now, we will use the encoder (prepared in step 2) to encode the images (prepared in step 1). We will save feature vector of each image as an array in a directory. After processing, we will save these embeddings for later use.</p><h3 class="anchor anchorWithStickyNavbar_mojV" id="step-4-metadata-and-indexing">Step 4: Metadata and Indexing<a class="hash-link" href="#step-4-metadata-and-indexing" title="Direct link to heading">​</a></h3><p>We will assign a unique id to each image and create dictionaries to locate information of this image: 1) Image id to Image name dictionary, 2) Image id to image feature vector dictionary, and 3) (optional) Image id to metadata product id dictionary. We will also create an image id to image feature vector indexing. Then we will save these dictionaries and index object for later use.</p><h3 class="anchor anchorWithStickyNavbar_mojV" id="step-5-api-call">Step 5: API Call<a class="hash-link" href="#step-5-api-call" title="Direct link to heading">​</a></h3><p>We will receive an image from user, encode it with our image encoder, find TopK similar vectors using Indexing object, and retrieve the image (and metadata) using dictionaries. We send these images (and metadata) back to the user.</p><h1>Deployment</h1><p>The API was deployed on AWS cloud infrastructure using AWS Elastic Beanstalk service.</p><p><img loading="lazy" alt="/img/content-blog-raw-blog-image-similarity-system-untitled-2.png" src="/ai-kb/assets/images/content-blog-raw-blog-image-similarity-system-untitled-2-ca3d98690fca750590d55d9899c4d862.png" width="1883" height="593"></p></div><footer class="row docusaurus-mt-lg"><div class="col col--9"><b>Tags:</b><ul class="tags_XVD_ padding--none margin-left--sm"><li class="tag_JSN8"><a class="tag_hD8n tagRegular_D6E_" href="/ai-kb/blog/tags/aws-beanstalk">aws beanstalk</a></li><li class="tag_JSN8"><a class="tag_hD8n tagRegular_D6E_" href="/ai-kb/blog/tags/flask">flask</a></li><li class="tag_JSN8"><a class="tag_hD8n tagRegular_D6E_" href="/ai-kb/blog/tags/similarity">similarity</a></li><li class="tag_JSN8"><a class="tag_hD8n tagRegular_D6E_" href="/ai-kb/blog/tags/vision">vision</a></li></ul></div><div class="col text--right col--3"><a aria-label="Read more about Image Similarity System" href="/ai-kb/blog/2021/10/01/image-similarity-system"><b>Read More</b></a></div></footer></article><article class="margin-bottom--xl" itemprop="blogPost" itemscope="" itemtype="http://schema.org/BlogPosting"><header><h2 class="blogPostTitle_rzP5" itemprop="headline"><a itemprop="url" href="/ai-kb/blog/2021/10/01/name-&amp;-address-parsing">Name &amp; Address Parsing</a></h2><div class="blogPostData_Zg1s margin-vert--md"><time datetime="2021-10-01T00:00:00.000Z" itemprop="datePublished">October 1, 2021</time> · <!-- -->4 min read</div><div class="margin-top--md margin-bottom--sm row"><div class="col col--6 authorCol_FlmR"><div class="avatar margin-bottom--sm"><span class="avatar__photo-link avatar__photo"><a href="https://github.com/sparsh-ai" target="_blank" rel="noopener noreferrer"><img class="image_o0gy" src="https://avatars.githubusercontent.com/u/62965911?v=4" alt="Sparsh Agarwal"></a></span><div class="avatar__intro" itemprop="author" itemscope="" itemtype="https://schema.org/Person"><div class="avatar__name"><a href="https://github.com/sparsh-ai" target="_blank" rel="noopener noreferrer" itemprop="url"><span itemprop="name">Sparsh Agarwal</span></a></div><small class="avatar__subtitle" itemprop="description">Principal Developer</small></div></div></div></div></header><div class="markdown" itemprop="articleBody"><p><img loading="lazy" alt="/img/content-blog-raw-blog-name-&amp;amp;-address-parsing-untitled.png" src="/ai-kb/assets/images/content-blog-raw-blog-name-&amp;-address-parsing-untitled-35aab4c126484cf369b9cf212de0564a.png" width="700" height="342"></p><h1>Introduction</h1><p>Create an API that can parse and classify names and addresses given a string. We tried <a href="https://github.com/datamade/probablepeople" target="_blank" rel="noopener noreferrer">probablepeople</a> and <a href="https://github.com/datamade/usaddress" target="_blank" rel="noopener noreferrer">usaddress</a>. These work well separately but need the functionality of these packages combined, and better accuracy than what probablepeople provides.
For the API, I&#x27;d like to mimic <a href="https://parserator.datamade.us/api-docs/" target="_blank" rel="noopener noreferrer">this</a> with some minor modifications.
A few examples: </p><ul><li>&quot;KING JOHN A 5643 ROUTH CREEK PKWY #1314 RICHARDSON TEXAS 750820146 UNITED STATES OF AMERICA&quot; would return type: person; first_name: JOHN; last_name: KING; middle: A; street_address: 5643 ROUTH CREEK PKWY #1314; city: RICHARDSON; state: TEXAS; zip: 75082-0146; country: UNITED STATES OF AMERICA.</li><li>&quot;THRM NGUYEN LIVING TRUST 2720 SUMMERTREE CARROLLTON HOUSTON TEXAS 750062646 UNITED STATES OF AMERICA&quot; would return type: entity; name: THRM NGUYEN LIVING TRUST; street_address: 2720 SUMMERTREE CARROLLTON; state: TEXAS; city: HOUSTON; zip: 75006-2646; country: UNITED STATES OF AMERICA.</li></ul><h1>Modeling Approach</h1><h3 class="anchor anchorWithStickyNavbar_mojV" id="list-of-entities">List of Entities<a class="hash-link" href="#list-of-entities" title="Direct link to heading">​</a></h3><p>List of Entities A - Person, Household, Corporation</p><p>List of Entities B - Person First name, Person Middle name, Person Last name, Street address, City, State, Pincode, Country, Company name</p><h3 class="anchor anchorWithStickyNavbar_mojV" id="endpoint-configuration">Endpoint Configuration<a class="hash-link" href="#endpoint-configuration" title="Direct link to heading">​</a></h3><p><strong>OOR Endpoint</strong></p><p>Input Instance: ANDERSON, EARLINE 1423 NEW YORK AVE FORT WORTH, TX 76104 7522</p><div class="codeBlockContainer_I0IT theme-code-block"><div class="codeBlockContent_wNvx"><pre tabindex="0" class="prism-code language-text codeBlock_jd64 thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_mRuA"><span class="token-line" style="color:#393A34"><span class="token plain">Output Tags:-</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">&lt;Type&gt; - Person/Household/Corporation</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">&lt;GivenName&gt;, &lt;MiddleName&gt;, &lt;Surname&gt; - if Type Person/Household</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">&lt;Name&gt; - Full Name - if Type Person </span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">&lt;Name&gt; - Household - if Type Household</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">&lt;Name&gt; - Corporation - If Type Corporation</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">&lt;Address&gt; - Full Address</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">&lt;StreetAddress&gt;, &lt;City&gt;, &lt;State&gt;, &lt;Zipcode&gt;, &lt;Country&gt;</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">~~NameConfidence, AddrConfidence~~</span><br></span></code></pre><button type="button" aria-label="Copy code to clipboard" class="copyButton_wuS7 clean-btn">Copy</button></div></div><p><strong>Name Endpoint</strong></p><p>Input Instance: ANDERSON, EARLINE</p><div class="codeBlockContainer_I0IT theme-code-block"><div class="codeBlockContent_wNvx"><pre tabindex="0" class="prism-code language-text codeBlock_jd64 thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_mRuA"><span class="token-line" style="color:#393A34"><span class="token plain">Output Tags:-</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">- &lt;Type&gt; - Person/Household/Corporation</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">- &lt;GivenName&gt;, &lt;MiddleName&gt;, &lt;Surname&gt; - if Type Person/Household</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">- &lt;Name&gt; - Full Name - if Type Person</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">- &lt;Name&gt; - Household - if Type Household</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">- &lt;Name&gt; - Corporation - If Type Corporation</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">- ~~NameConfidence~~</span><br></span></code></pre><button type="button" aria-label="Copy code to clipboard" class="copyButton_wuS7 clean-btn">Copy</button></div></div><p><strong>Address Endpoint</strong></p><p>Input Instance: 1423 NEW YORK AVE FORT WORTH, TX 76104 7522</p><div class="codeBlockContainer_I0IT theme-code-block"><div class="codeBlockContent_wNvx"><pre tabindex="0" class="prism-code language-text codeBlock_jd64 thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_mRuA"><span class="token-line" style="color:#393A34"><span class="token plain">Output Tags:-</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">- &lt;Address&gt; - Full Address</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">- &lt;StreetAddress&gt;, &lt;City&gt;, &lt;State&gt;, &lt;Zipcode&gt;, &lt;Country&gt;</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">- ~~AddrConfidence~~</span><br></span></code></pre><button type="button" aria-label="Copy code to clipboard" class="copyButton_wuS7 clean-btn">Copy</button></div></div><h3 class="anchor anchorWithStickyNavbar_mojV" id="process-flow">Process Flow<a class="hash-link" href="#process-flow" title="Direct link to heading">​</a></h3><ul><li>Pytorch Flair NER model</li><li>Pre trained word embeddings</li><li>Additional parsing models on top of name tags</li><li>Tagging of 1000+ records to create training data</li><li>Deployment as REST api with 3 endpoints - name parse, address parse and whole string parse</li></ul><h1>Framework</h1><p><img loading="lazy" alt="/img/content-blog-raw-blog-name-&amp;amp;-address-parsing-untitled-1.png" src="/ai-kb/assets/images/content-blog-raw-blog-name-&amp;-address-parsing-untitled-1-0c56bfea9f214ccebee859fd1f6d5b33.png" width="1299" height="447"></p><p><img loading="lazy" alt="/img/content-blog-raw-blog-name-&amp;amp;-address-parsing-untitled-2.png" src="/ai-kb/assets/images/content-blog-raw-blog-name-&amp;-address-parsing-untitled-2-bf22e8e0a5e1f39628fdb8d05e2e14b7.png" width="1302" height="534"></p><h1>Tagging process</h1><p>I used Doccano (<a href="https://github.com/doccano/doccano" target="_blank" rel="noopener noreferrer">https://github.com/doccano/doccano</a>) for labeling the dataset. This tool is open-source and free to use. I deployed it with a one-click Heroku service (fig 1). After launching the app, log in with the provided credentials, and create a project (fig 2). Create the labels and upload the dataset (fig 3). Start the annotation process (fig 4). Now after enough annotations (you do not need complete all annotations in one go), go back to projects &gt; edit section and export the data (fig 5). Bring the exported JSON file in python and run the model training code. The whole model will automatically get trained on the new annotations. To make the training faster, you can use Nvidia GPU support.</p><p><img loading="lazy" alt="fig 1: screenshot taken from Doccano&amp;#39;s github page" src="/ai-kb/assets/images/content-blog-raw-blog-name-&amp;-address-parsing-untitled-3-ad825b4a9077c68bb1069d9c595af800.png" width="435" height="323"></p><p>fig 1: screenshot taken from Doccano&#x27;s github page</p><p><img loading="lazy" alt="fig 2: Doccano&amp;#39;s deployed app homepage" src="/ai-kb/assets/images/content-blog-raw-blog-name-&amp;-address-parsing-untitled-4-3b2954d0a30a62e7231f231eeccd0604.png" width="1166" height="675"></p><p>fig 2: Doccano&#x27;s deployed app homepage</p><p><img loading="lazy" alt="fig 3: create the labels. I defined these labels for my project" src="/ai-kb/assets/images/content-blog-raw-blog-name-&amp;-address-parsing-untitled-5-c02e457933ab867810fe666c34fb4e76.png" width="1349" height="623"></p><p>fig 3: create the labels. I defined these labels for my project</p><p><img loading="lazy" alt="fig 5: export the annotations" src="/ai-kb/assets/images/content-blog-raw-blog-name-&amp;-address-parsing-untitled-6-6de4153eff54d52c35fd5c0bed123a3d.png" width="1342" height="260"></p><p>fig 5: export the annotations</p><h1>Model</h1><p>I first tried the Spacy NER blank model but it was not giving high-quality results. So I moved to the PyTorch Flair NER model. This model was a way faster (5 min training because of GPU compatibility comparing to 1-hour Spacy training time) and also much more accurate. F1 results for all tags were near perfect (score of 1).  This score will increase further with more labeled data. This model is production-ready.</p><h1>Inference</h1><p>For OOR, I directly used the model&#x27;s output for core tagging and created the aggregated tags like recipient (aggregation of name tags) and address (aggregation of address tags like city and state) using simple conditional concatenation. For only Name and only Address inference, I added the dummy address in name text and dummy name in address text. This way, I passed the text in same model and later on filtered the required tags as output. </p><h3 class="anchor anchorWithStickyNavbar_mojV" id="api">API<a class="hash-link" href="#api" title="Direct link to heading">​</a></h3><p>I used Flask REST framework in Python to build the API with 3 endpoints. This API is production-ready.</p><h1>Results and Discussion</h1><ul><li>0.99 F1 score on 6 out of 8 tags &amp; 0.95+ F1 score on other 2 tags</li><li>API inference time of less than 1 second on single CPU</li></ul></div><footer class="row docusaurus-mt-lg"><div class="col col--9"><b>Tags:</b><ul class="tags_XVD_ padding--none margin-left--sm"><li class="tag_JSN8"><a class="tag_hD8n tagRegular_D6E_" href="/ai-kb/blog/tags/app">app</a></li><li class="tag_JSN8"><a class="tag_hD8n tagRegular_D6E_" href="/ai-kb/blog/tags/flask">flask</a></li><li class="tag_JSN8"><a class="tag_hD8n tagRegular_D6E_" href="/ai-kb/blog/tags/ner">ner</a></li><li class="tag_JSN8"><a class="tag_hD8n tagRegular_D6E_" href="/ai-kb/blog/tags/nlp">nlp</a></li></ul></div><div class="col text--right col--3"><a aria-label="Read more about Name &amp; Address Parsing" href="/ai-kb/blog/2021/10/01/name-&amp;-address-parsing"><b>Read More</b></a></div></footer></article><nav class="pagination-nav" aria-label="Blog list page navigation"><div class="pagination-nav__item"></div><div class="pagination-nav__item pagination-nav__item--next"></div></nav></main></div></div></div><footer class="footer footer--dark"><div class="container container-fluid"><div class="footer__bottom text--center"><div class="footer__copyright">Copyright © 2022 AIKB Docs, Inc. Built with Docusaurus.</div></div></div></footer></div>
<script src="/ai-kb/assets/js/runtime~main.4448d8b2.js"></script>
<script src="/ai-kb/assets/js/main.b26486b4.js"></script>
</body>
</html>