"use strict";(self.webpackChunkdocs=self.webpackChunkdocs||[]).push([[6180],{3905:function(e,t,n){n.d(t,{Zo:function(){return d},kt:function(){return _}});var r=n(67294);function i(e,t,n){return t in e?Object.defineProperty(e,t,{value:n,enumerable:!0,configurable:!0,writable:!0}):e[t]=n,e}function o(e,t){var n=Object.keys(e);if(Object.getOwnPropertySymbols){var r=Object.getOwnPropertySymbols(e);t&&(r=r.filter((function(t){return Object.getOwnPropertyDescriptor(e,t).enumerable}))),n.push.apply(n,r)}return n}function l(e){for(var t=1;t<arguments.length;t++){var n=null!=arguments[t]?arguments[t]:{};t%2?o(Object(n),!0).forEach((function(t){i(e,t,n[t])})):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(n)):o(Object(n)).forEach((function(t){Object.defineProperty(e,t,Object.getOwnPropertyDescriptor(n,t))}))}return e}function p(e,t){if(null==e)return{};var n,r,i=function(e,t){if(null==e)return{};var n,r,i={},o=Object.keys(e);for(r=0;r<o.length;r++)n=o[r],t.indexOf(n)>=0||(i[n]=e[n]);return i}(e,t);if(Object.getOwnPropertySymbols){var o=Object.getOwnPropertySymbols(e);for(r=0;r<o.length;r++)n=o[r],t.indexOf(n)>=0||Object.prototype.propertyIsEnumerable.call(e,n)&&(i[n]=e[n])}return i}var a=r.createContext({}),s=function(e){var t=r.useContext(a),n=t;return e&&(n="function"==typeof e?e(t):l(l({},t),e)),n},d=function(e){var t=s(e.components);return r.createElement(a.Provider,{value:t},e.children)},u={inlineCode:"code",wrapper:function(e){var t=e.children;return r.createElement(r.Fragment,{},t)}},m=r.forwardRef((function(e,t){var n=e.components,i=e.mdxType,o=e.originalType,a=e.parentName,d=p(e,["components","mdxType","originalType","parentName"]),m=s(n),_=i,f=m["".concat(a,".").concat(_)]||m[_]||u[_]||o;return n?r.createElement(f,l(l({ref:t},d),{},{components:n})):r.createElement(f,l({ref:t},d))}));function _(e,t){var n=arguments,i=t&&t.mdxType;if("string"==typeof e||i){var o=n.length,l=new Array(o);l[0]=m;var p={};for(var a in t)hasOwnProperty.call(t,a)&&(p[a]=t[a]);p.originalType=e,p.mdxType="string"==typeof e?e:i,l[1]=p;for(var s=2;s<o;s++)l[s]=n[s];return r.createElement.apply(null,l)}return r.createElement.apply(null,n)}m.displayName="MDXCreateElement"},86634:function(e,t,n){n.r(t),n.d(t,{assets:function(){return d},contentTitle:function(){return a},default:function(){return _},frontMatter:function(){return p},metadata:function(){return s},toc:function(){return u}});var r=n(87462),i=n(63366),o=(n(67294),n(3905)),l=["components"],p={},a="Model Optimization",s={unversionedId:"tutorials/model-optimization",id:"tutorials/model-optimization",title:"Model Optimization",description:"Keras Model Pruning",source:"@site/docs/04-tutorials/model-optimization.md",sourceDirName:"04-tutorials",slug:"/tutorials/model-optimization",permalink:"/ai-kb/docs/tutorials/model-optimization",editUrl:"https://github.com/sparsh-ai/ai-kb/docs/04-tutorials/model-optimization.md",tags:[],version:"current",frontMatter:{},sidebar:"tutorialSidebar",previous:{title:"MLOps",permalink:"/ai-kb/docs/tutorials/mlops"},next:{title:"Multi-Touch Attribution",permalink:"/ai-kb/docs/tutorials/multi-touch-attribution"}},d={},u=[{value:"Keras Model Pruning",id:"keras-model-pruning",level:2},{value:"Keras Model Quantization",id:"keras-model-quantization",level:2}],m={toc:u};function _(e){var t=e.components,n=(0,i.Z)(e,l);return(0,o.kt)("wrapper",(0,r.Z)({},m,n,{components:t,mdxType:"MDXLayout"}),(0,o.kt)("h1",{id:"model-optimization"},"Model Optimization"),(0,o.kt)("h2",{id:"keras-model-pruning"},"Keras Model Pruning"),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-py"},"!pip install -q tensorflow-model-optimization\n\nimport tempfile\n\n_, keras_file = tempfile.mkstemp('.h5')\ntf.keras.models.save_model(model, keras_file, include_optimizer=True)\nprint('Saved baseline model to:', keras_file)\n\n# Compute end step to finish pruning after 2 epochs.\nbatch_size = 32\nepochs = 100\nvalidation_split = 0.\n\nnum_samples = X_train.shape[0] * (1 - validation_split)\nend_step = np.ceil(num_samples / batch_size).astype(np.int32) * epochs\nend_step\n\nimport tensorflow_model_optimization as tfmot\n\nprune_low_magnitude = tfmot.sparsity.keras.prune_low_magnitude\n\n# Define model for pruning.\npruning_params = {\n      'pruning_schedule': tfmot.sparsity.keras.PolynomialDecay(initial_sparsity=0.50,\n                                                               final_sparsity=0.80,\n                                                               begin_step=0,\n                                                               end_step=end_step)\n}\n\nmodel_for_pruning = prune_low_magnitude(model, **pruning_params)\n\nopt = tf.keras.optimizers.Adam(learning_rate=0.001)\n\n# `prune_low_magnitude` requires a recompile.\nmodel_for_pruning.compile(optimizer=opt, loss='mean_squared_error', metrics=['mae'])\n\nmodel_for_pruning.summary()\n\nlogdir = tempfile.mkdtemp()\n\ncallbacks = [\n  tfmot.sparsity.keras.UpdatePruningStep(),\n  tfmot.sparsity.keras.PruningSummaries(log_dir=logdir),\n  es_callback\n]\n\nhistory_prune = model_for_pruning.fit(X_train, y_train,\n                                    batch_size=batch_size, epochs=epochs,\n                                    callbacks=callbacks,\n                                    validation_data=(X_test, y_test))\n\nmodel_for_pruning_score = r2_score(y_test, model_for_pruning.predict(X_test))\n\nprint('Baseline test score:', baseline_model_score) \nprint('Pruned test score:', model_for_pruning_score)\n\n# summarize history for mae\nplt.plot(history.history['mae'])\nplt.plot(history.history['val_mae'])\nplt.plot(history_prune.history['mae'])\nplt.plot(history_prune.history['val_mae'])\nplt.title('model Mean Absolute Error (MAE)')\nplt.ylabel('MAE')\nplt.xlabel('epoch')\nplt.legend(['train', 'test', 'pruned train', 'pruned test'], loc='upper left')\nplt.show()\n\n# summarize history for loss\nplt.plot(history.history['loss'])\nplt.plot(history.history['val_loss'])\nplt.plot(history_prune.history['loss'])\nplt.plot(history_prune.history['val_loss'])\nplt.title('model loss')\nplt.ylabel('loss')\nplt.xlabel('epoch')\nplt.legend(['train', 'test', 'pruned train', 'pruned test'], loc='upper left')\nplt.show()\n\nmodel_for_export = tfmot.sparsity.keras.strip_pruning(model_for_pruning)\n\n_, pruned_keras_file = tempfile.mkstemp('.h5')\ntf.keras.models.save_model(model_for_export, pruned_keras_file, include_optimizer=False)\nprint('Saved pruned Keras model to:', pruned_keras_file)\n\nconverter = tf.lite.TFLiteConverter.from_keras_model(model_for_export)\npruned_tflite_model = converter.convert()\n\n_, pruned_tflite_file = tempfile.mkstemp('.tflite')\n\nwith open(pruned_tflite_file, 'wb') as f:\n  f.write(pruned_tflite_model)\n\nprint('Saved pruned TFLite model to:', pruned_tflite_file)\n\ndef get_gzipped_model_size(file):\n  # Returns size of gzipped model, in bytes.\n  import os\n  import zipfile\n\n  _, zipped_file = tempfile.mkstemp('.zip')\n  with zipfile.ZipFile(zipped_file, 'w', compression=zipfile.ZIP_DEFLATED) as f:\n    f.write(file)\n\n  return os.path.getsize(zipped_file)\n\n\nprint(\"Size of gzipped baseline Keras model: %.2f bytes\" % (get_gzipped_model_size(keras_file)))\nprint(\"Size of gzipped pruned Keras model: %.2f bytes\" % (get_gzipped_model_size(pruned_keras_file)))\nprint(\"Size of gzipped pruned TFlite model: %.2f bytes\" % (get_gzipped_model_size(pruned_tflite_file)))\n")),(0,o.kt)("h2",{id:"keras-model-quantization"},"Keras Model Quantization"),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-py"},"# applying post-training quantization to the pruned model for additional benefits\nconverter = tf.lite.TFLiteConverter.from_keras_model(model_for_export)\nconverter.optimizations = [tf.lite.Optimize.DEFAULT]\nquantized_and_pruned_tflite_model = converter.convert()\n\n_, quantized_and_pruned_tflite_file = tempfile.mkstemp('.tflite')\n\nwith open(quantized_and_pruned_tflite_file, 'wb') as f:\n  f.write(quantized_and_pruned_tflite_model)\n\nprint('Saved quantized and pruned TFLite model to:', quantized_and_pruned_tflite_file)\n\nprint(\"Size of gzipped baseline Keras model: %.2f bytes\" % (get_gzipped_model_size(keras_file)))\nprint(\"Size of gzipped pruned and quantized TFlite model: %.2f bytes\" % (get_gzipped_model_size(quantized_and_pruned_tflite_file)))\n")))}_.isMDXComponent=!0}}]);