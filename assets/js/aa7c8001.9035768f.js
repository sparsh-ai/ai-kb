"use strict";(self.webpackChunkdocs=self.webpackChunkdocs||[]).push([[7043],{3905:function(e,t,n){n.d(t,{Zo:function(){return c},kt:function(){return u}});var a=n(67294);function r(e,t,n){return t in e?Object.defineProperty(e,t,{value:n,enumerable:!0,configurable:!0,writable:!0}):e[t]=n,e}function l(e,t){var n=Object.keys(e);if(Object.getOwnPropertySymbols){var a=Object.getOwnPropertySymbols(e);t&&(a=a.filter((function(t){return Object.getOwnPropertyDescriptor(e,t).enumerable}))),n.push.apply(n,a)}return n}function o(e){for(var t=1;t<arguments.length;t++){var n=null!=arguments[t]?arguments[t]:{};t%2?l(Object(n),!0).forEach((function(t){r(e,t,n[t])})):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(n)):l(Object(n)).forEach((function(t){Object.defineProperty(e,t,Object.getOwnPropertyDescriptor(n,t))}))}return e}function s(e,t){if(null==e)return{};var n,a,r=function(e,t){if(null==e)return{};var n,a,r={},l=Object.keys(e);for(a=0;a<l.length;a++)n=l[a],t.indexOf(n)>=0||(r[n]=e[n]);return r}(e,t);if(Object.getOwnPropertySymbols){var l=Object.getOwnPropertySymbols(e);for(a=0;a<l.length;a++)n=l[a],t.indexOf(n)>=0||Object.prototype.propertyIsEnumerable.call(e,n)&&(r[n]=e[n])}return r}var p=a.createContext({}),i=function(e){var t=a.useContext(p),n=t;return e&&(n="function"==typeof e?e(t):o(o({},t),e)),n},c=function(e){var t=i(e.components);return a.createElement(p.Provider,{value:t},e.children)},m={inlineCode:"code",wrapper:function(e){var t=e.children;return a.createElement(a.Fragment,{},t)}},d=a.forwardRef((function(e,t){var n=e.components,r=e.mdxType,l=e.originalType,p=e.parentName,c=s(e,["components","mdxType","originalType","parentName"]),d=i(n),u=r,g=d["".concat(p,".").concat(u)]||d[u]||m[u]||l;return n?a.createElement(g,o(o({ref:t},c),{},{components:n})):a.createElement(g,o({ref:t},c))}));function u(e,t){var n=arguments,r=t&&t.mdxType;if("string"==typeof e||r){var l=n.length,o=new Array(l);o[0]=d;var s={};for(var p in t)hasOwnProperty.call(t,p)&&(s[p]=t[p]);s.originalType=e,s.mdxType="string"==typeof e?e:r,o[1]=s;for(var i=2;i<l;i++)o[i]=n[i];return a.createElement.apply(null,o)}return a.createElement.apply(null,n)}d.displayName="MDXCreateElement"},2719:function(e,t,n){n.r(t),n.d(t,{assets:function(){return c},contentTitle:function(){return p},default:function(){return u},frontMatter:function(){return s},metadata:function(){return i},toc:function(){return m}});var a=n(87462),r=n(63366),l=(n(67294),n(3905)),o=["components"],s={},p="Email Classification",i={unversionedId:"tutorials/email-classification",id:"tutorials/email-classification",title:"Email Classification",description:"Fetching data from MS-Sql",source:"@site/docs/04-tutorials/email-classification.md",sourceDirName:"04-tutorials",slug:"/tutorials/email-classification",permalink:"/ai-kb/docs/tutorials/email-classification",editUrl:"https://github.com/sparsh-ai/ai-kb/docs/04-tutorials/email-classification.md",tags:[],version:"current",frontMatter:{},sidebar:"tutorialSidebar",previous:{title:"Database Connections",permalink:"/ai-kb/docs/tutorials/database-conn"},next:{title:"Google Cloud Big Data and Machine Learning Fundamentals",permalink:"/ai-kb/docs/tutorials/google-cloud"}},c={},m=[{value:"Fetching data from MS-Sql",id:"fetching-data-from-ms-sql",level:2},{value:"Wrangling",id:"wrangling",level:2},{value:"Text Cleaning",id:"text-cleaning",level:2},{value:"Transformer model",id:"transformer-model",level:2},{value:"TFIDF model",id:"tfidf-model",level:2},{value:"FastText model",id:"fasttext-model",level:2},{value:"Pipeline",id:"pipeline",level:2}],d={toc:m};function u(e){var t=e.components,s=(0,r.Z)(e,o);return(0,l.kt)("wrapper",(0,a.Z)({},d,s,{components:t,mdxType:"MDXLayout"}),(0,l.kt)("h1",{id:"email-classification"},"Email Classification"),(0,l.kt)("a",{href:"https://nbviewer.org/github/recohut/notebook/blob/master/_notebooks/2022-01-02-email-classification.ipynb",alt:""}," ",(0,l.kt)("img",{src:"https://colab.research.google.com/assets/colab-badge.svg"})),(0,l.kt)("h2",{id:"fetching-data-from-ms-sql"},"Fetching data from MS-Sql"),(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre",className:"language-python"},"!apt install unixodbc-dev\n!pip install pyodbc\n")),(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre",className:"language-sh"},"%%sh\ncurl https://packages.microsoft.com/keys/microsoft.asc | apt-key add -\ncurl https://packages.microsoft.com/config/ubuntu/16.04/prod.list > /etc/apt/sources.list.d/mssql-release.list\nsudo apt-get update\nsudo ACCEPT_EULA=Y apt-get -q -y install msodbcsql17\n")),(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre",className:"language-python"},"import os\nimport pyodbc\nimport urllib\nimport pandas as pd\nfrom sqlalchemy import create_engine\n")),(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre",className:"language-python"},"driver = [item for item in pyodbc.drivers()][-1]\nconn_string = f'Driver={driver};Server=tcp:server.<domain>.com,<port>;Database=<db>;Uid=<userid>;Pwd=<pass>;Encrypt=yes;TrustServerCertificate=yes;Connection Timeout=30;'\nconn = pyodbc.connect(conn_string)\ncursor = conn.cursor()\n")),(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre",className:"language-python"},"# params = urllib.parse.quote_plus(conn_string)\n# conn_str = 'mssql+pyodbc:///?odbc_connect={}'.format(params)\n# engine_feat = create_engine(conn_str, echo=True)\n# print(engine_feat.table_names())\n")),(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre",className:"language-python"},"tname = 'tbl_Final_Lable_Data_18_n_19'\nquery = f'select count(*) from {tname}'\n\ncursor.execute(query)\ncursor.fetchall()\n")),(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre",className:"language-python"},"query = f'select top 5 * from {tname}'\ndf = pd.read_sql(query, conn)\ndf.info()\n")),(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre"},"<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 5 entries, 0 to 4\nData columns (total 18 columns):\n #   Column                Non-Null Count  Dtype \n---  ------                --------------  ----- \n 0   aglobalcaseid         5 non-null      int64 \n 1   Team In               5 non-null      object\n 2   Department            5 non-null      object\n 3   QueryType             5 non-null      object\n 4   SubQueryType          5 non-null      object\n 5   Comment               5 non-null      object\n 6   OriginalQuery         5 non-null      object\n 7   OriginalSubQuery      5 non-null      object\n 8   tSubject              5 non-null      object\n 9   mMsgContent           5 non-null      object\n 10  UmMsgContent          0 non-null      object\n 11  MasterDepartment      5 non-null      object\n 12  nCaseState            0 non-null      object\n 13  nCreatedFromMedia     5 non-null      int64 \n 14  Interaction_category  5 non-null      object\n 15  LastTeamName          0 non-null      object\n 16  Rating                5 non-null      object\n 17  AcustId               5 non-null      int64 \ndtypes: int64(3), object(15)\nmemory usage: 848.0+ bytes\n")),(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre",className:"language-python"},"%reload_ext google.colab.data_table\n")),(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre",className:"language-python"},"df\n")),(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre",className:"language-python"},"df.columns\n")),(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre"},"Index(['aglobalcaseid', 'Team In', 'Department', 'QueryType', 'SubQueryType',\n       'Comment', 'OriginalQuery', 'OriginalSubQuery', 'tSubject',\n       'mMsgContent', 'UmMsgContent', 'MasterDepartment', 'nCaseState',\n       'nCreatedFromMedia', 'Interaction_category', 'LastTeamName', 'Rating',\n       'AcustId'],\n      dtype='object')\n")),(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre",className:"language-python"},"query = f'select tSubject, mMsgContent, QueryType, SubQueryType from {tname}'\ndf = pd.read_sql(query, conn)\ndf.info()\n")),(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre"},"<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 930913 entries, 0 to 930912\nData columns (total 4 columns):\n #   Column        Non-Null Count   Dtype \n---  ------        --------------   ----- \n 0   tSubject      930913 non-null  object\n 1   mMsgContent   716024 non-null  object\n 2   QueryType     930913 non-null  object\n 3   SubQueryType  930913 non-null  object\ndtypes: object(4)\nmemory usage: 28.4+ MB\n")),(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre",className:"language-python"},"df.to_pickle('data.p')\n")),(0,l.kt)("h2",{id:"wrangling"},"Wrangling"),(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre",className:"language-python"},"# wrangling.py\nimport os\nimport numpy as np\nimport pandas as pd\nspath = '/content/email_class'\ndf = pd.read_pickle(os.path.join(spath,'data','raw','data.p'))\ndf.columns = ['subj','msg','qtype','stype']\ndf['type'] = df['qtype'] + ' | ' + df['stype']\ndf = df.replace(r'^\\s*$', np.nan, regex=True)\ndf = df.dropna(how='all')\ndf = df.drop_duplicates()\ndf = df.dropna(subset=['subj', 'msg'], how='all')\ndf = df.replace(r'^\\s*$', np.nan, regex=True)\ndf = df.dropna(how='all')\ndf = df.drop_duplicates()\ndf = df.dropna(subset=['subj', 'msg'], how='all')\ndf = df.fillna(' ')\ndf['subj&msg'] = df['subj'] + ' sub_eos_token ' + df['msg']\ndf = df[['subj&msg','type']]\ndf.columns = ['text','target']\ndf.sample(10000).to_pickle('df_raw_wrangled_sample_10k.p')\n# df.sample(10000).to_pickle(os.path.join(spath,'data','wrangled','df_raw_wrangled_sample_10k.p'))\n# df.to_pickle(os.path.join(spath,'data','wrangled','df_raw_wrangled_full.p'))\n")),(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre",className:"language-python"},"import os\nimport numpy as np\nimport pandas as pd\nfrom tqdm import tqdm\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\ntqdm.pandas()\n%reload_ext autoreload\n%autoreload 2\n%reload_ext google.colab.data_table\n%config InlineBackend.figure_format = 'retina'\n\nplt.style.use('fivethirtyeight')\nplt.style.use('seaborn-notebook')\n")),(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre",className:"language-python"},"df = pd.read_pickle(os.path.join(spath,'data','raw','data.p'))\ndf.info()\n")),(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre"},"<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 930913 entries, 0 to 930912\nData columns (total 4 columns):\n #   Column        Non-Null Count   Dtype \n---  ------        --------------   ----- \n 0   tSubject      930913 non-null  object\n 1   mMsgContent   716024 non-null  object\n 2   QueryType     930913 non-null  object\n 3   SubQueryType  930913 non-null  object\ndtypes: object(4)\nmemory usage: 28.4+ MB\n")),(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre",className:"language-python"},"df.sample(20)\n")),(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre",className:"language-python"},"df.columns\n")),(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre",className:"language-python"},"df.columns = ['subj','msg','qtype','stype']\n")),(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre",className:"language-python"},"df.qtype.nunique()\n")),(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre",className:"language-python"},"df.qtype.value_counts()[:50]\n")),(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre",className:"language-python"},"df.stype.nunique()\n")),(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre",className:"language-python"},"df.stype.value_counts()[:50]\n")),(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre",className:"language-python"},"df['type'] = df['qtype'] + ' | ' + df['stype']\n")),(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre",className:"language-python"},"df['type'].nunique()\n")),(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre",className:"language-python"},"df['type'].value_counts()[:50]\n")),(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre",className:"language-python"},"df.subj.nunique()\n")),(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre"},"688740\n")),(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre",className:"language-python"},"df.subj.value_counts()[:50]\n")),(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre",className:"language-python"},"df.msg.nunique()\n")),(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre"},"665982\n")),(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre",className:"language-python"},"df.msg.value_counts()[:50]\n")),(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre",className:"language-python"},"df[df.msg.isnull()].sample(10)\n")),(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre",className:"language-python"},"df[(df.msg.isnull()) & (df.subj.isnull())].info()\n")),(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre"},"<class 'pandas.core.frame.DataFrame'>\nInt64Index: 0 entries\nData columns (total 4 columns):\n #   Column  Non-Null Count  Dtype \n---  ------  --------------  ----- \n 0   subj    0 non-null      object\n 1   msg     0 non-null      object\n 2   qtype   0 non-null      object\n 3   stype   0 non-null      object\ndtypes: object(4)\nmemory usage: 0.0+ bytes\n")),(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre",className:"language-python"},"df2 = df.replace(r'^\\s*$', np.nan, regex=True)\ndf2.info()\n")),(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre"},"<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 930913 entries, 0 to 930912\nData columns (total 5 columns):\n #   Column  Non-Null Count   Dtype \n---  ------  --------------   ----- \n 0   subj    907099 non-null  object\n 1   msg     712917 non-null  object\n 2   qtype   930913 non-null  object\n 3   stype   585435 non-null  object\n 4   type    930913 non-null  object\ndtypes: object(5)\nmemory usage: 35.5+ MB\n")),(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre",className:"language-python"},"df3 = df2.dropna(how='all')\ndf3.info()\n")),(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre"},"<class 'pandas.core.frame.DataFrame'>\nInt64Index: 930913 entries, 0 to 930912\nData columns (total 5 columns):\n #   Column  Non-Null Count   Dtype \n---  ------  --------------   ----- \n 0   subj    907099 non-null  object\n 1   msg     712917 non-null  object\n 2   qtype   930913 non-null  object\n 3   stype   585435 non-null  object\n 4   type    930913 non-null  object\ndtypes: object(5)\nmemory usage: 42.6+ MB\n")),(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre",className:"language-python"},"df4 = df3.drop_duplicates()\ndf4.info()\n")),(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre"},"<class 'pandas.core.frame.DataFrame'>\nInt64Index: 873726 entries, 0 to 930912\nData columns (total 5 columns):\n #   Column  Non-Null Count   Dtype \n---  ------  --------------   ----- \n 0   subj    860490 non-null  object\n 1   msg     684972 non-null  object\n 2   qtype   873726 non-null  object\n 3   stype   557609 non-null  object\n 4   type    873726 non-null  object\ndtypes: object(5)\nmemory usage: 40.0+ MB\n")),(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre",className:"language-python"},"df4[(df4.msg.isnull()) & (df4.subj.isnull())]\n")),(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre",className:"language-python"},"df5 = df4.dropna(subset=['subj', 'msg'], how='all')\ndf5.info()\n")),(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre"},"<class 'pandas.core.frame.DataFrame'>\nInt64Index: 873578 entries, 0 to 930912\nData columns (total 5 columns):\n #   Column  Non-Null Count   Dtype \n---  ------  --------------   ----- \n 0   subj    860490 non-null  object\n 1   msg     684972 non-null  object\n 2   qtype   873578 non-null  object\n 3   stype   557569 non-null  object\n 4   type    873578 non-null  object\ndtypes: object(5)\nmemory usage: 40.0+ MB\n")),(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre",className:"language-python"},"df4.shape, df5.shape\n")),(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre"},"((873726, 5), (873578, 5))\n")),(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre",className:"language-python"},"sample = df5.sample(10)\nsample\n")),(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre",className:"language-python"},"!pip install ekphrasis\nfrom ekphrasis.classes.preprocessor import TextPreProcessor\nfrom ekphrasis.classes.tokenizer import SocialTokenizer\nfrom ekphrasis.dicts.emoticons import emoticons\ntext_processor = TextPreProcessor(\n  normalize=['url', 'email', 'percent', 'money', 'phone', \n              'user', 'time', 'date', 'number'],\n  # annotate={\"hashtag\", \"allcaps\", \"elongated\", \"repeated\",\n  #           'emphasis', 'censored'},\n  fix_html=True,\n  segmenter=\"twitter\",\n  corrector=\"twitter\", \n  unpack_hashtags=True,\n  unpack_contractions=True,\n  spell_correct_elong=False,\n  tokenizer=SocialTokenizer(lowercase=False).tokenize,\n  dicts=[emoticons]\n  )\n")),(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre",className:"language-python"},"import re\nfrom bs4 import BeautifulSoup\n")),(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre",className:"language-python"},"# text_raw = sample.loc[[299500]].msg.tolist()[0]\n# text = text_raw\n# text = BeautifulSoup(text, \"lxml\").text\n# text = re.sub(r'<.*?>', ' ', text)\n# text = re.sub(r'\\{[^{}]*\\}', ' ', text)\n# text = re.sub(r'\\s', ' ', text)\n# text = re.sub(r'.*\\..*ID.*?(?=\\s)', ' ', text)\n# text = re.sub(r'DIV..*?(?=\\s)', ' ', text)\n# text = BeautifulSoup(text, \"lxml\").text\n# text = text.strip()\n# text = \" \".join(text_processor.pre_process_doc(text))\n# text\n")),(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre",className:"language-python"},"from itertools import groupby\n")),(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre",className:"language-python"},"html_residual = 'P . ImprintUniqueID LI . ImprintUniqueID DIV . ImprintUniqueID TABLE . ImprintUniqueIDTable DIV . Section '\ncaution_residual =  'CAUTION This email originated from outside of the organization . Do not click links or open attachments unless you recognize the sender and know the content is safe . '\n")),(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre",className:"language-python"},"def clean_text(text):\n  text = ' ' + text + ' '\n  text = BeautifulSoup(text, \"lxml\").text\n  text = re.sub(r'<.*?>', ' ', text)\n  text = re.sub(r'\\{[^{}]*\\}', ' ', text)\n  text = re.sub(r'\\s', ' ', text)\n  # text = re.sub(r'(?=\\s).*\\..*ID.*?(?=\\s)', ' ', text)\n  # text = re.sub(r'(?=\\s)DIV..*?(?=\\s)', ' ', text)\n  text = re.sub(r'Forwarded message.*?(?=____)', ' ', text)\n  text = BeautifulSoup(text, \"lxml\").text\n  text = ' '.join(text_processor.pre_process_doc(text))\n  text = re.sub(r'[^A-Za-z<>. ]', ' ', text)\n  text = ' '.join(text.split())\n  text = re.sub(html_residual, '', text)\n  text = re.sub(caution_residual, '', text)\n  text = re.sub(r'(?:\\d+[a-zA-Z]+|[a-zA-Z]+\\d+)', '<hash>', text)\n  # text = re.sub(r'\\b\\w{1,2}\\b', '', text)\n  text = ' '.join(text.split())\n  text = ' '.join([k for k,v in groupby(text.split())])\n  return text\n")),(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre",className:"language-python"},"# # text_raw = sample.loc[[75806]].msg.tolist()[0]\n# text = text_raw\n# text = BeautifulSoup(text, \"lxml\").text\n# text = re.sub(r'<.*?>', ' ', text)\n# text = re.sub(r'\\{[^{}]*\\}', ' ', text)\n# text = re.sub(r'\\s', ' ', text)\n# # text = re.sub(r'.*\\..*ID.*?(?=\\s)', ' ', text)\n# # text = re.sub(r'DIV..*?(?=\\s)', ' ', text)\n# text = re.sub(r'Forwarded message.*?(?=____)', ' ', text)\n# text = BeautifulSoup(text, \"lxml\").text\n# text = \" \".join(text_processor.pre_process_doc(text))\n# text = re.sub(r'[^A-Za-z0-9<>. ]', ' ', text)\n# text = ' '.join(text.split())\n# text\n")),(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre",className:"language-python"},"sample = df5.sample(1000)\n")),(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre",className:"language-python"},"sample['subj_clean'] = sample['subj'].fillna(' ').apply(clean_text)\n")),(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre",className:"language-python"},"[(x,y) for x,y in zip(sample.subj.tolist()[:50],sample.subj_clean.tolist()[:50])]\n")),(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre",className:"language-python"},"sample['msg_clean'] = sample['msg'].fillna(' ').apply(clean_text)\nsample.msg.tolist()\n")),(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre",className:"language-python"},"sample.msg_clean.tolist()\n")),(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre",className:"language-python"},"sample = df5.sample(1000, random_state=40)\nsample['subj_clean'] = sample['subj'].fillna(' ').apply(clean_text)\nsample['msg_clean'] = sample['msg'].fillna(' ').apply(clean_text)\nsample['subj&msg'] = sample['subj_clean'] + ' | ' + sample['msg_clean']\nsample = sample[['subj&msg','type']]\nsample.columns = ['text','target']\nsample.info()\n")),(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre"},"<class 'pandas.core.frame.DataFrame'>\nInt64Index: 1000 entries, 36966 to 574457\nData columns (total 2 columns):\n #   Column  Non-Null Count  Dtype \n---  ------  --------------  ----- \n 0   text    1000 non-null   object\n 1   target  1000 non-null   object\ndtypes: object(2)\nmemory usage: 23.4+ KB\n")),(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre",className:"language-python"},"sample\n")),(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre",className:"language-python"},"sample = df5.copy()\nsample['subj_clean'] = sample['subj'].fillna(' ').apply(clean_text)\nsample['msg_clean'] = sample['msg'].fillna(' ').apply(clean_text)\nsample['subj&msg'] = sample['subj_clean'] + ' | ' + sample['msg_clean']\nsample = sample[['subj&msg','type']]\nsample.columns = ['text','target']\nsample.info()\n")),(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre"},"<class 'pandas.core.frame.DataFrame'>\nInt64Index: 873578 entries, 0 to 930912\nData columns (total 2 columns):\n #   Column  Non-Null Count   Dtype \n---  ------  --------------   ----- \n 0   text    873578 non-null  object\n 1   target  873578 non-null  object\ndtypes: object(2)\nmemory usage: 20.0+ MB\n")),(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre",className:"language-python"},"sample.nunique()\n")),(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre"},"text      803255\ntarget       500\ndtype: int64\n")),(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre",className:"language-python"},"sample2 = sample.drop_duplicates()\nsample2.info()\n")),(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre"},"<class 'pandas.core.frame.DataFrame'>\nInt64Index: 820778 entries, 0 to 930912\nData columns (total 2 columns):\n #   Column  Non-Null Count   Dtype \n---  ------  --------------   ----- \n 0   text    820778 non-null  object\n 1   target  820778 non-null  object\ndtypes: object(2)\nmemory usage: 18.8+ MB\n")),(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre",className:"language-python"},"sample3 = sample2.replace(r'^\\s*$', np.nan, regex=True)\nsample3.info()\n")),(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre"},"<class 'pandas.core.frame.DataFrame'>\nInt64Index: 820778 entries, 0 to 930912\nData columns (total 2 columns):\n #   Column  Non-Null Count   Dtype \n---  ------  --------------   ----- \n 0   text    820778 non-null  object\n 1   target  820778 non-null  object\ndtypes: object(2)\nmemory usage: 18.8+ MB\n")),(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre",className:"language-python"},"df5.to_pickle('df_raw_wrangled.p')\n")),(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre",className:"language-python"},"sample2.info()\n")),(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre"},"<class 'pandas.core.frame.DataFrame'>\nInt64Index: 820778 entries, 0 to 930912\nData columns (total 2 columns):\n #   Column  Non-Null Count   Dtype \n---  ------  --------------   ----- \n 0   text    820778 non-null  object\n 1   target  820778 non-null  object\ndtypes: object(2)\nmemory usage: 18.8+ MB\n")),(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre",className:"language-python"},"sample2.nunique()\n")),(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre"},"text      803255\ntarget       500\ndtype: int64\n")),(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre",className:"language-python"},"sample2.describe()\n")),(0,l.kt)("h2",{id:"text-cleaning"},"Text Cleaning"),(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre",className:"language-python"},"import os\nimport re\nimport numpy as np\nimport pandas as pd\nfrom tqdm import tqdm\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\nfrom bs4 import BeautifulSoup\n\nimport nltk\nnltk.download('stopwords')\nfrom nltk.corpus import stopwords\nstopwords = list(set(stopwords.words('english')))\n\nfrom nltk.stem import WordNetLemmatizer\nnltk.download('wordnet') \nlemmatizer = WordNetLemmatizer() \n\nfrom nltk.stem import PorterStemmer\nps = PorterStemmer()\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\ntqdm.pandas()\n%reload_ext autoreload\n%autoreload 2\n%reload_ext google.colab.data_table\n%config InlineBackend.figure_format = 'retina'\n\nplt.style.use('fivethirtyeight')\nplt.style.use('seaborn-notebook')\n")),(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre",className:"language-python"},"df = pd.read_pickle(os.path.join(path,'data','wrangled','df_raw_wrangled_sample_10k.p'))\ndf.info()\n")),(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre"},"<class 'pandas.core.frame.DataFrame'>\nInt64Index: 10000 entries, 930773 to 629839\nData columns (total 2 columns):\n #   Column  Non-Null Count  Dtype \n---  ------  --------------  ----- \n 0   text    10000 non-null  object\n 1   target  10000 non-null  object\ndtypes: object(2)\nmemory usage: 234.4+ KB\n")),(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre",className:"language-python"},"df.sample(5)\n")),(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre",className:"language-python"},"df[df.target=='<redacted>'].sample(5)\n")),(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre",className:"language-python"},"df.target.value_counts()\n")),(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre",className:"language-python"},"# # cleaning pipe\n# - lowercase\n# - remove nonalpha\n# - stopword\n# - lemmatization\n# - stemming\n\n# - min occurence\n# - max occurence\n# - ngram\n# - misspell\n# - contraction\n\n# - encode plus tokenizer\n")),(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre",className:"language-python"},"df = df.reset_index(drop=True)\n")),(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre",className:"language-python"},"# label tokens\ncaution_label = 'CAUTION: This email originated from outside of the organization. \\\nDo not click links or open attachments unless you recognize the sender and know the \\\ncontent is safe'\n\nconfidential_label = '<redacted>'\n\nconfidential_label = '<redacted>'\n\nretransmit_label = '<redacted>'\n\nalert_label = '<redacted>'\n\nhtml_labels = ['P.ImprintUniqueID', 'LI.ImprintUniqueID', 'DIV.ImprintUniqueID',\n              'TABLE.ImprintUniqueIDTable', 'DIV.Section1']\nhtml_regex = re.compile('|'.join(map(re.escape, html_labels)))\n\nnewline_token = '\\n'\n\n\ncustom_stopwords = ['best', 'regard', 'direct', 'number', 'phone', 'mobile', 'number', 'reply', 'url', 'com']\n\n")),(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre",className:"language-python"},"!pip install clean-text\n")),(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre",className:"language-python"},"def clean_l1(text):\n  text = re.sub(caution_label, ' cautionlabel ', text)\n  text = re.sub(confidential_label, ' confidentiallabel ', text)\n  text = html_regex.sub('htmltoken', text)\n  text = re.sub(retransmit_label, ' retransmittoken ', text)\n  text = re.sub(alert_label, ' alerttoken ', text)\n  text = re.sub('sub_eos_token', ' bodytoken ', text)\n  text = ' ' + text + ' '\n  text = BeautifulSoup(text, \"lxml\").text\n  text = re.sub(r'<.*?>', ' ', text)\n  text = re.sub(r'\\{[^{}]*\\}', ' ', text)\n  # # text = re.sub(r'Forwarded message.*?(?=____)', ' ', text)\n  text = BeautifulSoup(text, \"lxml\").text\n  text = re.sub(newline_token, ' newlinetoken ', text)\n  text = ' '.join(text.split())\n  text = re.sub(r'[^A-Za-z.,?\\'@]', ' ', text)\n  \n\n  # text = ' '.join(text.split())\n  return text\n")),(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre",className:"language-python"},"xx = clean_l1(df.loc[idx,'text']); xx\n# print(xx)\n")),(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre",className:"language-python"},"df.loc[idx,'text']\n")),(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre",className:"language-python"},"idx = np.random.randint(0,len(df))\nprint(idx)\nprint(df.loc[idx,'text'])\n")),(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre",className:"language-python"},"# idx = 9\n# print(df.text.iloc[[idx]].tolist()[0])\n# xx = df.text.iloc[[idx]].apply(clean_l1).tolist()[0]\n# xx\n")),(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre",className:"language-python"},"df['text_clean_l1'] = df.text.apply(clean_l1)\n")),(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre",className:"language-python"},"df.text_clean_l1.sample().tolist()[0]\n")),(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre",className:"language-python"},"set1_words = ['best regards', 'regards', 'thanks regards', 'warm regards']\nset1_regex = re.compile('|'.join(map(re.escape, set1_words)))\n")),(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre",className:"language-python"},"from itertools import groupby\n")),(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre",className:"language-python"},"def replace_words(s, words):\n  for k, v in words.items():\n      s = s.replace(k, v)\n  return s\n\nword_mapping = {' f o ':' fno ',\n                ' a c ':' account ',\n                ' a/c ':' account ',\n                ' fw ':' forward ',\n                ' fwd ':' forward ',\n                ' forwarded ':' forward ',\n                ' no. ':' number ',\n                }\n")),(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre",className:"language-python"},"def clean_l2(text):\n  text = ' ' + text + ' '\n  text = text.lower()\n  text = ' '.join(text.split())\n  text = replace_words(text, word_mapping)\n  text = re.sub('[.]', ' . ', text)\n  text = re.sub('[,]', ' , ', text)\n  text = re.sub('[?]', ' ? ', text)\n  text = ' '.join([w for w in text.split() if re.match('^[a-z.,?\\'\\-\\~#`!&*()]+$', w)])\n  text = re.sub(r'[^a-z.,?\\']', ' ', text)\n  text = set1_regex.sub('eostoken', text)\n  text = text + ' eostoken '\n  text = re.match(r'^.*?eostoken', text).group(0)\n  text = re.sub(r'eostoken', '', text)\n  text = re.sub(r'\\b\\w{1,1}\\b', '', text)\n  text = ' '.join([k for k,v in groupby(text.split())])\n  text = ' '.join(text.split())\n  return text\n")),(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre",className:"language-python"},"xxy = 'warm regards regrads hello regards'\nxxy = ' '.join([w for w in xxy.split() if re.match('^[a-z.,?\\'\\-\\~#`!&*()]+$', w)])\nxxy = re.sub(r'[^a-z.,?\\']', ' ', xxy)\nxxy = set1_regex.sub('eostoken', xxy)\nre.match(r'^.*?eostoken', xxy).group(0)\n")),(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre"},"'warm eostoken'\n")),(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre",className:"language-python"},"idx = 1\n# print(df.text.iloc[[idx]].tolist()[0])\nprint(df.text_clean_l1.iloc[[idx]].tolist()[0])\ndf.text_clean_l1.iloc[[idx]].apply(clean_l2).tolist()[0]\n")),(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre",className:"language-python"},"df['text_clean_l2'] = df.text_clean_l1.apply(clean_l2)\n")),(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre",className:"language-python"},"df.drop_duplicates().replace(r'^\\s*$', np.nan, regex=True).info()\n")),(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre"},"<class 'pandas.core.frame.DataFrame'>\nInt64Index: 10000 entries, 930773 to 629839\nData columns (total 4 columns):\n #   Column         Non-Null Count  Dtype \n---  ------         --------------  ----- \n 0   text           10000 non-null  object\n 1   target         10000 non-null  object\n 2   text_clean_l1  10000 non-null  object\n 3   text_clean_l2  9999 non-null   object\ndtypes: object(4)\nmemory usage: 390.6+ KB\n")),(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre",className:"language-python"},"xx = df.text_clean_l2.apply(lambda x: len(x.split())).values\nsns.distplot(xx[xx<1000])\nxx.min(), xx.max()\n")),(0,l.kt)("p",null,(0,l.kt)("img",{loading:"lazy",alt:"png",src:n(98671).Z,width:"1115",height:"703"})),(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre",className:"language-python"},"# print(np.argmax(-xx))\nprint(list(np.argsort(-xx))[:20])\ndf.iloc[[374]]\n")),(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre",className:"language-python"},"df1 = df[(df.text_clean_l2.apply(lambda x: len(x.split()))>3) & (df.text_clean_l2.apply(lambda x: len(x.split()))<200)]\ndf1.info()\n")),(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre"},"<class 'pandas.core.frame.DataFrame'>\nInt64Index: 9012 entries, 930773 to 629839\nData columns (total 4 columns):\n #   Column         Non-Null Count  Dtype \n---  ------         --------------  ----- \n 0   text           9012 non-null   object\n 1   target         9012 non-null   object\n 2   text_clean_l1  9012 non-null   object\n 3   text_clean_l2  9012 non-null   object\ndtypes: object(4)\nmemory usage: 352.0+ KB\n")),(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre",className:"language-python"},"def clean_l3(text):\n  text = re.sub(r'[^a-z]', '', text)\n  text = ' '.join([lemmatizer.lemmatize(w, 'v') for w in text.split()])\n  text = ' '.join([lemmatizer.lemmatize(w) for w in text.split()])\n  text = ' '.join([w for w in text.split() if not w in stopwords])\n  text = ' '.join([w for w in text.split() if not w in custom_stopwords])\n  # seen = set()\n  # seen_add = seen.add\n  # text = ' '.join([x for x in text.split() if not (x in seen or seen_add(x))])\n  text = ' '.join([ps.stem(w) for w in text.split()])\n  return text\n")),(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre",className:"language-python"},"# def temp(text):\n#   text = ' '.join([lemmatizer.lemmatize(w) for w in text.split()])\n#   text = ' '.join([lemmatizer.lemmatize(w, 'j') for w in text.split()])\n#   text = ' '.join([lemmatizer.lemmatize(w, 'V') for w in text.split()])\n#   text = ' '.join([lemmatizer.lemmatize(w, 'R') for w in text.split()])\n#   return text\n\n# # temp('communicate communication')\n\n# import spacy\n# from spacy.lemmatizer import Lemmatizer, ADJ, NOUN, VERB\n# lemmatizer = nlp.vocab.morphology.lemmatizer\n# lemmatizer('communicate communication', VERB)\n\n# from nltk.stem import PorterStemmer\n# ps = PorterStemmer()\n# for w in ['commute', 'communication']:\n#     rootWord=ps.stem(w)\n#     print(rootWord)\n")),(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre",className:"language-python"},"df['text_clean_l2'] = df.text_clean_l1.apply(clean_l2)\n")),(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre",className:"language-python"},"df.text_clean_l2.sample(5, random_state=10).tolist()\n")),(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre",className:"language-python"},"# import spacy\n# nlp = spacy.load(\"en_core_web_sm\")\n\n# def ners(text):\n#   doc = nlp(text)\n  # for token in doc:\n  #   print(token.text)\n  # x = list(set([ent.text for ent in doc.ents if ent.label_=='ORG']))\n  # x = list(set([(ent.text,ent.label_) for ent in doc.ents]))\n  # return x\n\n# df.sample(20).text.apply(ners).tolist()\n# df.text.iloc[[14]].apply(ners)\n")),(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre",className:"language-python"},"from sklearn.feature_extraction.text import CountVectorizer\nvectorizer = CountVectorizer(stop_words='english', max_df=0.5, min_df=10, ngram_range=(1,3))\nvectorizer.fit_transform(df.clean.tolist())\n")),(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre"},"<10000x5456 sparse matrix of type '<class 'numpy.int64'>'\n    with 270546 stored elements in Compressed Sparse Row format>\n")),(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre",className:"language-python"},"idx = 100\nprint(df.text.iloc[[idx]].tolist()[0])\npd.DataFrame(vectorizer.inverse_transform(vectorizer.transform([df.clean.tolist()[idx]]))).T[0]\n")),(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre",className:"language-python"},"from sklearn.feature_extraction.text import TfidfVectorizer\nvect = TfidfVectorizer(stop_words='english', max_df=0.5, min_df=10, ngram_range=(1,2))\nX = vect.fit_transform(df.clean.tolist())\n\ndef top_tfidf_feats(row, features, top_n=20):\n    topn_ids = np.argsort(row)[::-1][:top_n]\n    top_feats = [(features[i], row[i]) for i in topn_ids]\n    df = pd.DataFrame(top_feats, columns=['features', 'score'])\n    return df\ndef top_feats_in_doc(X, features, row_id, top_n=25):\n    row = np.squeeze(X[row_id].toarray())\n    return top_tfidf_feats(row, features, top_n)\n\nfeatures = vect.get_feature_names()\n")),(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre",className:"language-python"},"idx = 14\nprint(df.text.iloc[[idx]].tolist()[0])\nprint(top_feats_in_doc(X, features, idx, 10))\n")),(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre",className:"language-python"},"df[df.clean.str.contains('vora')]\n")),(0,l.kt)("h2",{id:"transformer-model"},"Transformer model"),(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre",className:"language-python"},"!pip install -q clean-text[gpl] && cp '/content/drive/My Drive/clean_v2.py' .\nfrom clean_v2 import clean_l1\n")),(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre",className:"language-python"},"import os\nimport re\n\nimport numpy as np\nimport pandas as pd\nfrom tqdm import tqdm\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\nfrom collections import OrderedDict\nfrom sklearn.preprocessing import LabelEncoder\n\nfrom sklearn.model_selection import train_test_split\n\nimport csv\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\ntqdm.pandas()\n%reload_ext autoreload\n%autoreload 2\n%reload_ext google.colab.data_table\n%config InlineBackend.figure_format = 'retina'\n\nplt.style.use('fivethirtyeight')\nplt.style.use('seaborn-notebook')\n")),(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre",className:"language-python"},"df_raw = pd.read_pickle(os.path.join(path,'data_clean_v2.p'))\ndf_raw.info()\n")),(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre"},"<class 'pandas.core.frame.DataFrame'>\nInt64Index: 824410 entries, 0 to 930912\nData columns (total 2 columns):\n #   Column  Non-Null Count   Dtype \n---  ------  --------------   ----- \n 0   text    824410 non-null  object\n 1   target  824410 non-null  object\ndtypes: object(2)\nmemory usage: 18.9+ MB\n")),(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre",className:"language-python"},"df = df_raw.sample(10000, random_state=42)\n")),(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre",className:"language-python"},"tokenlist = ['emailtoken', 'urltoken', 'newlinetoken', 'htmltoken', 'currencytoken', 'token', 'digittoken', 'numbertoken']\ndef preprocess(text):\n  text = text.lower()\n  text = ' '.join([w for w in text.split() if w not in tokenlist])\n  text = ' '.join(text.split()[:50])\n  return text\ndf['text'] = df.text.apply(preprocess)\ndf = df[df.text.apply(lambda x: len(x.split()))>3]\n")),(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre",className:"language-python"},"df['target'] = df.target.apply(clean_l1).str.lower().str.split().apply(lambda x: OrderedDict.fromkeys(x).keys()).str.join(' ')\nminority_labels = df.target.value_counts()[df.target.value_counts()<100].index.tolist()\ndf['target'] = df.target.replace(dict.fromkeys(minority_labels, 'other'))\n")),(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre",className:"language-python"},"df = df[df.target!='other']\ndf.target.value_counts()[:25]\n")),(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre",className:"language-python"},"target_map = {'<redacted>': '<redacted>'}\ndf = df.replace({'target': target_map})\ndf.target.value_counts()[:25]\n")),(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre",className:"language-python"},"label_encoder = LabelEncoder()\ndf['target'] = label_encoder.fit_transform(df['target'])\n")),(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre",className:"language-python"},"df.head()\n")),(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre",className:"language-python"},"with open('label.csv', 'w') as f:\n    wr = csv.writer(f,delimiter=\"\\n\")\n    wr.writerow(df.target.unique().tolist())\n")),(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre",className:"language-python"},"train, val = train_test_split(df, test_size=0.2, random_state=42)\ntrain.reset_index(drop=True).to_csv('train.csv')\nval.reset_index(drop=True).to_csv('val.csv')\n")),(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre",className:"language-python"},"!pip install -q fast-bert\n")),(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre",className:"language-python"},"from fast_bert.data_cls import BertDataBunch\n\ndatabunch = BertDataBunch('/content', '/content',\n                          tokenizer='distilbert-base-uncased',\n                          train_file='train.csv',\n                          val_file='val.csv',\n                          label_file='label.csv',\n                          text_col='text',\n                          label_col='target',\n                          batch_size_per_gpu=16,\n                          max_seq_length=100,\n                          multi_gpu=False,\n                          multi_label=False,\n                          model_type='distilbert')\n")),(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre"},"HBox(children=(FloatProgress(value=0.0, description='Downloading', max=442.0, style=ProgressStyle(description_\u2026\n\n\n\n\n\n\nHBox(children=(FloatProgress(value=0.0, description='Downloading', max=231508.0, style=ProgressStyle(descripti\u2026\n")),(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre",className:"language-python"},"from fast_bert.learner_cls import BertLearner\nfrom fast_bert.metrics import accuracy\nimport logging\nimport torch\n\nlogger = logging.getLogger()\ndevice_cuda = 'cuda' if torch.cuda.is_available() else 'cpu'\nmetrics = [{'name': 'accuracy', 'function': accuracy}]\n")),(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre",className:"language-python"},"learner = BertLearner.from_pretrained_model(\n                        databunch,\n                        pretrained_path='distilbert-base-uncased',\n                        metrics=metrics,\n                        device=device_cuda,\n                        logger=logger,\n                        output_dir='/content',\n                        finetuned_wgts_path=None,\n                        warmup_steps=500,\n                        multi_gpu=False,\n                        is_fp16=True,\n                        multi_label=False,\n                        logging_steps=50)\n")),(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre"},"HBox(children=(FloatProgress(value=0.0, description='Downloading', max=267967963.0, style=ProgressStyle(descri\u2026\n\n\n\n\n\nSome weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_transform.weight', 'vocab_transform.bias', 'vocab_layer_norm.weight', 'vocab_layer_norm.bias', 'vocab_projector.weight', 'vocab_projector.bias']\n- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).\n- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\nSome weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['pre_classifier.weight', 'pre_classifier.bias', 'classifier.weight', 'classifier.bias']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n")),(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre",className:"language-python"},"learner.lr_find(start_lr=1e-4,optimizer_type='lamb')\n")),(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre"},"HBox(children=(FloatProgress(value=0.0), HTML(value='')))\n\n\nStopping early, the loss has diverged\nLearning rate search finished. See the graph with {finder_name}.plot()\n")),(0,l.kt)("p",null,(0,l.kt)("img",{loading:"lazy",alt:"png",src:n(94261).Z,width:"1104",height:"734"})),(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre",className:"language-python"},"learner.plot(show_lr=2e-2)\n")),(0,l.kt)("p",null,(0,l.kt)("img",{loading:"lazy",alt:"png",src:n(95155).Z,width:"1104",height:"734"})),(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre",className:"language-python"},' learner.fit(epochs=1,\n            lr=2e-2,\n            validate=True,\n            schedule_type="warmup_cosine",\n            optimizer_type="lamb",\n            return_results=True)\n')),(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre"},"(206,\n 1.1582254237920335,\n [{'accuracy': 0.5176184690157959, 'loss': 1.860094638971182}])\n")),(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre",className:"language-python"},"learner.validate()\n")),(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre"},"{'accuracy': 0.5820170109356014, 'loss': 1.515815230516287}\n")),(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre",className:"language-python"},"learner.save_model()\n")),(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre",className:"language-python"},"xx = val.sample(5); xx\n")),(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre",className:"language-python"},"predictions = learner.predict_batch(xx.text.tolist())\npd.DataFrame(predictions).T\n")),(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre",className:"language-python"},"# from fast_bert.prediction import BertClassificationPredictor\n# MODEL_PATH = '/content/model_out'\n\n# predictor = BertClassificationPredictor(\n#               model_path='/content',\n#               label_path='/content',\n#               multi_label=False,\n#               model_type='xlnet',\n#               do_lower_case=True)\n\n# single_prediction = predictor.predict(\"just get me result for this text\")\n# texts = [\"this is the first text\", \"this is the second text\"]\n# multiple_predictions = predictor.predict_batch(texts)\n")),(0,l.kt)("hr",null),(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre",className:"language-python"},"train, val = train_test_split(df, test_size=0.2, random_state=42)\n\ntrain = train.reset_index(drop=True)\ntrain.columns = ['text','labels']\n\nval = val.reset_index(drop=True)\nval.columns = ['text','labels']\n")),(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre",className:"language-python"},"!pip install -q simpletransformers\n")),(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre",className:"language-python"},'from simpletransformers.classification import ClassificationModel, ClassificationArgs\nimport logging\n\nlogging.basicConfig(level=logging.INFO)\ntransformers_logger = logging.getLogger("transformers")\ntransformers_logger.setLevel(logging.WARNING)\n')),(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre",className:"language-python"},"model_args = ClassificationArgs(num_train_epochs=1, learning_rate=1e-2)\n")),(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre",className:"language-python"},"model = ClassificationModel('distilbert', 'distilbert-base-uncased', num_labels=df.target.nunique(), args=model_args)\n")),(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre",className:"language-python"},"model.train_model(train)\n")),(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre",className:"language-python"},"scores1, model_outputs, wrong_predictions = model.eval_model(val)\n")),(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre",className:"language-python"},"scores1\n")),(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre",className:"language-python"},"from sklearn.metrics import f1_score, accuracy_score\ndef f1_multiclass(labels, preds):\n    return f1_score(labels, preds, average='micro')\n")),(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre",className:"language-python"},"scores2, model_outputs, wrong_predictions = model.eval_model(val, f1=f1_multiclass, acc=accuracy_score)\n")),(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre",className:"language-python"},"scores2\n")),(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre",className:"language-python"},"predictions, raw_output  = model.predict(['<redacted>'])\n")),(0,l.kt)("h2",{id:"tfidf-model"},"TFIDF model"),(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre",className:"language-python"},"!pip install -q clean-text[gpl] && cp '/content/drive/My Drive/clean_v2.py' .\n\nfrom clean_v2 import clean_l1\n")),(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre",className:"language-python"},"import os\nimport re\nimport numpy as np\nimport pandas as pd\nfrom tqdm import tqdm\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\nimport nltk\nnltk.download('stopwords')\nfrom nltk.corpus import stopwords\nstopwords = list(set(stopwords.words('english')))\n\nfrom nltk.stem import WordNetLemmatizer\nnltk.download('wordnet') \nlemmatizer = WordNetLemmatizer()\n\nfrom nltk.stem import PorterStemmer\nps = PorterStemmer()\n\nfrom sklearn.feature_extraction.text import CountVectorizer\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\ntqdm.pandas()\n%reload_ext autoreload\n%autoreload 2\n%reload_ext google.colab.data_table\n%config InlineBackend.figure_format = 'retina'\n\nplt.style.use('fivethirtyeight')\nplt.style.use('seaborn-notebook')\n")),(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre",className:"language-python"},"df_raw = pd.read_pickle(os.path.join(path,'data_clean_v2.p'))\ndf_raw.info()\n")),(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre"},"<class 'pandas.core.frame.DataFrame'>\nInt64Index: 824410 entries, 0 to 930912\nData columns (total 2 columns):\n #   Column  Non-Null Count   Dtype \n---  ------  --------------   ----- \n 0   text    824410 non-null  object\n 1   target  824410 non-null  object\ndtypes: object(2)\nmemory usage: 18.9+ MB\n")),(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre",className:"language-python"},"df = df_raw.sample(10000, random_state=42)\n")),(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre",className:"language-python"},"def clean_l2(text):\n  text = text.lower()\n  text = re.sub(r'[^a-z ]', '', text)\n  text = ' '.join([lemmatizer.lemmatize(w, 'v') for w in text.split()])\n  text = ' '.join([lemmatizer.lemmatize(w) for w in text.split()])\n  text = ' '.join([w for w in text.split() if not w in stopwords])\n  text = ' '.join([ps.stem(w) for w in text.split()])\n  return text\n")),(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre",className:"language-python"},"from collections import OrderedDict\ndf['target'] = df.target.apply(clean_l1).str.lower().str.split().apply(lambda x: OrderedDict.fromkeys(x).keys()).str.join(' ')\n")),(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre",className:"language-python"},"df['text'] = df['text'].apply(clean_l2)\n")),(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre",className:"language-python"},"df = df[df.text.apply(lambda x: len(x.split()))>3]\n")),(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre",className:"language-python"},"xx = df.sample(5, random_state=40)\nxx\n")),(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre",className:"language-python"},"import gensim\nimport nltk\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\nfrom sklearn.metrics import accuracy_score, confusion_matrix\n")),(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre",className:"language-python"},"df['text'].apply(lambda x: len(x.split(' '))).sum()\n")),(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre",className:"language-python"},"minority_labels = df.target.value_counts()[df.target.value_counts()<100].index.tolist()\ndf['target'] = df.target.replace(dict.fromkeys(minority_labels, 'Other'))\ndf = df[df.target!='Other']\n")),(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre",className:"language-python"},"X = df.text\ny = df.target\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state = 42)\n")),(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre",className:"language-python"},"from sklearn.naive_bayes import MultinomialNB\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.feature_extraction.text import TfidfTransformer\n\nnb = Pipeline([('vect', CountVectorizer()),\n               ('tfidf', TfidfTransformer()),\n               ('clf', MultinomialNB()),\n              ])\nnb.fit(X_train, y_train)\n")),(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre"},"Pipeline(memory=None,\n         steps=[('vect',\n                 CountVectorizer(analyzer='word', binary=False,\n                                 decode_error='strict',\n                                 dtype=<class 'numpy.int64'>, encoding='utf-8',\n                                 input='content', lowercase=True, max_df=1.0,\n                                 max_features=None, min_df=1,\n                                 ngram_range=(1, 1), preprocessor=None,\n                                 stop_words=None, strip_accents=None,\n                                 token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n                                 tokenizer=None, vocabulary=None)),\n                ('tfidf',\n                 TfidfTransformer(norm='l2', smooth_idf=True,\n                                  sublinear_tf=False, use_idf=True)),\n                ('clf',\n                 MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True))],\n         verbose=False)\n")),(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre",className:"language-python"},"label_list = list(df.target.unique())\n")),(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre",className:"language-python"},"%%time\nfrom sklearn.metrics import classification_report\ny_pred = nb.predict(X_test)\n\nprint('accuracy %s' % accuracy_score(y_pred, y_test))\nprint(classification_report(y_test, y_pred,target_names=label_list))\n")),(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre",className:"language-python"},"from sklearn.linear_model import SGDClassifier\n\nsgd = Pipeline([('vect', CountVectorizer()),\n                ('tfidf', TfidfTransformer()),\n                ('clf', SGDClassifier(loss='hinge', penalty='l2',alpha=1e-3, random_state=42, max_iter=5, tol=None)),\n               ])\nsgd.fit(X_train, y_train)\n")),(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre"},"Pipeline(memory=None,\n         steps=[('vect',\n                 CountVectorizer(analyzer='word', binary=False,\n                                 decode_error='strict',\n                                 dtype=<class 'numpy.int64'>, encoding='utf-8',\n                                 input='content', lowercase=True, max_df=1.0,\n                                 max_features=None, min_df=1,\n                                 ngram_range=(1, 1), preprocessor=None,\n                                 stop_words=None, strip_accents=None,\n                                 token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n                                 tokenizer=None, vocabulary=Non...\n                ('clf',\n                 SGDClassifier(alpha=0.001, average=False, class_weight=None,\n                               early_stopping=False, epsilon=0.1, eta0=0.0,\n                               fit_intercept=True, l1_ratio=0.15,\n                               learning_rate='optimal', loss='hinge',\n                               max_iter=5, n_iter_no_change=5, n_jobs=None,\n                               penalty='l2', power_t=0.5, random_state=42,\n                               shuffle=True, tol=None, validation_fraction=0.1,\n                               verbose=0, warm_start=False))],\n         verbose=False)\n")),(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre",className:"language-python"},"%%time\ny_pred = sgd.predict(X_test)\n\nprint('accuracy %s' % accuracy_score(y_pred, y_test))\nprint(classification_report(y_test, y_pred,target_names=label_list))\n")),(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre",className:"language-python"},"from sklearn.linear_model import LogisticRegression\n\nlogreg = Pipeline([('vect', CountVectorizer()),\n                ('tfidf', TfidfTransformer()),\n                ('clf', LogisticRegression(n_jobs=1, C=1e5)),\n               ])\nlogreg.fit(X_train, y_train)\n")),(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre"},"Pipeline(memory=None,\n         steps=[('vect',\n                 CountVectorizer(analyzer='word', binary=False,\n                                 decode_error='strict',\n                                 dtype=<class 'numpy.int64'>, encoding='utf-8',\n                                 input='content', lowercase=True, max_df=1.0,\n                                 max_features=None, min_df=1,\n                                 ngram_range=(1, 1), preprocessor=None,\n                                 stop_words=None, strip_accents=None,\n                                 token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n                                 tokenizer=None, vocabulary=None)),\n                ('tfidf',\n                 TfidfTransformer(norm='l2', smooth_idf=True,\n                                  sublinear_tf=False, use_idf=True)),\n                ('clf',\n                 LogisticRegression(C=100000.0, class_weight=None, dual=False,\n                                    fit_intercept=True, intercept_scaling=1,\n                                    l1_ratio=None, max_iter=100,\n                                    multi_class='auto', n_jobs=1, penalty='l2',\n                                    random_state=None, solver='lbfgs',\n                                    tol=0.0001, verbose=0, warm_start=False))],\n         verbose=False)\n")),(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre",className:"language-python"},"%%time\ny_pred = logreg.predict(X_test)\n\nprint('accuracy %s' % accuracy_score(y_pred, y_test))\nprint(classification_report(y_test, y_pred,target_names=label_list))\n")),(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre",className:"language-python"},'from tqdm import tqdm\ntqdm.pandas(desc="progress-bar")\nfrom gensim.models import Doc2Vec\nfrom sklearn import utils\nimport gensim\nfrom gensim.models.doc2vec import TaggedDocument\nimport re\n')),(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre",className:"language-python"},'def label_sentences(corpus, label_type):\n    """\n    Gensim\'s Doc2Vec implementation requires each document/paragraph to have a label associated with it.\n    We do this by using the TaggedDocument method. The format will be "TRAIN_i" or "TEST_i" where "i" is\n    a dummy index of the post.\n    """\n    labeled = []\n    for i, v in enumerate(corpus):\n        label = label_type + \'_\' + str(i)\n        labeled.append(TaggedDocument(v.split(), [label]))\n    return labeled\n')),(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre",className:"language-python"},"X_train, X_test, y_train, y_test = train_test_split(df.text, df.target, random_state=0, test_size=0.3)\nX_train = label_sentences(X_train, 'Train')\nX_test = label_sentences(X_test, 'Test')\nall_data = X_train + X_test\n")),(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre",className:"language-python"},"model_dbow = Doc2Vec(dm=0, vector_size=300, negative=5, min_count=1, alpha=0.065, min_alpha=0.065)\nmodel_dbow.build_vocab([x for x in tqdm(all_data)])\n")),(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre"},"100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 4348/4348 [00:00<00:00, 2353747.26it/s]\n")),(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre",className:"language-python"},"for epoch in range(30):\n    model_dbow.train(utils.shuffle([x for x in tqdm(all_data)]), total_examples=len(all_data), epochs=1)\n    model_dbow.alpha -= 0.002\n    model_dbow.min_alpha = model_dbow.alpha\n")),(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre",className:"language-python"},'def get_vectors(model, corpus_size, vectors_size, vectors_type):\n    """\n    Get vectors from trained doc2vec model\n    :param doc2vec_model: Trained Doc2Vec model\n    :param corpus_size: Size of the data\n    :param vectors_size: Size of the embedding vectors\n    :param vectors_type: Training or Testing vectors\n    :return: list of vectors\n    """\n    vectors = np.zeros((corpus_size, vectors_size))\n    for i in range(0, corpus_size):\n        prefix = vectors_type + \'_\' + str(i)\n        vectors[i] = model.docvecs[prefix]\n    return vectors\n')),(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre",className:"language-python"},"train_vectors_dbow = get_vectors(model_dbow, len(X_train), 300, 'Train')\ntest_vectors_dbow = get_vectors(model_dbow, len(X_test), 300, 'Test')\n")),(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre",className:"language-python"},"logreg = LogisticRegression(n_jobs=1, C=1e5)\nlogreg.fit(train_vectors_dbow, y_train)\n")),(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre"},"LogisticRegression(C=100000.0, class_weight=None, dual=False,\n                   fit_intercept=True, intercept_scaling=1, l1_ratio=None,\n                   max_iter=100, multi_class='auto', n_jobs=1, penalty='l2',\n                   random_state=None, solver='lbfgs', tol=0.0001, verbose=0,\n                   warm_start=False)\n")),(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre",className:"language-python"},"y_pred = logreg.predict(test_vectors_dbow)\n")),(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre",className:"language-python"},"print('accuracy %s' % accuracy_score(y_pred, y_test))\nprint(classification_report(y_test, y_pred,target_names=label_list))\n")),(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre",className:"language-python"},"import itertools\nimport tensorflow as tf\n\nfrom sklearn.preprocessing import LabelBinarizer, LabelEncoder\nfrom sklearn.metrics import confusion_matrix\n\nfrom tensorflow import keras\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Activation, Dropout\nfrom keras.preprocessing import text, sequence\nfrom keras import utils\n")),(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre",className:"language-python"},'train_size = int(len(df) * .7)\nprint ("Train size: %d" % train_size)\nprint ("Test size: %d" % (len(df) - train_size))\n')),(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre"},"Train size: 3043\nTest size: 1305\n")),(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre",className:"language-python"},"train_posts = df['text'][:train_size]\ntrain_tags = df['target'][:train_size]\n\ntest_posts = df['text'][train_size:]\ntest_tags = df['target'][train_size:]\n")),(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre",className:"language-python"},"max_words = 1000\ntokenize = text.Tokenizer(num_words=max_words, char_level=False)\n")),(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre",className:"language-python"},"tokenize.fit_on_texts(train_posts) # only fit on train\nx_train = tokenize.texts_to_matrix(train_posts)\nx_test = tokenize.texts_to_matrix(test_posts)\n")),(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre",className:"language-python"},"encoder = LabelEncoder()\nencoder.fit(train_tags)\ny_train = encoder.transform(train_tags)\ny_test = encoder.transform(test_tags)\n")),(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre",className:"language-python"},"num_classes = np.max(y_train) + 1\ny_train = utils.to_categorical(y_train, num_classes)\ny_test = utils.to_categorical(y_test, num_classes)\n")),(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre",className:"language-python"},"print('x_train shape:', x_train.shape)\nprint('x_test shape:', x_test.shape)\nprint('y_train shape:', y_train.shape)\nprint('y_test shape:', y_test.shape)\n")),(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre"},"x_train shape: (3043, 1000)\nx_test shape: (1305, 1000)\ny_train shape: (3043, 21)\ny_test shape: (1305, 21)\n")),(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre",className:"language-python"},"batch_size = 32\nepochs = 2\n")),(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre",className:"language-python"},"# Build the model\nmodel = Sequential()\nmodel.add(Dense(512, input_shape=(max_words,)))\nmodel.add(Activation('relu'))\nmodel.add(Dropout(0.5))\nmodel.add(Dense(num_classes))\nmodel.add(Activation('softmax'))\n\nmodel.compile(loss='categorical_crossentropy',\n              optimizer='adam',\n              metrics=['accuracy'])\n")),(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre",className:"language-python"},"history = model.fit(x_train, y_train,\n                    batch_size=batch_size,\n                    epochs=epochs,\n                    verbose=1,\n                    validation_split=0.1)\n")),(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre"},"Epoch 1/2\n86/86 [==============================] - 1s 7ms/step - loss: 2.3918 - accuracy: 0.3561 - val_loss: 1.8259 - val_accuracy: 0.5574\nEpoch 2/2\n86/86 [==============================] - 0s 5ms/step - loss: 1.4318 - accuracy: 0.6402 - val_loss: 1.2779 - val_accuracy: 0.6557\n")),(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre",className:"language-python"},"score = model.evaluate(x_test, y_test,\n                       batch_size=batch_size, verbose=1)\nprint('Test accuracy:', score[1])\n")),(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre"},"41/41 [==============================] - 0s 2ms/step - loss: 1.3431 - accuracy: 0.6368\nTest accuracy: 0.636781632900238\n")),(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre",className:"language-python"},"def checkinside(V, T):\n  lAB = ((V[1][0] - V[0][0])**2 + (V[1][1] - V[0][1])**2)**(0.5)\n  lBC = ((V[2][0] - V[1][0])**2 + (V[2][1] - V[1][1])**2)**(0.5)\n  uAB = ((V[1][0] - V[0][0]) / lAB, (V[1][1] - V[0][1]) / lAB)\n  uBC = ((V[2][0] - V[1][0]) / lBC, (V[2][1] - V[1][1]) / lBC)\n  BP = ((T[0][0] - V[1][0]), (T[0][1] - V[1][1]))\n  SignedDistABP = BP[0] * uAB[1] - BP[1] * uAB[0]\n  SignedDistBCP = - BP[0] * uBC[1] + BP[1] * uBC[0]\n  result = 'inside' if ((SignedDistABP*SignedDistBCP > 0) and \\\n                        (abs(SignedDistABP) <= lBC) and \\\n                        abs(SignedDistBCP) <= lAB) \\\n                        else 'not inside'\n  return result\n\nV = [(670273, 4879507), (677241, 4859302), (670388, 4856938), (663420, 4877144)]\nT = [(670831, 4867989), (675097, 4869543)]\nprint(checkinside(V,[T[0]]))\nprint(checkinside(V,[T[1]]))\n")),(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre"},"not inside\ninside\n")),(0,l.kt)("h2",{id:"fasttext-model"},"FastText model"),(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre",className:"language-python"},"!pip install -q clean-text[gpl] && cp '/content/drive/My Drive/clean_v2.py' .\n\nfrom clean_v2 import clean_l1\n")),(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre",className:"language-python"},"import os\nimport re\n\nimport numpy as np\nimport pandas as pd\nfrom tqdm import tqdm\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\ntqdm.pandas()\n%reload_ext autoreload\n%autoreload 2\n%reload_ext google.colab.data_table\n%config InlineBackend.figure_format = 'retina'\n\nplt.style.use('fivethirtyeight')\nplt.style.use('seaborn-notebook')\n")),(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre",className:"language-python"},"df_raw = pd.read_pickle(os.path.join(path,'data_clean_v2.p'))\ndf_raw.info()\n")),(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre"},"<class 'pandas.core.frame.DataFrame'>\nInt64Index: 824410 entries, 0 to 930912\nData columns (total 2 columns):\n #   Column  Non-Null Count   Dtype \n---  ------  --------------   ----- \n 0   text    824410 non-null  object\n 1   target  824410 non-null  object\ndtypes: object(2)\nmemory usage: 18.9+ MB\n")),(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre",className:"language-python"},"df = df_raw.sample(100000, random_state=42)\n")),(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre",className:"language-python"},"# !pip install fasttext\n")),(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre",className:"language-python"},"# preprocessing\n# lowercase\n# remove tokens\n# truncate post-regards\n# lower th remove\n# upper th truncate\n# clean categories\n# collate categories\n# train test split\n\n# tokenlist = ' '.join([i for i in df['text']]).split()\n# tokenlist = list(set([w for w in tokenlist if 'token' in w]))\ntokenlist = ['emailtoken', 'urltoken', 'htmltoken', 'currencytoken', 'token', 'digittoken', 'numbertoken']\n\ndef preprocess(text):\n  text = text.lower()\n  text = ' '.join([w for w in text.split() if w not in tokenlist])\n  text = ' '.join(text.split()[:50])\n  return text\n")),(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre",className:"language-python"},"print(tokenlist)\n")),(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre"},"['emailtoken', 'urltoken', 'htmltoken', 'currencytoken', 'token', 'digittoken', 'numbertoken']\n")),(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre",className:"language-python"},"# xx = df.sample()\n# xx\ndf['text'] = df.text.apply(preprocess)\ndf = df[df.text.apply(lambda x: len(x.split()))>3]\n# preprocess(xx.text.tolist()[0])\n")),(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre",className:"language-python"},"# from collections import OrderedDict\ndf['target'] = df.target.apply(clean_l1).str.lower().str.split().apply(lambda x: OrderedDict.fromkeys(x).keys()).str.join(' ')\n")),(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre",className:"language-python"},"df.sample(5)\n")),(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre",className:"language-python"},"df.target.value_counts()[:20]\n")),(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre",className:"language-python"},"# sns.distplot(df.target.value_counts())\n")),(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre",className:"language-python"},"minority_labels = df.target.value_counts()[df.target.value_counts()<100].index.tolist()\ndf['target'] = df.target.replace(dict.fromkeys(minority_labels, 'Other'))\n")),(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre",className:"language-python"},"df = df[df.target!='Other']\nsns.distplot(df.target.value_counts());\n")),(0,l.kt)("p",null,(0,l.kt)("img",{loading:"lazy",alt:"png",src:n(47596).Z,width:"1115",height:"731"})),(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre",className:"language-python"},"from sklearn.preprocessing import LabelEncoder\nlabel_encoder = LabelEncoder()\ndf['target'] = label_encoder.fit_transform(df.target)\n")),(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre",className:"language-python"},"df.isna().any()\n")),(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre"},"text      False\ntarget    False\ndtype: bool\n")),(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre",className:"language-python"},"df['target'] = ['__label__'+str(s) for s in df['target']]\ndf = df[['target','text']]\n")),(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre",className:"language-python"},"from sklearn.model_selection import train_test_split\ntrain, test = train_test_split(df, test_size=0.2, random_state=42)\n")),(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre",className:"language-python"},"import csv\ntrain.to_csv('train.txt', index=False, sep=' ', header=False, quoting=csv.QUOTE_NONE, quotechar=\"\", escapechar=\" \")\ntest.to_csv('test.txt', index=False, sep=' ', header=False, quoting=csv.QUOTE_NONE, quotechar=\"\", escapechar=\" \")\n")),(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre",className:"language-python"},"# train.sample(5).to_csv('sample.txt', index=False, sep=' ', header=False, quoting=csv.QUOTE_NONE, quotechar=\"\", escapechar=\" \")\n")),(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre",className:"language-python"},"# import fasttext\n# model = fasttext.train_supervised(input='train.txt', epoch=50) --\x3e (17874, 0.5248405505203089, 0.5248405505203089)\n# model = fasttext.train_supervised(input='train.txt', epoch=50, lr=0.5, wordNgrams=2, loss='hs') --\x3e (17874, 0.46620789974264293, 0.46620789974264293)\n# model = fasttext.train_supervised(input='train.txt', --\x3e (17874, 0.4858453619782925, 0.4858453619782925)\n#                                   epoch=25,\n#                                   lr=0.2,\n#                                   loss='hs',\n#                                   autotuneMetric='f1',\n#                                   verbose=5,\n#                                   minCount=10,\n#                                   )\n# model = fasttext.train_supervised(input='train.txt', --\x3e (17874, 0.5262392301667226, 0.5262392301667226)\n#                                   epoch=50,\n#                                   lr=0.1,\n#                                   loss='softmax',\n#                                   autotuneMetric='f1',\n#                                   verbose=5,\n#                                   minCount=20,\n#                                   )\nmodel = fasttext.train_supervised(input='train.txt', \n                                  epoch=50,\n                                  lr=0.1,\n                                  loss='softmax',\n                                  autotuneMetric='f1',\n                                  verbose=5,\n                                  minCount=20,\n                                  )\n")),(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre",className:"language-python"},'model.test("test.txt", k=1)\n')),(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre"},"(17874, 0.5262392301667226, 0.5262392301667226)\n")),(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre",className:"language-python"},'model.test("test.txt", k=5)\n')),(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre"},"(17874, 0.15201740752253654, 0.6840102942821976)\n")),(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre",className:"language-python"},"xx = df.sample(); xx\n")),(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre",className:"language-python"},"model.predict(xx.text.tolist()[0], k=5)\n")),(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre"},"(('__label__99', '__label__55', '__label__165', '__label__83', '__label__46'),\n array([1.00007915e+00, 1.08352187e-05, 1.00006037e-05, 1.00005846e-05,\n        1.00005755e-05]))\n")),(0,l.kt)("h2",{id:"pipeline"},"Pipeline"),(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre",className:"language-python"},"!pip install -q fast-bert\n!pip install -q fasttext\n!pip install -q clean-text[gpl]\n")),(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre",className:"language-python"},'# setup\nimport os\nimport pickle\nimport shutil\nimport warnings\nimport numpy as np\nimport pandas as pd\nfrom tqdm import tqdm\nfrom pathlib import Path\nfrom collections import OrderedDict\n\nwarnings.filterwarnings("ignore")\nPath(work_path).mkdir(parents=True, exist_ok=True)\nos.chdir(work_path)\n')),(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre",className:"language-python"},"shutil.copyfile(os.path.join(save_path,'utils_clean.py'), os.path.join(work_path,'utils_clean.py'))\nfrom utils_clean import clean_l1, clean_l2\n\nshutil.copyfile(os.path.join(save_path,'utils_preprocess.py'), os.path.join(work_path,'utils_preprocess.py'))\nfrom utils_preprocess import *\n\nshutil.copyfile(os.path.join(save_path,'label_encoder.p'), os.path.join(work_path,'label_encoder.p'))\nlabel_encoder = pickle.load(open('label_encoder.p', 'rb'))\n\nshutil.copyfile(os.path.join(save_path,'label_map.p'), os.path.join(work_path,'label_map.p'))\nlabel_map = pickle.load(open('label_map.p', 'rb'))\n\nimport fasttext\nshutil.copyfile(os.path.join(save_path,'fasttext.bin'), os.path.join(work_path,'fasttext.bin'))\nmodel_fasttext = fasttext.load_model('fasttext.bin')\n\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.metrics.pairwise import linear_kernel\nvectorizer = CountVectorizer(ngram_range=(1,1))\nshutil.copyfile(os.path.join(save_path,'model_countvectorizer_large.p'), os.path.join(work_path,'model_countvectorizer.p'))\nmodel_countvectorizer = pickle.load(open('model_countvectorizer.p', 'rb'))\ndtmx = model_countvectorizer['dtm']\nvectorizerx = model_countvectorizer['vectorizer']\ntarget_categories = model_countvectorizer['target_categories']\ntarget_labels = model_countvectorizer['target_labels']\n\nshutil.copyfile(os.path.join(save_path,'model_tfidf_large.p'), os.path.join(work_path,'model_tfidf.p'))\nmodel_tfidf = pickle.load(open(os.path.join(work_path,'model_tfidf.p'), 'rb'))\n\nfrom fast_bert.prediction import BertClassificationPredictor\nshutil.unpack_archive(os.path.join(save_path,'fastbert_large_iter2.zip'), work_path, 'zip')\nMODEL_PATH = os.path.join(work_path, 'model_out')\nmodel_fastbert = BertClassificationPredictor(\n                    model_path=MODEL_PATH,\n                    label_path=work_path,\n                    multi_label=False,\n                    model_type='distilbert',\n                    do_lower_case=True)\n")),(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre",className:"language-python"},"X = pd.DataFrame(label_map, index=[0]).T.reset_index()\nX.columns = ['Raw_data_labels','processed_labels']\nX.to_csv('')\n")),(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre",className:"language-python"},"df_raw = pd.read_pickle(os.path.join('/content/drive/My Drive/','data','raw','data_raw.p'))\ndf_raw.info()\n")),(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre"},"<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 930913 entries, 0 to 930912\nData columns (total 4 columns):\n #   Column        Non-Null Count   Dtype \n---  ------        --------------   ----- \n 0   tSubject      930913 non-null  object\n 1   mMsgContent   716024 non-null  object\n 2   QueryType     930913 non-null  object\n 3   SubQueryType  930913 non-null  object\ndtypes: object(4)\nmemory usage: 28.4+ MB\n")),(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre",className:"language-python"},"df_raw['labels_raw'] = df_raw['QueryType'] + ' | ' + df_raw['SubQueryType']\ndf_raw['labels_raw'].value_counts().to_csv('labels_rawdata.csv')\n")),(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre",className:"language-python"},"# test_set = df_raw.sample(50000, random_state=10)\n# def preprocess(X):\n#   X = X.drop_duplicates()\n#   X = X.dropna(subset=['tSubject', 'mMsgContent'], how='all')\n#   X['type'] = X['QueryType'] + ' | ' + X['SubQueryType']\n#   X['type'] = X['type'].fillna(' ')\n#   X['type_orig'] = X['type']\n#   X['type'] = X['type'].apply(clean_l1).str.lower().str.split().apply(lambda x: OrderedDict.fromkeys(x).keys()).str.join(' ').apply(clean_l1)\n#   X = X[X['type'].isin(list(label_encoder.classes_))]\n#   X['subj&msg'] = X['tSubject'] + ' sub_eos_token ' + X['mMsgContent']\n#   X = X[['subj&msg','type', 'type_orig']]\n#   X.columns = ['text','target', 'type_orig']\n#   X = X.dropna()\n#   return X\n# test_set = preprocess(test_set)\n# test_set.describe()\n\n# label_map = test_set[['type_orig','target']].set_index('type_orig').to_dict()['target']\n# pickle.dump(label_map, open(os.path.join(#save_path, 'label_map.p'), 'wb'))\n")),(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre",className:"language-python"},"# test_set = df_raw.sample(10000, random_state=10)\n# def preprocess(X):\n#   X = X.drop_duplicates()\n#   X = X.dropna(subset=['tSubject', 'mMsgContent'], how='all')\n#   X['type'] = X['QueryType'] + ' | ' + X['SubQueryType']\n#   X['type'] = X['type'].fillna(' ')\n#   X['type'] = X['type'].apply(clean_l1).str.lower().str.split().apply(lambda x: OrderedDict.fromkeys(x).keys()).str.join(' ').apply(clean_l1)\n#   X = X[X['type'].isin(list(label_encoder.classes_))]\n#   X['subj&msg'] = X['tSubject'] + ' sub_eos_token ' + X['mMsgContent']\n#   X = X[['subj&msg','type']]\n#   X.columns = ['text','target']\n#   X = X.dropna()\n#   return X\n# test_set = preprocess(test_set)\n# test_set.describe()\n")),(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre",className:"language-python"},"  ##### <----- freezed backup -----\x3e #####\n\n# def predict_fasttext(text):\n#   text = clean_l1(text)\n#   text = preprocess_fasttext(text)\n#   preds = model_fasttext.predict(text, k=-1)\n#   label_names = label_encoder.inverse_transform([int(x.split('__')[-1]) for x in preds[0]])\n#   preds = [(x,y) for x,y in zip(label_names,preds[1])]\n#   return preds\n\n# def predict_fastbert(text):\n#   text = clean_l1(text)\n#   text = preprocess_fastbert(text)\n#   preds = model_fastbert.predict(text)\n#   preds = [(label_encoder.inverse_transform([int(x[0])])[0],x[1]) for x in preds]\n#   return preds\n\n# def predict_countvect(text):\n#   text = clean_l1(text)\n#   text = clean_l2(text)\n#   text = preprocess_countvectorizer(text)\n#   cosim = linear_kernel(vectorizerx.transform([text]), dtmx).flatten()\n#   preds = [(target_categories[i],cosim[i]) for i in range(len(cosim))]\n#   preds = [target_categories[x] for x in np.argsort(-cosim)[:20]]\n#   return preds\n\n# def predict_tfidf(text):\n#   text = clean_l1(text)\n#   text = clean_l2(text)\n#   preds = model_tfidf.predict_proba([text])[0]\n#   preds = [(label_encoder.inverse_transform([int(x)])[0],y) for x,y in zip(model_tfidf.classes_, preds)]\n#   preds.sort(key = lambda x: x[1], reverse=True)  \n#   return preds\n\n\n# query = test_set.sample()\n# print('Text: ',query.text.values[0])\n# print('Actual Label: ',query.target.values[0])\n# print('Predicted Labels: ')\n# pred1 = predict_fasttext(query.text.values[0])\n# pred2 = predict_fastbert(query.text.values[0])\n# pred3 = predict_countvect(query.text.values[0])\n# pred4 = predict_tfidf(query.text.values[0])\n")),(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre",className:"language-python"},"# lmr = {label_map[v]:v for v in label_map.keys()}\n")),(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre",className:"language-python"},"def predict_fasttext(text):\n  text = clean_l1(text)\n  text = preprocess_fasttext(text)\n  preds = model_fasttext.predict(text, k=-1)\n  preds = [int(x.split('__')[-1]) for x in preds[0]]\n  preds = pd.DataFrame([(x,(1/(i+1))) for i,x in enumerate(preds)],\n                       columns=['label','rank_fasttext']).set_index('label')\n  return preds\n\ndef predict_fastbert(text):\n  text = clean_l1(text)\n  text = preprocess_fastbert(text)\n  preds = model_fastbert.predict(text)\n  preds = pd.DataFrame([(int(x[0]),(1/(i+1))) for i,x in enumerate(preds)],\n                      columns=['label','rank_fastbert']).set_index('label')\n  return preds\n\ndef predict_tfidf(text):\n  text = clean_l1(text)\n  text = clean_l2(text)\n  preds = model_tfidf.predict_proba([text])[0]\n  preds = [(int(x),y) for x,y in zip(model_tfidf.classes_, preds)]\n  preds.sort(key = lambda x: x[1], reverse=True)\n  preds = pd.DataFrame([(int(x[0]),(1/(i+1))) for i,x in enumerate(preds)],\n                    columns=['label','rank_tfidf']).set_index('label')\n  return preds\n\ndef predict_countvect(text):\n  text = clean_l1(text)\n  text = clean_l2(text)\n  text = preprocess_countvectorizer(text)\n  cosim = linear_kernel(vectorizerx.transform([text]), dtmx).flatten()\n  preds = [(int(target_categories[i]),cosim[i]) for i in range(len(cosim))]\n  preds.sort(key = lambda x: x[1], reverse=True)\n  preds = pd.DataFrame([(int(x[0]),(1/(i+1))) for i,x in enumerate(preds)],\n                  columns=['label','rank_cvt']).set_index('label')\n  return preds\n\nmodel_weight = {'fasttext':10, 'fastbert':5, 'tfidf':3, 'cvt':2}\n\ndef predict(text):\n  pred = predict_fasttext(text)\n  pred = pred.join(predict_fastbert(text), on='label')\n  pred = pred.join(predict_tfidf(text), on='label')\n  pred = pred.join(predict_countvect(text), on='label')\n  pred['score'] = (pred['rank_fasttext']*model_weight['fasttext']) + \\\n  (pred['rank_fastbert']*model_weight['fastbert']) + \\\n  (pred['rank_tfidf']*model_weight['tfidf']) + \\\n  (pred['rank_cvt']*model_weight['cvt'])\n  pred = pred.sort_values(by='score', ascending=False)\n  return pred\n\ndef predict(text):\n  pred = predict_fasttext(text)\n  pred = pred.join(predict_tfidf(text), on='label')\n  pred = pred.join(predict_countvect(text), on='label')\n  pred['score'] = (pred['rank_fasttext']*model_weight['fasttext']) + \\\n  (pred['rank_tfidf']*model_weight['tfidf']) + \\\n  (pred['rank_cvt']*model_weight['cvt'])\n  pred = pred.sort_values(by='score', ascending=False)\n  return pred\n")),(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre",className:"language-python"},"testx = pd.read_excel(os.path.join(save_path,'TSD_v1.xlsx'), sheet_name='Database records')\ntestx.head()\n")),(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre",className:"language-python"},"print(testx.info())\nX = testx[(testx['OriginalQuery']+ ' | '+testx['OriginalSubQuery']).isin(list(set(label_map.keys())))]\nprint(X.info())\nX.head()\n")),(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre",className:"language-python"},"X['text'] = X['tSubject'] + ' sub_eos_token ' + X['mMsgContent']\nX['label'] = X['OriginalQuery'] + ' | '+ X['OriginalSubQuery']\nX['plabel'] = X['label'].apply(lambda x: label_map[x])\nX['clabel'] = label_encoder.transform(X.plabel)\nX = X[['text','clabel']]\nprint(X.info())\nX = X.dropna().drop_duplicates()\nprint(X.info())\nX.head()\n")),(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre",className:"language-python"},"from tqdm import tqdm\ntqdm.pandas()\ntop1 = top2 = top3 = 0\nfor index, row in tqdm(X.iterrows(), total=X.shape[0]):\n  text = row.text\n  label = row.clabel\n  preds = predict(text).index.tolist()[:3]\n  if label==preds[0]:\n    top1+=1\n  elif label==preds[1]:\n    top2+=1\n  elif label==preds[2]:\n    top3+=1\nprint(top1, top2, top3)\n")),(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre"},"100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1800/1800 [30:07<00:00,  1.00s/it]\n\n1131 258 93\n")),(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre",className:"language-python"},"top1p = top1/X.shape[0]\ntop2p = top1p + top2/X.shape[0]\ntop3p = top2p + top3/X.shape[0]\n\nprint(top1p, top2p, top3p)\n")),(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre"},"0.6344444444444445 0.7555555555555555 0.8061111111111111\n")),(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre",className:"language-python"},"query = test_set.sample()\n# print('Text: ',query.text.values[0])\nprint('Actual Label: ',query.target.values[0])\nprint('Actual Label: ',label_encoder.transform([query.target.values[0]]))\npredict(query.text.values[0]).head()\n")),(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre"},"Actual Label:  drf requisition status\nActual Label:  [80]\n")),(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre",className:"language-python"},"test_set['target_cat'] = label_encoder.transform(test_set.target)\ntest_set.head()\n")),(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre",className:"language-python"},"from tqdm import tqdm\ntqdm.pandas()\ntop1 = top2 = top3 = 0\nfor index, row in tqdm(test_set.iterrows(), total=test_set.shape[0]):\n  text = row.text\n  label = row.target_cat\n  preds = predict(text).index.tolist()[:3]\n  if label==preds[0]:\n    top1+=1\n  elif label==preds[1]:\n    top2+=1\n  elif label==preds[2]:\n    top3+=1\nprint(top1, top2, top3)\n")),(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre"},"100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 7183/7183 [2:00:36<00:00,  1.01s/it]\n\n5889 664 151\n")),(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre",className:"language-python"},"top1p = top1/test_set.shape[0]\ntop2p = top1p + top2/test_set.shape[0]\ntop3p = top2p + top3/test_set.shape[0]\n\nprint(top1p, top2p, top3p)\n")),(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre"},"0.8198524293470695 0.9122929138243074 0.9333147709870528\n")),(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre",className:"language-python"},"def func(subj, msg):\n  text = str(subj) + ' sub_eos_token ' + str(msg)\n  preds = predict(text).head(3)\n  preds.index = label_encoder.inverse_transform(preds.index.tolist())\n  preds = preds.rename_axis('label').reset_index()[['label','score']]\n  preds.label = preds.label.apply(lambda x: label_map[x])\n  preds = preds.T.to_dict()\n  preds = str(preds)\n  return preds\n")),(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre",className:"language-python"},"x = test_set.sample()\nx\n")),(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre",className:"language-python"},"x.text.tolist()[0]\n")),(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre",className:"language-python"},"subj = '<redacted>'\nmsg = '<redacted>'\n")),(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre",className:"language-python"},"xx = func(subj, msg)\nxx\n")),(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre",className:"language-python"},"xx = func(subj, msg)\nxx\n")),(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre",className:"language-python"},"pd.Series(test_set['type_orig'].unique()).to_csv('label_list.csv', index=False)\n")),(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre",className:"language-python"},"!pip install -q gradio\nimport gradio as gr\n")),(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre",className:"language-python"},"gr.Interface(func, \n             [\n              gr.inputs.Textbox(lines=2, label='Email Subject'),\n              gr.inputs.Textbox(lines=10, label='Email Body'),\n             ],\n             gr.outputs.Textbox(label='Output Labels')).launch();\n")),(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre",className:"language-python"},"xx = pd.DataFrame(label_map, index=[0]).T.reset_index()\nxx.columns = ['plabel', 'olabel']\nxx.head()\n")),(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre",className:"language-python"},"xx.olabel.value_counts()\n")))}u.isMDXComponent=!0},98671:function(e,t,n){t.Z=n.p+"assets/images/260222_email_classification_1-a8ac49b62bbfb39a5e756d81f7e5a652.png"},94261:function(e,t,n){t.Z=n.p+"assets/images/260222_email_classification_2-a42172433a35c8a6d50f80310a52ee4b.png"},95155:function(e,t,n){t.Z=n.p+"assets/images/260222_email_classification_3-10b74145090b4e31140075f72ed20f61.png"},47596:function(e,t,n){t.Z=n.p+"assets/images/x_197_0-6558151b47db06d02ec6f2da86a39178.png"}}]);