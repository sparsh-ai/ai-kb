"use strict";(self.webpackChunkdocs=self.webpackChunkdocs||[]).push([[7192],{3905:function(e,t,n){n.d(t,{Zo:function(){return d},kt:function(){return p}});var o=n(67294);function r(e,t,n){return t in e?Object.defineProperty(e,t,{value:n,enumerable:!0,configurable:!0,writable:!0}):e[t]=n,e}function a(e,t){var n=Object.keys(e);if(Object.getOwnPropertySymbols){var o=Object.getOwnPropertySymbols(e);t&&(o=o.filter((function(t){return Object.getOwnPropertyDescriptor(e,t).enumerable}))),n.push.apply(n,o)}return n}function i(e){for(var t=1;t<arguments.length;t++){var n=null!=arguments[t]?arguments[t]:{};t%2?a(Object(n),!0).forEach((function(t){r(e,t,n[t])})):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(n)):a(Object(n)).forEach((function(t){Object.defineProperty(e,t,Object.getOwnPropertyDescriptor(n,t))}))}return e}function l(e,t){if(null==e)return{};var n,o,r=function(e,t){if(null==e)return{};var n,o,r={},a=Object.keys(e);for(o=0;o<a.length;o++)n=a[o],t.indexOf(n)>=0||(r[n]=e[n]);return r}(e,t);if(Object.getOwnPropertySymbols){var a=Object.getOwnPropertySymbols(e);for(o=0;o<a.length;o++)n=a[o],t.indexOf(n)>=0||Object.prototype.propertyIsEnumerable.call(e,n)&&(r[n]=e[n])}return r}var s=o.createContext({}),c=function(e){var t=o.useContext(s),n=t;return e&&(n="function"==typeof e?e(t):i(i({},t),e)),n},d=function(e){var t=c(e.components);return o.createElement(s.Provider,{value:t},e.children)},u={inlineCode:"code",wrapper:function(e){var t=e.children;return o.createElement(o.Fragment,{},t)}},m=o.forwardRef((function(e,t){var n=e.components,r=e.mdxType,a=e.originalType,s=e.parentName,d=l(e,["components","mdxType","originalType","parentName"]),m=c(n),p=r,v=m["".concat(s,".").concat(p)]||m[p]||u[p]||a;return n?o.createElement(v,i(i({ref:t},d),{},{components:n})):o.createElement(v,i({ref:t},d))}));function p(e,t){var n=arguments,r=t&&t.mdxType;if("string"==typeof e||r){var a=n.length,i=new Array(a);i[0]=m;var l={};for(var s in t)hasOwnProperty.call(t,s)&&(l[s]=t[s]);l.originalType=e,l.mdxType="string"==typeof e?e:r,i[1]=l;for(var c=2;c<a;c++)i[c]=n[c];return o.createElement.apply(null,i)}return o.createElement.apply(null,n)}m.displayName="MDXCreateElement"},91853:function(e,t,n){n.r(t),n.d(t,{assets:function(){return d},contentTitle:function(){return s},default:function(){return p},frontMatter:function(){return l},metadata:function(){return c},toc:function(){return u}});var o=n(87462),r=n(63366),a=(n(67294),n(3905)),i=["components"],l={},s="Text Embeddings",c={unversionedId:"tutorials/text-embeddings",id:"tutorials/text-embeddings",title:"Text Embeddings",description:"Utilities",source:"@site/docs/04-tutorials/text-embeddings.md",sourceDirName:"04-tutorials",slug:"/tutorials/text-embeddings",permalink:"/ai-kb/docs/tutorials/text-embeddings",editUrl:"https://github.com/sparsh-ai/ai-kb/docs/04-tutorials/text-embeddings.md",tags:[],version:"current",frontMatter:{},sidebar:"tutorialSidebar",previous:{title:"Text Cleaning",permalink:"/ai-kb/docs/tutorials/text-cleaning"},next:{title:"Text Processing",permalink:"/ai-kb/docs/tutorials/text-processing"}},d={},u=[{value:"Utilities",id:"utilities",level:2},{value:"Averaging word vectors to create sentence vector",id:"averaging-word-vectors-to-create-sentence-vector",level:3},{value:"Word2vec",id:"word2vec",level:2},{value:"Training a custom Word2vec CBoW model with Gensim",id:"training-a-custom-word2vec-cbow-model-with-gensim",level:3},{value:"Training a custom Word2vec SkipGram model with Gensim",id:"training-a-custom-word2vec-skipgram-model-with-gensim",level:3},{value:"GloVe",id:"glove",level:2},{value:"TF-IDF",id:"tf-idf",level:2},{value:"BERT",id:"bert",level:2},{value:"Sentence BERT",id:"sentence-bert",level:3}],m={toc:u};function p(e){var t=e.components,l=(0,r.Z)(e,i);return(0,a.kt)("wrapper",(0,o.Z)({},m,l,{components:t,mdxType:"MDXLayout"}),(0,a.kt)("h1",{id:"text-embeddings"},"Text Embeddings"),(0,a.kt)("h2",{id:"utilities"},"Utilities"),(0,a.kt)("h3",{id:"averaging-word-vectors-to-create-sentence-vector"},"Averaging word vectors to create sentence vector"),(0,a.kt)("p",null,"In case we already have the vectors for the words in the text, it makes sense to aggregate the word embeddings into a single vector representing the whole text."),(0,a.kt)("p",null,(0,a.kt)("img",{loading:"lazy",src:n(87071).Z,width:"597",height:"231"})),(0,a.kt)("p",null,"This is a great baseline approach chosen by many practitioners, and probably the one we should take first if we already have the word vectors or can easily obtain them."),(0,a.kt)("p",null,"The most frequent operations for aggregation are:"),(0,a.kt)("ol",null,(0,a.kt)("li",{parentName:"ol"},"averaging"),(0,a.kt)("li",{parentName:"ol"},"max-pooling")),(0,a.kt)("pre",null,(0,a.kt)("code",{parentName:"pre",className:"language-py"},"def get_mean_vector(words, word2vec_model):\n    # remove out-of-vocabulary words\n    words = [word for word in words if word in word2vec_model.wv.vocab]\n    if len(words) >= 1:\n        return np.mean(word2vec_model.wv[words], axis=0)\n    else:\n        return []\n")),(0,a.kt)("h2",{id:"word2vec"},"Word2vec"),(0,a.kt)("h3",{id:"training-a-custom-word2vec-cbow-model-with-gensim"},"Training a custom Word2vec CBoW model with Gensim"),(0,a.kt)("pre",null,(0,a.kt)("code",{parentName:"pre",className:"language-py"},"model_cbow = gensim.models.Word2Vec(window=10, min_count=10, workers=2, size=100)\nmodel_cbow.build_vocab(df.tokens.tolist())\nmodel_cbow.train(df.tokens.tolist(), total_examples=model_cbow.corpus_count, epochs=20)\ndf['vecs_w2v_cbow'] = df['tokens'].apply(get_mean_vector, word2vec_model=model_cbow)\ndf.head()\n")),(0,a.kt)("h3",{id:"training-a-custom-word2vec-skipgram-model-with-gensim"},"Training a custom Word2vec SkipGram model with Gensim"),(0,a.kt)("pre",null,(0,a.kt)("code",{parentName:"pre",className:"language-py"},"model_sg = gensim.models.Word2Vec(window=10, min_count=10, workers=2, size=100, sg=1)\nmodel_sg.build_vocab(df.tokens.tolist())\nmodel_sg.train(df.tokens.tolist(), total_examples=model_sg.corpus_count, epochs=20)\ndf['vecs_w2v_sg'] = df['tokens'].apply(get_mean_vector, word2vec_model=model_sg)\ndf.head()\n")),(0,a.kt)("h2",{id:"glove"},"GloVe"),(0,a.kt)("p",null,"Files with the pre-trained vectors Glove can be found in many sites like Kaggle or in the previous link of the Stanford University. We will use the glove.6B.100d.txt file containing the glove vectors trained on the Wikipedia and GigaWord dataset."),(0,a.kt)("p",null,"First we convert the GloVe file containing the word embeddings to the word2vec format for convenience of use. We can do it using the gensim library, a function called glove2word2vec."),(0,a.kt)("pre",null,(0,a.kt)("code",{parentName:"pre",className:"language-sh"},"wget -O glove.6B.zip -q --show-progress https://nlp.stanford.edu/data/glove.6B.zip\nunzip glove.6B.zip\n")),(0,a.kt)("pre",null,(0,a.kt)("code",{parentName:"pre",className:"language-py"},"# We just need to run this code once, the function glove2word2vec saves the Glove embeddings in the word2vec format \n# that will be loaded in the next section\nfrom gensim.scripts.glove2word2vec import glove2word2vec\n\n# glove_input_file = glove_filename\nword2vec_output_file = 'glove.word2vec'\nglove2word2vec('glove.6B.100d.txt', word2vec_output_file)\n")),(0,a.kt)("p",null,"So our vocabulary contains 400K words represented by a feature vector of shape 100. Now we can load the Glove embeddings in word2vec format and then analyze some analogies. In this way if we want to use a pre-trained word2vec embeddings we can simply change the filename and reuse all the code below."),(0,a.kt)("pre",null,(0,a.kt)("code",{parentName:"pre",className:"language-py"},"# load the Stanford GloVe model\nmodel_glove = KeyedVectors.load_word2vec_format(word2vec_output_file, binary=False)\n\ndf['vecs_w2v_glove'] = df['tokens'].apply(get_mean_vector, word2vec_model=model_glove)\ndf.head()\n")),(0,a.kt)("h2",{id:"tf-idf"},"TF-IDF"),(0,a.kt)("p",null,"The TfidfVectorizer will tokenize documents, learn the vocabulary and inverse document frequency weightings, and allow you to encode new documents. Alternately, if you already have a learned CountVectorizer, you can use it with a TfidfTransformer to just calculate the inverse document frequencies and start encoding documents."),(0,a.kt)("p",null,"Counts and frequencies can be very useful, but one limitation of these methods is that the vocabulary can become very large."),(0,a.kt)("p",null,"This, in turn, will require large vectors for encoding documents and impose large requirements on memory and slow down algorithms."),(0,a.kt)("p",null,"A clever work around is to use a one way hash of words to convert them to integers. The clever part is that no vocabulary is required and you can choose an arbitrary-long fixed length vector. A downside is that the hash is a one-way function so there is no way to convert the encoding back to a word (which may not matter for many supervised learning tasks)."),(0,a.kt)("p",null,"The HashingVectorizer class implements this approach that can be used to consistently hash words, then tokenize and encode documents as needed."),(0,a.kt)("pre",null,(0,a.kt)("code",{parentName:"pre",className:"language-py"},"from sklearn.feature_extraction.text import HashingVectorizer\n\n# create the transform\nvectorizer = HashingVectorizer(n_features=100)\n\n# encode document\nvector = vectorizer.transform(df.clean_text.tolist())\n\n# summarize encoded vector\nprint(vector.shape)\n\ndf['vecs_tfidf'] = list(vector.toarray())\ndf.head()\n")),(0,a.kt)("h2",{id:"bert"},"BERT"),(0,a.kt)("h3",{id:"sentence-bert"},"Sentence BERT"),(0,a.kt)("pre",null,(0,a.kt)("code",{parentName:"pre",className:"language-py"},"from sentence_transformers import SentenceTransformer\n\nsbert_model = SentenceTransformer('bert-base-nli-mean-tokens')\n\ndf['vecs_bert'] = df['clean_text'].progress_apply(sbert_model.encode)\n")))}p.isMDXComponent=!0},87071:function(e,t,n){t.Z=n.p+"assets/images/nlp_vec_avg-24152a7e9e98f2e56d7522eef666b863.png"}}]);