"use strict";(self.webpackChunkdocs=self.webpackChunkdocs||[]).push([[3573],{3905:function(e,t,n){n.d(t,{Zo:function(){return c},kt:function(){return u}});var o=n(67294);function i(e,t,n){return t in e?Object.defineProperty(e,t,{value:n,enumerable:!0,configurable:!0,writable:!0}):e[t]=n,e}function s(e,t){var n=Object.keys(e);if(Object.getOwnPropertySymbols){var o=Object.getOwnPropertySymbols(e);t&&(o=o.filter((function(t){return Object.getOwnPropertyDescriptor(e,t).enumerable}))),n.push.apply(n,o)}return n}function a(e){for(var t=1;t<arguments.length;t++){var n=null!=arguments[t]?arguments[t]:{};t%2?s(Object(n),!0).forEach((function(t){i(e,t,n[t])})):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(n)):s(Object(n)).forEach((function(t){Object.defineProperty(e,t,Object.getOwnPropertyDescriptor(n,t))}))}return e}function r(e,t){if(null==e)return{};var n,o,i=function(e,t){if(null==e)return{};var n,o,i={},s=Object.keys(e);for(o=0;o<s.length;o++)n=s[o],t.indexOf(n)>=0||(i[n]=e[n]);return i}(e,t);if(Object.getOwnPropertySymbols){var s=Object.getOwnPropertySymbols(e);for(o=0;o<s.length;o++)n=s[o],t.indexOf(n)>=0||Object.prototype.propertyIsEnumerable.call(e,n)&&(i[n]=e[n])}return i}var p=o.createContext({}),l=function(e){var t=o.useContext(p),n=t;return e&&(n="function"==typeof e?e(t):a(a({},t),e)),n},c=function(e){var t=l(e.components);return o.createElement(p.Provider,{value:t},e.children)},m={inlineCode:"code",wrapper:function(e){var t=e.children;return o.createElement(o.Fragment,{},t)}},d=o.forwardRef((function(e,t){var n=e.components,i=e.mdxType,s=e.originalType,p=e.parentName,c=r(e,["components","mdxType","originalType","parentName"]),d=l(n),u=i,h=d["".concat(p,".").concat(u)]||d[u]||m[u]||s;return n?o.createElement(h,a(a({ref:t},c),{},{components:n})):o.createElement(h,a({ref:t},c))}));function u(e,t){var n=arguments,i=t&&t.mdxType;if("string"==typeof e||i){var s=n.length,a=new Array(s);a[0]=d;var r={};for(var p in t)hasOwnProperty.call(t,p)&&(r[p]=t[p]);r.originalType=e,r.mdxType="string"==typeof e?e:i,a[1]=r;for(var l=2;l<s;l++)a[l]=n[l];return o.createElement.apply(null,a)}return o.createElement.apply(null,n)}d.displayName="MDXCreateElement"},70798:function(e,t,n){n.r(t),n.d(t,{assets:function(){return c},contentTitle:function(){return p},default:function(){return u},frontMatter:function(){return r},metadata:function(){return l},toc:function(){return m}});var o=n(87462),i=n(63366),s=(n(67294),n(3905)),a=["components"],r={},p="Pose Estimation",l={unversionedId:"concepts/vision/pose-estimation",id:"concepts/vision/pose-estimation",title:"Pose Estimation",description:"/img/content-concepts-raw-computer-vision-pose-estimation-slide52.png",source:"@site/docs/03-concepts/vision/pose-estimation.mdx",sourceDirName:"03-concepts/vision",slug:"/concepts/vision/pose-estimation",permalink:"/ai-kb/docs/concepts/vision/pose-estimation",editUrl:"https://github.com/sparsh-ai/ai-kb/docs/03-concepts/vision/pose-estimation.mdx",tags:[],version:"current",frontMatter:{},sidebar:"tutorialSidebar",previous:{title:"Object Tracking",permalink:"/ai-kb/docs/concepts/vision/object-tracking"},next:{title:"Scene Text Recognition",permalink:"/ai-kb/docs/concepts/vision/scene-text-recognition"}},c={},m=[{value:"Introduction",id:"introduction",level:2},{value:"Models",id:"models",level:2},{value:"OpenPose",id:"openpose",level:3},{value:"PoseNet",id:"posenet",level:3},{value:"Process flow",id:"process-flow",level:2},{value:"Use Cases",id:"use-cases",level:2},{value:"OpenPose Experiments",id:"openpose-experiments",level:3},{value:"Pose Estimation Inference Experiments",id:"pose-estimation-inference-experiments",level:3},{value:"Pose Detection on the Edge",id:"pose-detection-on-the-edge",level:3},{value:"Pose Detection on the Edge using OpenVINO",id:"pose-detection-on-the-edge-using-openvino",level:3}],d={toc:m};function u(e){var t=e.components,r=(0,i.Z)(e,a);return(0,s.kt)("wrapper",(0,o.Z)({},d,r,{components:t,mdxType:"MDXLayout"}),(0,s.kt)("h1",{id:"pose-estimation"},"Pose Estimation"),(0,s.kt)("p",null,(0,s.kt)("img",{loading:"lazy",alt:"/img/content-concepts-raw-computer-vision-pose-estimation-slide52.png",src:n(93230).Z,width:"960",height:"720"})),(0,s.kt)("p",null,(0,s.kt)("img",{loading:"lazy",alt:"/img/content-concepts-raw-computer-vision-pose-estimation-img.png",src:n(79168).Z,width:"960",height:"720"})),(0,s.kt)("h2",{id:"introduction"},"Introduction"),(0,s.kt)("ul",null,(0,s.kt)("li",{parentName:"ul"},(0,s.kt)("strong",{parentName:"li"},"Definition:")," Pose estimation is a computer vision task that infers the pose of a person or object in an image or video. This is typically done by identifying, locating, and tracking the number of\xa0key points\xa0on a given object or person. For objects, this could be corners or other significant features. And for humans, these key points represent major joints like an elbow or knee."),(0,s.kt)("li",{parentName:"ul"},(0,s.kt)("strong",{parentName:"li"},"Applications:")," Activity recognition, motion capture, fall detection, plank pose corrector, yoga pose identifier, body ration estimation"),(0,s.kt)("li",{parentName:"ul"},(0,s.kt)("strong",{parentName:"li"},"Scope:")," 2D skeleton map, Human Poses, Single and Multi-pose, Real-time"),(0,s.kt)("li",{parentName:"ul"},(0,s.kt)("strong",{parentName:"li"},"Tools:")," Tensorflow PoseNet API")),(0,s.kt)("h2",{id:"models"},"Models"),(0,s.kt)("h3",{id:"openpose"},"OpenPose"),(0,s.kt)("p",null,(0,s.kt)("em",{parentName:"p"},(0,s.kt)("a",{parentName:"em",href:"https://arxiv.org/abs/1812.08008"},"OpenPose: Realtime Multi-Person 2D Pose Estimation using Part Affinity Fields. arXiv, 2016."))),(0,s.kt)("p",null,"A standard bottom-up model that supports real-time multi-person 2D pose estimation. The authors of the paper have shared two models \u2013 one is trained on the Multi-Person Dataset ( MPII ) and the other is trained on the COCO dataset. The COCO model produces 18 points, while the MPII model outputs 15 points. "),(0,s.kt)("h3",{id:"posenet"},"PoseNet"),(0,s.kt)("p",null,"PoseNet is a machine learning model that allows for Real-time Human Pose Estimation. PoseNet can be used to estimate either a single pose or multiple poses PoseNet v1 is trained on MobileNet backbone and v2 on ResNet backbone. "),(0,s.kt)("h2",{id:"process-flow"},"Process flow"),(0,s.kt)("p",null,"Step 1: Collect Images"),(0,s.kt)("p",null,"Capture via camera, scrap from the internet or use public datasets"),(0,s.kt)("p",null,"Step 2: Create Labels"),(0,s.kt)("p",null,"Use a pre-trained model like PoseNet, OpenPose to identify the key points. These key points are our labels for pose estimation based classification task. If the model is not compatible/available for the required key points (e.g. identify the cap and bottom of a bottle product to measure if manufacturing is correct), we have to first train a pose estimation model using transfer learning in that case (this is out of scope though, as we are only focusing on human poses and pre-trained models are already available for this use case)"),(0,s.kt)("p",null,"Step 3: Data Preparation"),(0,s.kt)("p",null,"Setup the database connection and fetch the data into the environment. Explore the data, validate it, and create a preprocessing strategy. Clean the data and make it ready for modeling"),(0,s.kt)("p",null,"Step 4: Model Building"),(0,s.kt)("p",null,"Create the model architecture in python and perform a sanity check. Start the training process and track the progress and experiments. Validate the final set of models and select/assemble the final model"),(0,s.kt)("p",null,"Step 5: UAT Testing"),(0,s.kt)("p",null,"Wrap the model inference engine in API for client testing"),(0,s.kt)("p",null,"Step 6: Deployment"),(0,s.kt)("p",null,"Deploy the model on cloud or edge as per the requirement"),(0,s.kt)("p",null,"Step 7: Documentation"),(0,s.kt)("p",null,"Prepare the documentation and transfer all assets to the client  "),(0,s.kt)("h2",{id:"use-cases"},"Use Cases"),(0,s.kt)("h3",{id:"openpose-experiments"},"OpenPose Experiments"),(0,s.kt)("p",null,"Four types of experiments with pre-trained OpenPose model - Single and Multi-Person Pose Estimation with OpenCV, Multi-Person Pose Estimation with PyTorch and Pose Estimation on Videos. Check out ",(0,s.kt)("a",{parentName:"p",href:"https://www.notion.so/Pose-Estimation-with-OpenPose-2D8F5-7f01bee1534243f3836728d03a419969"},"this")," notion."),(0,s.kt)("h3",{id:"pose-estimation-inference-experiments"},"Pose Estimation Inference Experiments"),(0,s.kt)("p",null,"Experimented with pre-trained pose estimation models. Check out ",(0,s.kt)("a",{parentName:"p",href:"https://www.notion.so/Pose-Estimation-with-OpenPifPaf-7517E-8cb982455e01478e876c52e9324d8e6b"},"this")," notion for experiments with the OpenPifPaf model, ",(0,s.kt)("a",{parentName:"p",href:"https://www.notion.so/Pose-Estimation-with-Keypoint-RCNN-in-TorchVision-96e6aad0f36f44d3bff28e60525c6d31"},"this")," one for the TorchVision Keypoint R-CNN model, and ",(0,s.kt)("a",{parentName:"p",href:"https://www.notion.so/Detectron-2-D281D-bb7f769860fa434d923feef3a99f9cbb"},"this")," notion for the Detectron2 model."),(0,s.kt)("h3",{id:"pose-detection-on-the-edge"},"Pose Detection on the Edge"),(0,s.kt)("p",null,"Train the pose detector using Teachable machine, employing the PoseNet model (multi-person real-time pose estimation) as the backbone and serve it to the web browser using ml5.js. This system will infer the end-users pose in real-time via a web browser. Check out ",(0,s.kt)("a",{parentName:"p",href:"https://teachablemachine.withgoogle.com/train/pose"},"this")," link and ",(0,s.kt)("a",{parentName:"p",href:"https://www.notion.so/ml5-js-Pose-Estimation-with-PoseNet-5661cefe46b449998cc31838441dc26a"},"this")," notion. "),(0,s.kt)("h3",{id:"pose-detection-on-the-edge-using-openvino"},"Pose Detection on the Edge using OpenVINO"),(0,s.kt)("p",null,"Optimize the pre-trained pose estimation model using the OpenVINO toolkit to make it ready to serve at the edge (e.g. small embedded devices) and create an OpenVINO inference engine for real-time inference. Check out ",(0,s.kt)("a",{parentName:"p",href:"https://www.notion.so/OpenVINO-4c4fc4f167cc4601ade5795a241a60da"},"this")," notion."))}u.isMDXComponent=!0},79168:function(e,t,n){t.Z=n.p+"assets/images/content-concepts-raw-computer-vision-pose-estimation-img-8f472a887796c5c2f37ceb8f657dae8a.png"},93230:function(e,t,n){t.Z=n.p+"assets/images/content-concepts-raw-computer-vision-pose-estimation-slide52-5edb6e33626915350ed941f259e4a02f.png"}}]);