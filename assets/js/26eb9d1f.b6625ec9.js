"use strict";(self.webpackChunkdocs=self.webpackChunkdocs||[]).push([[8818],{92735:function(e){e.exports=JSON.parse('{"blogPosts":[{"id":"/2021/10/01/clinical-decision-making","metadata":{"permalink":"/ai-kb/blog/2021/10/01/clinical-decision-making","source":"@site/blog/2021-10-01-clinical-decision-making.mdx","title":"Clinical Decision Making","description":"Health insurance can be complicated\u2014especially when it comes to prior authorization (also referred to as pre-approval, pre-authorization, and pre-certification). The manual labor involved in obtaining prior authorizations (PAs) is a well-recognized burden among providers. Up to 46% of PA requests are still submitted by fax, and 60% require a telephone call, according to America\u2019s Health Insurance Plans (AHIP). A 2018 survey by the American Medical Association (AMA) found that doctors and their staff spend an average of 2 days a week completing PAs. In addition to eating up time that physicians could spend with patients, PAs also contribute to burnout.","date":"2021-10-01T00:00:00.000Z","formattedDate":"October 1, 2021","tags":[{"label":"classification","permalink":"/ai-kb/blog/tags/classification"},{"label":"healthcare","permalink":"/ai-kb/blog/tags/healthcare"}],"readingTime":1.46,"truncated":false,"authors":[{"name":"Sparsh Agarwal","title":"Principal Developer","url":"https://github.com/sparsh-ai","imageURL":"https://avatars.githubusercontent.com/u/62965911?v=4","key":"sparsh"}],"frontMatter":{"title":"Clinical Decision Making","authors":"sparsh","tags":["classification","healthcare"]},"nextItem":{"title":"Detectron 2","permalink":"/ai-kb/blog/2021/10/01/detectron-2"}},"content":"Health insurance can be complicated\u2014especially when it comes to prior authorization (also referred to as pre-approval, pre-authorization, and pre-certification). The manual labor involved in obtaining prior authorizations (PAs) is a well-recognized burden among providers. Up to 46% of PA requests are still submitted by fax, and 60% require a telephone call, according to America\u2019s Health Insurance Plans (AHIP). A 2018 survey by the American Medical Association (AMA) found that doctors and their staff spend an average of 2 days a week completing PAs. In addition to eating up time that physicians could spend with patients, PAs also contribute to burnout.\\n\\nThe objective was to identify the patterns from data to create clinical decision making in Pre-Auth and improve the accuracy in a clinical decision based on historical data analysis. \\n\\nTwo use cases were identified. Use Case 1 - *Supervised Learning Model - to aid clinicians in UM decision making. Tasks -* Ingest Pre-authorization data from Mongo DB into the analytical environment, Exploratory Data Analysis and Feature Engineering, Train supervised analytical models, model validation and model selection, Create a web service to be plugged into the case processing flow to call the model, and Display the recommendation from the model on UI on the authorization review screen. Use Case 2 - *Unsupervised Learning Model - to generate insights from the pre-authorization data. Tasks -* Ingest Pre-authorization data from Mongo DB into the analytical environment, Cluster analysis, univariate and multivariate analysis, and Generate insights and display insights on the dashboard.\\n\\nFinal Deliverables - Model re-training (batch mode), validation and deployment code (python scripts) with Unix command line support, Documentation - PPT, Recorded video, Technical document, Flask API backend system, HTML/PHP Web App frontend UI integration, and Plotly Dash Supervised/Unsupervised learning and insights generation dashboard."},{"id":"/2021/10/01/detectron-2","metadata":{"permalink":"/ai-kb/blog/2021/10/01/detectron-2","source":"@site/blog/2021-10-01-detectron-2.mdx","title":"Detectron 2","description":"/img/content-blog-raw-blog-detectron-2-untitled.png","date":"2021-10-01T00:00:00.000Z","formattedDate":"October 1, 2021","tags":[{"label":"tool","permalink":"/ai-kb/blog/tags/tool"},{"label":"vision","permalink":"/ai-kb/blog/tags/vision"}],"readingTime":6.13,"truncated":false,"authors":[{"name":"Sparsh Agarwal","title":"Principal Developer","url":"https://github.com/sparsh-ai","imageURL":"https://avatars.githubusercontent.com/u/62965911?v=4","key":"sparsh"}],"frontMatter":{"title":"Detectron 2","authors":"sparsh","tags":["tool","vision"]},"prevItem":{"title":"Clinical Decision Making","permalink":"/ai-kb/blog/2021/10/01/clinical-decision-making"},"nextItem":{"title":"Distributed Training of Recommender Systems","permalink":"/ai-kb/blog/2021/10/01/distributed-training-of-recommender-systems"}},"content":"![/img/content-blog-raw-blog-detectron-2-untitled.png](/img/content-blog-raw-blog-detectron-2-untitled.png)\\n\\n# Introduction\\n\\nDetectron 2 is a next-generation open-source object detection system from Facebook AI Research. With the repo you can use and train the various state-of-the-art models for detection tasks such as bounding-box detection, instance and semantic segmentation, and person keypoint detection.\\n\\nThe following is the directory tree of detectron 2:\\n\\n```\\ndetectron2\\n\u251c\u2500checkpoint  <- checkpointer and model catalog handlers\\n\u251c\u2500config      <- default configs and handlers\\n\u251c\u2500data        <- dataset handlers and data loaders\\n\u251c\u2500engine      <- predictor and trainer engines\\n\u251c\u2500evaluation  <- evaluator for each dataset\\n\u251c\u2500export      <- converter of detectron2 models to caffe2 (ONNX)\\n\u251c\u2500layers      <- custom layers e.g. deformable conv.\\n\u251c\u2500model_zoo   <- pre-trained model links and handler\\n\u251c\u2500modeling   \\n\u2502  \u251c\u2500meta_arch <- meta architecture e.g. R-CNN, RetinaNet\\n\u2502  \u251c\u2500backbone  <- backbone network e.g. ResNet, FPN\\n\u2502  \u251c\u2500proposal_generator <- region proposal network\\n\u2502  \u2514\u2500roi_heads <- head networks for pooled ROIs e.g. box, mask heads\\n\u251c\u2500solver       <- optimizer and scheduler builders\\n\u251c\u2500structures   <- structure classes e.g. Boxes, Instances, etc\\n\u2514\u2500utils        <- utility modules e.g. visualizer, logger, etc\\n```\\n\\n# Installation\\n\\n```python\\n%%time\\n!pip install -U torch==1.4+cu100 torchvision==0.5+cu100 -f https://download.pytorch.org/whl/torch_stable.html;\\n!pip install cython pyyaml==5.1;\\n!pip install -U \'git+https://github.com/cocodataset/cocoapi.git#subdirectory=PythonAPI\';\\n!pip install detectron2 -f https://dl.fbaipublicfiles.com/detectron2/wheels/cu100/index.html;\\n\\nfrom detectron2 import model_zoo\\nfrom detectron2.engine import DefaultPredictor\\nfrom detectron2.config import get_cfg\\nfrom detectron2.utils.visualizer import Visualizer\\nfrom detectron2.data import MetadataCatalog\\n```\\n\\n# Inference on pre-trained models\\n\\n![Original image](/img/content-blog-raw-blog-detectron-2-untitled-1.png)\\n\\nOriginal image\\n\\n![Object detection with Faster-RCNN-101](/img/content-blog-raw-blog-detectron-2-untitled-2.png)\\n\\nObject detection with Faster-RCNN-101\\n\\n![Instance segmentation with Mask-RCNN-50](/img/content-blog-raw-blog-detectron-2-untitled-3.png)\\n\\nInstance segmentation with Mask-RCNN-50\\n\\n![Keypoint estimation with Keypoint-RCNN-50](/img/content-blog-raw-blog-detectron-2-untitled-4.png)\\n\\nKeypoint estimation with Keypoint-RCNN-50\\n\\n![Panoptic segmentation with Panoptic-FPN-101](/img/content-blog-raw-blog-detectron-2-untitled-5.png)\\n\\nPanoptic segmentation with Panoptic-FPN-101\\n\\n![Default Mask R-CNN (top) vs. Mask R-CNN with PointRend (bottom) comparison](/img/content-blog-raw-blog-detectron-2-untitled-6.png)\\n\\nDefault Mask R-CNN (top) vs. Mask R-CNN with PointRend (bottom) comparison\\n\\n# Fine-tuning Balloons Dataset\\n\\n### Load the data\\n\\n```\\n# download, decompress the data\\n!wget https://github.com/matterport/Mask_RCNN/releases/download/v2.1/balloon_dataset.zip\\n!unzip balloon_dataset.zip > /dev/null\\n```\\n\\n### Convert dataset into Detectron2\'s standard format\\n\\n```\\nfrom detectron2.structures import BoxMode\\n# write a function that loads the dataset into detectron2\'s standard format\\ndef get_balloon_dicts(img_dir):\\n    json_file = os.path.join(img_dir, \\"via_region_data.json\\")\\n    with open(json_file) as f:\\n        imgs_anns = json.load(f)\\n\\n    dataset_dicts = []\\n    for _, v in imgs_anns.items():\\n        record = {}\\n        \\n        filename = os.path.join(img_dir, v[\\"filename\\"])\\n        height, width = cv2.imread(filename).shape[:2]\\n        \\n        record[\\"file_name\\"] = filename\\n        record[\\"height\\"] = height\\n        record[\\"width\\"] = width\\n      \\n        annos = v[\\"regions\\"]\\n        objs = []\\n        for _, anno in annos.items():\\n            assert not anno[\\"region_attributes\\"]\\n            anno = anno[\\"shape_attributes\\"]\\n            px = anno[\\"all_points_x\\"]\\n            py = anno[\\"all_points_y\\"]\\n            poly = [(x + 0.5, y + 0.5) for x, y in zip(px, py)]\\n            poly = list(itertools.chain.from_iterable(poly))\\n\\n            obj = {\\n                \\"bbox\\": [np.min(px), np.min(py), np.max(px), np.max(py)],\\n                \\"bbox_mode\\": BoxMode.XYXY_ABS,\\n                \\"segmentation\\": [poly],\\n                \\"category_id\\": 0,\\n                \\"iscrowd\\": 0\\n            }\\n            objs.append(obj)\\n        record[\\"annotations\\"] = objs\\n        dataset_dicts.append(record)\\n    return dataset_dicts\\n\\nfrom detectron2.data import DatasetCatalog, MetadataCatalog\\nfor d in [\\"train\\", \\"val\\"]:\\n    DatasetCatalog.register(\\"balloon/\\" + d, lambda d=d: get_balloon_dicts(\\"balloon/\\" + d))\\n    MetadataCatalog.get(\\"balloon/\\" + d).set(thing_classes=[\\"balloon\\"])\\nballoon_metadata = MetadataCatalog.get(\\"balloon/train\\")\\n```\\n\\n### Model configuration and training\\n\\n```\\nfrom detectron2.engine import DefaultTrainer\\nfrom detectron2.config import get_cfg\\n\\ncfg = get_cfg()\\ncfg.merge_from_file(model_zoo.get_config_file(\\"COCO-InstanceSegmentation/mask_rcnn_R_50_FPN_3x.yaml\\"))\\ncfg.DATASETS.TRAIN = (\\"balloon/train\\",)\\ncfg.DATASETS.TEST = ()   # no metrics implemented for this dataset\\ncfg.DATALOADER.NUM_WORKERS = 2\\ncfg.MODEL.WEIGHTS = model_zoo.get_checkpoint_url(\\"COCO-InstanceSegmentation/mask_rcnn_R_50_FPN_3x.yaml\\")\\ncfg.SOLVER.IMS_PER_BATCH = 2\\ncfg.SOLVER.BASE_LR = 0.00025\\ncfg.SOLVER.MAX_ITER = 300    # 300 iterations seems good enough, but you can certainly train longer\\ncfg.MODEL.ROI_HEADS.BATCH_SIZE_PER_IMAGE = 128   # faster, and good enough for this toy dataset\\ncfg.MODEL.ROI_HEADS.NUM_CLASSES = 1  # only has one class (ballon)\\n\\nos.makedirs(cfg.OUTPUT_DIR, exist_ok=True)\\ntrainer = DefaultTrainer(cfg) \\ntrainer.resume_or_load(resume=False)\\ntrainer.train()\\n```\\n\\n### Inference and Visualization\\n\\n```\\nfrom detectron2.utils.visualizer import ColorMode\\n\\n# load weights\\ncfg.MODEL.WEIGHTS = os.path.join(cfg.OUTPUT_DIR, \\"model_final.pth\\")\\ncfg.MODEL.ROI_HEADS.SCORE_THRESH_TEST = 0.7   # set the testing threshold for this model\\n# Set training data-set path\\ncfg.DATASETS.TEST = (\\"balloon/val\\", )\\n# Create predictor (model for inference)\\npredictor = DefaultPredictor(cfg)\\n\\ndataset_dicts = get_balloon_dicts(\\"balloon/val\\")\\nfor d in random.sample(dataset_dicts, 3):    \\n    im = cv2.imread(d[\\"file_name\\"])\\n    outputs = predictor(im)\\n    v = Visualizer(im[:, :, ::-1],\\n                   metadata=balloon_metadata, \\n                   scale=0.8, \\n                   instance_mode=ColorMode.IMAGE_BW   # remove the colors of unsegmented pixels\\n    )\\n    v = v.draw_instance_predictions(outputs[\\"instances\\"].to(\\"cpu\\"))\\n    cv2_imshow(v.get_image()[:, :, ::-1])\\n```\\n\\n![/img/content-blog-raw-blog-detectron-2-untitled-7.png](/img/content-blog-raw-blog-detectron-2-untitled-7.png)\\n\\n![/img/content-blog-raw-blog-detectron-2-untitled-8.png](/img/content-blog-raw-blog-detectron-2-untitled-8.png)\\n\\n![/img/content-blog-raw-blog-detectron-2-untitled-9.png](/img/content-blog-raw-blog-detectron-2-untitled-9.png)\\n\\n# Fine-tuning Chip Dataset\\n\\n### Load the data\\n\\n```\\n#get the dataset\\n!pip install -q kaggle\\n!pip install -q kaggle-cli\\nos.environ[\'KAGGLE_USERNAME\'] = \\"sparshag\\" \\nos.environ[\'KAGGLE_KEY\'] = \\"1b1f894d1fa6febe9676681b44ad807b\\"\\n!kaggle datasets download -d tannergi/microcontroller-detection\\n!unzip microcontroller-detection.zip\\n```\\n\\n### Convert dataset into Detectron2\'s standard format\\n\\n```\\n# Registering the dataset\\nfrom detectron2.structures import BoxMode\\ndef get_microcontroller_dicts(csv_file, img_dir):\\n    df = pd.read_csv(csv_file)\\n    df[\'filename\'] = df[\'filename\'].map(lambda x: img_dir+x)\\n\\n    classes = [\'Raspberry_Pi_3\', \'Arduino_Nano\', \'ESP8266\', \'Heltec_ESP32_Lora\']\\n\\n    df[\'class_int\'] = df[\'class\'].map(lambda x: classes.index(x))\\n\\n    dataset_dicts = []\\n    for filename in df[\'filename\'].unique().tolist():\\n        record = {}\\n        \\n        height, width = cv2.imread(filename).shape[:2]\\n        \\n        record[\\"file_name\\"] = filename\\n        record[\\"height\\"] = height\\n        record[\\"width\\"] = width\\n\\n        objs = []\\n        for index, row in df[(df[\'filename\']==filename)].iterrows():\\n          obj= {\\n              \'bbox\': [row[\'xmin\'], row[\'ymin\'], row[\'xmax\'], row[\'ymax\']],\\n              \'bbox_mode\': BoxMode.XYXY_ABS,\\n              \'category_id\': row[\'class_int\'],\\n              \\"iscrowd\\": 0\\n          }\\n          objs.append(obj)\\n        record[\\"annotations\\"] = objs\\n        dataset_dicts.append(record)\\n    return dataset_dicts\\n\\nclasses = [\'Raspberry_Pi_3\', \'Arduino_Nano\', \'ESP8266\', \'Heltec_ESP32_Lora\']\\nfor d in [\\"train\\", \\"test\\"]:\\n  DatasetCatalog.register(\'microcontroller/\' + d, lambda d=d: get_microcontroller_dicts(\'Microcontroller Detection/\' + d + \'_labels.csv\', \'Microcontroller Detection/\' + d+\'/\'))\\n  MetadataCatalog.get(\'microcontroller/\' + d).set(thing_classes=classes)\\nmicrocontroller_metadata = MetadataCatalog.get(\'microcontroller/train\')\\n```\\n\\n### Model configuration and training\\n\\n```\\n# Train the model\\ncfg = get_cfg()\\ncfg.merge_from_file(model_zoo.get_config_file(\\"COCO-Detection/faster_rcnn_R_101_FPN_3x.yaml\\"))\\ncfg.DATASETS.TRAIN = (\'microcontroller/train\',)\\ncfg.DATASETS.TEST = ()   # no metrics implemented for this dataset\\ncfg.DATALOADER.NUM_WORKERS = 2\\ncfg.MODEL.WEIGHTS = model_zoo.get_checkpoint_url(\\"COCO-Detection/faster_rcnn_R_101_FPN_3x.yaml\\")\\ncfg.SOLVER.IMS_PER_BATCH = 2\\ncfg.SOLVER.MAX_ITER = 1000\\ncfg.MODEL.ROI_HEADS.NUM_CLASSES = 4\\n\\nos.makedirs(cfg.OUTPUT_DIR, exist_ok=True)\\ntrainer = DefaultTrainer(cfg) \\ntrainer.resume_or_load(resume=False)\\ntrainer.train()\\n```\\n\\n![/img/content-blog-raw-blog-detectron-2-untitled-10.png](/img/content-blog-raw-blog-detectron-2-untitled-10.png)\\n\\n![/img/content-blog-raw-blog-detectron-2-untitled-11.png](/img/content-blog-raw-blog-detectron-2-untitled-11.png)\\n\\n### Inference and Visualization\\n\\n```\\ncfg.MODEL.WEIGHTS = os.path.join(cfg.OUTPUT_DIR, \\"model_final.pth\\")\\ncfg.MODEL.ROI_HEADS.SCORE_THRESH_TEST = 0.8   # set the testing threshold for this model\\ncfg.DATASETS.TEST = (\'microcontroller/test\', )\\npredictor = DefaultPredictor(cfg)\\n\\ndf_test = pd.read_csv(\'Microcontroller Detection/test_labels.csv\')\\n\\ndataset_dicts = DatasetCatalog.get(\'microcontroller/test\')\\nfor d in random.sample(dataset_dicts, 3):    \\n    im = cv2.imread(d[\\"file_name\\"])\\n    outputs = predictor(im)\\n    v = Visualizer(im[:, :, ::-1], \\n                   metadata=microcontroller_metadata, \\n                   scale=0.8\\n                   )\\n    v = v.draw_instance_predictions(outputs[\\"instances\\"].to(\\"cpu\\"))\\n    cv2_imshow(v.get_image()[:, :, ::-1])\\n```\\n\\n### Real-time Webcam inference\\n\\n```\\nfrom IPython.display import display, Javascript\\nfrom google.colab.output import eval_js\\nfrom base64 import b64decode\\n\\ndef take_photo(filename=\'photo.jpg\', quality=0.8):\\n  js = Javascript(\'\'\'\\n    async function takePhoto(quality) {\\n      const div = document.createElement(\'div\');\\n      const capture = document.createElement(\'button\');\\n      capture.textContent = \'Capture\';\\n      div.appendChild(capture);\\n\\n      const video = document.createElement(\'video\');\\n      video.style.display = \'block\';\\n      const stream = await navigator.mediaDevices.getUserMedia({video: true});\\n\\n      document.body.appendChild(div);\\n      div.appendChild(video);\\n      video.srcObject = stream;\\n      await video.play();\\n\\n      // Resize the output to fit the video element.\\n      google.colab.output.setIframeHeight(document.documentElement.scrollHeight, true);\\n\\n      // Wait for Capture to be clicked.\\n      await new Promise((resolve) => capture.onclick = resolve);\\n\\n      const canvas = document.createElement(\'canvas\');\\n      canvas.width = video.videoWidth;\\n      canvas.height = video.videoHeight;\\n      canvas.getContext(\'2d\').drawImage(video, 0, 0);\\n      stream.getVideoTracks()[0].stop();\\n      div.remove();\\n      return canvas.toDataURL(\'image/jpeg\', quality);\\n    }\\n    \'\'\')\\n  display(js)\\n  data = eval_js(\'takePhoto({})\'.format(quality))\\n  binary = b64decode(data.split(\',\')[1])\\n  with open(filename, \'wb\') as f:\\n    f.write(binary)\\n  return filename\\n\\nfrom IPython.display import Image\\ntry:\\n  filename = take_photo()\\n  print(\'Saved to {}\'.format(filename))\\n  \\n  # Show the image which was just taken.\\n  display(Image(filename))\\nexcept Exception as err:\\n  # Errors will be thrown if the user does not have a webcam or if they do not\\n  # grant the page permission to access it.\\n  print(str(err))\\n```\\n\\n```\\nmodel_path = \'/content/output/model_final.pth\'\\nconfig_path= model_zoo.get_config_file(\\"COCO-Detection/faster_rcnn_R_101_FPN_3x.yaml\\")\\n\\n# Create config\\ncfg = get_cfg()\\ncfg.merge_from_file(config_path)\\ncfg.MODEL.ROI_HEADS.SCORE_THRESH_TEST = 0.1\\ncfg.MODEL.WEIGHTS = model_path\\n\\npredictor = DefaultPredictor(cfg)\\n\\nim = cv2.imread(\'photo.jpg\')\\noutputs = predictor(im)\\n\\nv = Visualizer(im[:, :, ::-1], MetadataCatalog.get(cfg.DATASETS.TRAIN[0]), scale=1.2)\\nv = v.draw_instance_predictions(outputs[\\"instances\\"].to(\\"cpu\\"))\\ncv2_imshow(v.get_image()[:, :, ::-1])\\n```\\n\\n# Fine-tuning on Face dataset\\n\\nThe process is same. Here is the output.\\n\\n![/img/content-blog-raw-blog-detectron-2-untitled-12.png](/img/content-blog-raw-blog-detectron-2-untitled-12.png)\\n\\n![/img/content-blog-raw-blog-detectron-2-untitled-13.png](/img/content-blog-raw-blog-detectron-2-untitled-13.png)\\n\\n![/img/content-blog-raw-blog-detectron-2-untitled-14.png](/img/content-blog-raw-blog-detectron-2-untitled-14.png)\\n\\n### Behind the scenes\\n\\n![/img/content-blog-raw-blog-detectron-2-untitled-15.png](/img/content-blog-raw-blog-detectron-2-untitled-15.png)\\n\\n### References\\n\\n- [How to embed Detectron2 in your computer vision project - blogpost](https://medium.com/deepvisionguru/how-to-embed-detectron2-in-your-computer-vision-project-817f29149461)\\n- [Detectron2 Train a Instance Segmentation Model by Gilbert Tanner](https://gilberttanner.com/blog/detectron2-train-a-instance-segmentation-model)\\n- [How to train Detectron2 with Custom COCO Datasets - DLology](https://www.dlology.com/blog/how-to-train-detectron2-with-custom-coco-datasets/)\\n- [Character Recognition and Segmentation For Custom Data Using Detectron2 - blogpost](https://towardsdatascience.com/character-recognition-and-segmentation-for-custom-data-using-detectron2-599de82b393c)\\n- [Training models with Panoptic Segmentation in Detectron2](https://www.celantur.com/blog/panoptic-segmentation-in-detectron2/)\\n- [Image segmentation using Detectron2 - Kaggle](https://www.kaggle.com/lewisgmorris/image-segmentation-using-detectron2)\\n- [A Beginner\u2019s Guide To Object Detection And Computer Vision With Facebook\u2019s Detectron2](https://towardsdatascience.com/a-beginners-guide-to-object-detection-and-computer-vision-with-facebook-s-detectron2-700b6273390e)\\n- [Face Detection on Custom Dataset with Detectron2 and PyTorch using Python](https://www.curiousily.com/posts/face-detection-on-custom-dataset-with-detectron2-in-python/)\\n- [My Experiment Notion](https://www.notion.so/Detectron-2-d31ac9c14a8d4d9888882df14a4e0eee)\\n- [Official Colab](https://colab.research.google.com/drive/16jcaJoc6bCFAQ96jDe2HwtXj7BMD_-m5)\\n- [Official Slide](https://research.fb.com/wp-content/uploads/2019/12/4.-detectron2.pdf)\\n- [Official Git](https://github.com/facebookresearch/detectron2)"},{"id":"/2021/10/01/distributed-training-of-recommender-systems","metadata":{"permalink":"/ai-kb/blog/2021/10/01/distributed-training-of-recommender-systems","source":"@site/blog/2021-10-01-distributed-training-of-recommender-systems.mdx","title":"Distributed Training of Recommender Systems","description":"The usage and importance of recommender systems are increasing at a fast pace. And deep learning is gaining traction as the preferred choice for model architecture. Giants like Google and Facebook are already using recommenders to earn billions of dollars.","date":"2021-10-01T00:00:00.000Z","formattedDate":"October 1, 2021","tags":[{"label":"distributed","permalink":"/ai-kb/blog/tags/distributed"},{"label":"recsys","permalink":"/ai-kb/blog/tags/recsys"}],"readingTime":5.85,"truncated":false,"authors":[{"name":"Sparsh Agarwal","title":"Principal Developer","url":"https://github.com/sparsh-ai","imageURL":"https://avatars.githubusercontent.com/u/62965911?v=4","key":"sparsh"}],"frontMatter":{"title":"Distributed Training of Recommender Systems","authors":"sparsh","tags":["distributed","recsys"]},"prevItem":{"title":"Detectron 2","permalink":"/ai-kb/blog/2021/10/01/detectron-2"},"nextItem":{"title":"Document Recommendation","permalink":"/ai-kb/blog/2021/10/01/document-recommendation"}},"content":"The usage and importance of recommender systems are increasing at a fast pace. And deep learning is gaining traction as the preferred choice for model architecture. Giants like Google and Facebook are already using recommenders to earn billions of dollars.\\n\\nRecently, Facebook shared its approach to maintain its 12 trillion parameter recommender. Building these large systems is challenging because it requires huge computation and memory resources. And we will soon enter into 100 trillion range. And SMEs will not be left behind due to open-source environment of software architectures and the decreasing cost of hardware, especially on the cloud infrastructure.\\n\\nAs per one estimate, a model with 100 trillion parameters would require at least 200TB just to store the model, even at 16-bit floating-point accuracy. So we need architectures that can support efficient and distributed training of recommendation models.\\n\\n***Memory-intensive vs Computation-intensive***: The increasing parameter comes mostly from the embedding layer which maps each entrance of an ID type feature (such as an user ID and a session ID) into a fixed length low-dimensional embedding vector. Consider the billion scale of entrances for the ID type features in a production recommender system and the wide utilization of feature crosses, the embedding layer usually domains the parameter space, which makes this component extremely **memory-intensive**. On the other hand, these low-dimensional embedding vectors are concatenated with diversified Non-ID type features (e.g., image, audio, video, social network, etc.) to feed a group of increasingly sophisticated neural networks (e.g., convolution, LSTM, multi-head attention) for prediction(s). Furthermore, in practice, multiple objectives can also be combined and optimized simultaneously for multiple tasks. These mechanisms make the rest neural network increasingly **computation-intensive**.\\n\\n![An example of a recommender models with 100+ trillions of parameter in the embedding layer and 50+ TFLOP computation in the neural network.](/img/content-blog-raw-blog-distributed-training-of-recommender-systems-untitled.png)\\n\\nAn example of a recommender models with 100+ trillions of parameter in the embedding layer and 50+ TFLOP computation in the neural network.\\n\\n[Alibaba\'s XDL](https://github.com/alibaba/x-deeplearning), [Baidu\'s PaddleRec](https://github.com/PaddlePaddle/PaddleRec), and [Kwai\'s Persia](https://github.com/persiaml/persia) are some open-source frameworks for this large-scale distributed training of recommender systems.\\n\\n<aside>\\n\ud83d\udccc ***Synchronous vs Asynchronous Algorithms***: Synchronous algorithms always use the up-to-date gradient to update the model to ensure the model accuracy. However, the overhead of communications for synchronous algorithms starts to become too expensive to scale out the training procedure, causing inefficiency in running time. While asynchronous algorithm have better hardware efficiency, it often leads to a \u201csignificant\u201d loss in model accuracy at this scale\u2014for production recommender systems (e.g., Baidu\u2019s search engine). Recall that even 0.1% drop of accuracy would lead to a noticeable loss in revenue.\\n\\n</aside>\\n\\n### Parameter Server Framework\\n\\nExisting distributed systems for deep learning based recommender models are usually built on top of the parameter server (PS) framework, where one can add elastic distributed storage to hold the increasingly large amount of parameters of the embedding layer. On the other hand, the computation workload does not scale linearly with the increasing parameter scale of the embedding layer\u2014in fact, with an efficient implementation, a lookup operation over a larger embedding table would introduce almost no additional computations.\\n\\n![Left: deep learning based recommender model training workflow over a heterogeneous cluster. Right: Gantt charts to compare fully synchronous, fully asynchronous, raw hybrid and optimized hybrid modes of distributed training of the deep learning recommender model. [Source](https://arxiv.org/pdf/2111.05897v1.pdf).](/img/content-blog-raw-blog-distributed-training-of-recommender-systems-untitled-1.png)\\n\\nLeft: deep learning based recommender model training workflow over a heterogeneous cluster. Right: Gantt charts to compare fully synchronous, fully asynchronous, raw hybrid and optimized hybrid modes of distributed training of the deep learning recommender model. [Source](https://arxiv.org/pdf/2111.05897v1.pdf).\\n\\n### PERSIA\\n\\n**PERSIA**\xa0(**P**arallel r**E**commendation t**R**aining\xa0**S**ystem with hybr**I**d\xa0**A**cceleration) is a PyTorch-based system for training deep learning recommendation models on commodity hardware. It supports models containing more than 100 trillion parameters.\\n\\nIt uses a hybrid training algorithm to tackle the embedding layer and dense neural network modules differently\u2014the embedding layer is trained in an asynchronous fashion to improve the throughput of training samples, while the rest neural network is trained in a synchronous fashion to preserve the statistical efficiency.\\n\\nIt also uses a distributed system to manage the hybrid computation resources (CPUs and GPUs) to optimize the co-existence of asynchronicity and synchronicity in the training algorithm.\\n\\n![Untitled](/img/content-blog-raw-blog-distributed-training-of-recommender-systems-untitled-2.png)\\n\\n![Untitled](/img/content-blog-raw-blog-distributed-training-of-recommender-systems-untitled-3.png)\\n\\nPersia includes a data loader module, a embedding PS (Parameter Server) module, a group of embedding workers over CPU nodes, and a group of NN workers over GPU instances. Each module can be dynamically scaled for different model scales and desired training throughput:\\n\\n- A data loader that fetches training data from distributed storages such as Hadoop, Kafka, etc;\\n- A embedding parameter server (embedding PS for short) manages the storage and update of the parameters in the embedding layer $\\\\mathrm{w}^{emb}$;\\n- A group of embedding workers that runs Algorithm 1 for getting the embedding parameters from the embedding PS; aggregating embedding vectors (potentially) and putting embedding gradients back to embedding PS;\\n- A group of NN workers that runs the forward-/backward- propagation of the neural network $\\\\mathrm{NN_{w^{nn}}(\xb7)}$.\\n\\n![The architecture of Persia.](/img/content-blog-raw-blog-distributed-training-of-recommender-systems-untitled-4.png)\\n\\nThe architecture of Persia.\\n\\nLogically, the training procedure is conducted by Persia in a data dispatching based paradigm as below:\\n\\n1. The data loader will dispatch the ID type feature $\\\\mathrm{x^{ID}}$ to an embedding worker\u2014the embedding worker will generate an unique sample ID \ud835\udf09 for this sample, buffer this sample ID with the ID type feature $\\\\mathrm{x_\\\\xi^{ID}}$ locally, and returns this ID \ud835\udf09 back the data loader; the data loader will associate this sample\u2019s Non-ID type features and labels with this unique ID.\\n2. Next, the data loader will dispatch the Non-ID type feature and label(s) $\\\\mathrm{(x_\\\\xi^{NID},y_\\\\xi)}$ to a NN worker.\\n3. Once a NN worker receives this incomplete training sample, it will issue a request to pull the ID type features\u2019 $\\\\mathrm{(x_\\\\xi^{ID})}$ embedding $\\\\mathrm{w_\\\\xi^{emb}}$ from some embedding worker according to the sample ID \ud835\udf09\u2014this would trigger the forward propagation in Algorithm 1, where the embedding worker will use the buffered ID type feature $\\\\mathrm{x_\\\\xi^{ID}}$ to get the corresponding $\\\\mathrm{w_\\\\xi^{emb}}$ from the embedding PS.\\n4. Then the embedding worker performs some potential aggregation of original embedding vectors. When this computation finishes, the aggregated embedding vector $\\\\mathrm{w_\\\\xi^{emb}}$ will be transmitted to the NN worker that issues the pull request.\\n5. Once the NN worker gets a group of complete inputs for the dense module, it will create a mini-batch and conduct the training computation of the NN according to Algorithm 2. Note that the parameter of the NN always locates in the device RAM of the NN worker, where the NN workers synchronize the gradients by the AllReduce Paradigm.\\n6. When the iteration of Algorithm 2 is finished, the NN worker will send the gradients of the embedding ($\\\\mathrm{F_\\\\xi^{emb\'}}$) back to the embedding worker (also along with the sample ID \ud835\udf09).\\n7. The embedding worker will query the buffered ID type feature $\\\\mathrm{x_\\\\xi^{ID}}$ according to the sample ID \ud835\udf09; compute gradients $\\\\mathrm{F_\\\\xi^{emb\'}}$ of the embedding parameters and send the gradients to the embedding PS, so that the embedding PS can finally compute the updates according the embedding parameter\u2019s gradients by its SGD optimizer and update the embedding parameters."},{"id":"/2021/10/01/document-recommendation","metadata":{"permalink":"/ai-kb/blog/2021/10/01/document-recommendation","source":"@site/blog/2021-10-01-document-recommendation.mdx","title":"Document Recommendation","description":"/img/content-blog-raw-blog-document-recommendation-untitled.png","date":"2021-10-01T00:00:00.000Z","formattedDate":"October 1, 2021","tags":[{"label":"nlp","permalink":"/ai-kb/blog/tags/nlp"},{"label":"similarity","permalink":"/ai-kb/blog/tags/similarity"}],"readingTime":1.285,"truncated":false,"authors":[{"name":"Sparsh Agarwal","title":"Principal Developer","url":"https://github.com/sparsh-ai","imageURL":"https://avatars.githubusercontent.com/u/62965911?v=4","key":"sparsh"}],"frontMatter":{"title":"Document Recommendation","authors":"sparsh","tags":["nlp","similarity"]},"prevItem":{"title":"Distributed Training of Recommender Systems","permalink":"/ai-kb/blog/2021/10/01/distributed-training-of-recommender-systems"},"nextItem":{"title":"Fake Voice Detection","permalink":"/ai-kb/blog/2021/10/01/fake-voice-detection"}},"content":"![/img/content-blog-raw-blog-document-recommendation-untitled.png](/img/content-blog-raw-blog-document-recommendation-untitled.png)\\n\\n## **Introduction**\\n\\n### Business objective\\n\\nFor the given user query, recommend relevant documents (BRM_ifam)\\n\\n### Technical objective\\n\\n1-to-N mapping of given input text\\n\\n## **Proposed Framework 1 \u2014 Hybrid Recommender System**\\n\\n- Text \u2192 Vector (Universal Sentence Embedding with TF Hub)\\n- Vector \u2192 Content-based Filtering Recommendation\\n- Index \u2192 Interaction Matrix\\n- Interaction Matrix \u2192 Collaborative Filtering Recommendation\\n- Collaborative + Content-based \u2192 Hybrid Recommendation\\n- Evaluation: Area-under-curve\\n\\n## **Proposed Framework 2 \u2014 Content-based Recommender System**\\n\\n1. Find A most similar user \u2192 Cosine similarity\\n2. For each user in A, find TopK Most Similar Items \u2192 Map Argsort\\n3. For each item Find TopL Most Similar Items \u2192 Cosine similarity\\n4. Display\\n5. Implement an evaluation metric\\n6. Evaluate\\n\\n## **Results and Discussion**\\n\\n- build.py \u2192 this script will take the training data as input and save all the required files in the same working directory\\n- recommend.py \u2192 this script will take the user query as input and predict top-K BRM recommendations\\n\\nVariables (during recommendation, you will be asked 2\u20133 choices, the meaning of those choices are as following)\\n\\n- top-K \u2014 how many top items you want to get in recommendation\\n- secondary items: this will determine how many similar items you would like to add in consideration, for each primary matching item\\n- sorted by frequency: since multiple input queries might point to same output, therefore this option allows to take that frequence count of outputs in consideration and will move the more frequent items at the top.\\n\\n### **Code**\\n\\n[https://gist.github.com/sparsh-ai/4e5f06ba3c55192b33a276ee67dbd42c#file-text-recommendations-ipynb](https://gist.github.com/sparsh-ai/4e5f06ba3c55192b33a276ee67dbd42c#file-text-recommendations-ipynb)"},{"id":"/2021/10/01/fake-voice-detection","metadata":{"permalink":"/ai-kb/blog/2021/10/01/fake-voice-detection","source":"@site/blog/2021-10-01-fake-voice-detection.mdx","title":"Fake Voice Detection","description":"/img/content-blog-raw-blog-fake-voice-detection-untitled.png","date":"2021-10-01T00:00:00.000Z","formattedDate":"October 1, 2021","tags":[{"label":"audio","permalink":"/ai-kb/blog/tags/audio"},{"label":"deepfake","permalink":"/ai-kb/blog/tags/deepfake"}],"readingTime":2.88,"truncated":false,"authors":[{"name":"Sparsh Agarwal","title":"Principal Developer","url":"https://github.com/sparsh-ai","imageURL":"https://avatars.githubusercontent.com/u/62965911?v=4","key":"sparsh"}],"frontMatter":{"title":"Fake Voice Detection","authors":"sparsh","tags":["audio","deepfake"]},"prevItem":{"title":"Document Recommendation","permalink":"/ai-kb/blog/2021/10/01/document-recommendation"},"nextItem":{"title":"Image Similarity System","permalink":"/ai-kb/blog/2021/10/01/image-similarity-system"}},"content":"![/img/content-blog-raw-blog-fake-voice-detection-untitled.png](/img/content-blog-raw-blog-fake-voice-detection-untitled.png)\\n\\n# Introduction\\n\\nFake audio can be used for malicious purposes which affect directly or indirectly human life. The objective is to differentiate between fake and real voice. Python and deep learning has been used and implemented to achieve the objective. Audio files or video file are being used as an input of this work then model has been trained for uniquely identify features for voice creation and voice detection. Deep learning technique is used to find accuracy between real and fake.\\n\\nSpeaker recognition usually refers to both speaker identification and speaker verification. A speaker identification system identifies who the speaker is, while an automatic speaker verification (ASV) system decides if an identity claim is true or false.\\nA general ASV system is robust to zero-effort impostors, they are\xa0vulnerable to more sophisticated attacks. Such vulnerability represents one of the security concerns of ASV systems.\xa0Spoofing involves an adversary (attacker) who masquerades as the target speaker to gain the access to a system.\xa0Such spoofing attacks can happen to various biometric traits, such as fingerprints, iris, face, and voice patterns. We are focusing only on the voice-based spoofing and anti-spoofing techniques for ASV system.\xa0The spoofed speech samples can be obtained through speech synthesis, voice conversion, or replay of recorded speech.\xa0**Imagine the following scenario\u2026**\\nYour phone rings, you pick up. It\u2019s your spouse asking you for details about your savings account \u2014 they don\u2019t have the account information on hand, but want to deposit money there this afternoon. Later, you realize a bunch of money has went missing! After investigating, you find out that the person masquerading as them on the other line was a voice 100% generated with AI. You\u2019ve just been scammed, and on top of that, can\u2019t believe the voice you thought belonged to your spouse was actually a fake.\\n\\nTo discern between real and fake audio, the detector uses visual representations of audio clips called spectrograms, which are also used to train speech synthesis models.\\nGoogle\u2019s 2019\xa0[AVSSpoof dataset](https://www.blog.google/outreach-initiatives/google-news-initiative/advancing-research-fake-audio-detection/)\xa0contains over 25,000 clips of audio, featuring both real and fake clips of a variety of male and female speakers.**Temporal Convolution Model**\\n\\n# Modeling Approach\\n\\nFirst, raw audio is preprocessed and converted into a mel-frequency spectrogram \u2014 this is the input for the model. The model performs convolutions over the time dimension of the spectrogram, then uses masked pooling to prevent overfitting. Finally, the output is passed into a dense layer and a sigmoid activation function, which ultimately outputs a predicted probability between 0 (fake) and 1 (real).\\nThe baseline model achieved 99%, 95%, and 85% accuracy on the train, validation, and test sets respectively. The differing performance is caused by differences between the three datasets. While all three datasets feature distinct and different speakers, the test set uses a different set of fake audio generating algorithms that were not present in the train or validation set.\\n\\n# Proposed Framework\\n\\n# Process Flow\\n\\n- Voice detection\\n    - Temporal Convolution model\\n        - Install packages\\n        - Download pretrained models\\n        - Initialize the model\\n        - Load data\\n        - Detect DeepFakes\\n    - GMM-UBG model\\n        - Install packages\\n        - Train the model\\n        - Load data\\n        - Detect DeepFakes\\n    - Convolutional VAE model\\n        - Install packages\\n        - Train the model\\n        - Load data\\n        - Detect DeepFakes\\n    - Voice Similarity\\n        - Install packages\\n        - Load data\\n        - Voice similarity match\\n        - Embedding visualization\\n\\n# Models Algorithms\\n\\n1. Temporal Convolution\\n2. ResNet\\n3. GMM\\n4. Light CNN\\n5. Fusion\\n6. SincNet\\n7. ASSERT\\n8. HOSA\\n9. CVAE"},{"id":"/2021/10/01/image-similarity-system","metadata":{"permalink":"/ai-kb/blog/2021/10/01/image-similarity-system","source":"@site/blog/2021-10-01-image-similarity-system.mdx","title":"Image Similarity System","description":"/img/content-blog-raw-blog-image-similarity-system-untitled.png","date":"2021-10-01T00:00:00.000Z","formattedDate":"October 1, 2021","tags":[{"label":"aws beanstalk","permalink":"/ai-kb/blog/tags/aws-beanstalk"},{"label":"flask","permalink":"/ai-kb/blog/tags/flask"},{"label":"similarity","permalink":"/ai-kb/blog/tags/similarity"},{"label":"vision","permalink":"/ai-kb/blog/tags/vision"}],"readingTime":3.045,"truncated":false,"authors":[{"name":"Sparsh Agarwal","title":"Principal Developer","url":"https://github.com/sparsh-ai","imageURL":"https://avatars.githubusercontent.com/u/62965911?v=4","key":"sparsh"}],"frontMatter":{"title":"Image Similarity System","authors":"sparsh","tags":["aws beanstalk","flask","similarity","vision"]},"prevItem":{"title":"Fake Voice Detection","permalink":"/ai-kb/blog/2021/10/01/fake-voice-detection"},"nextItem":{"title":"Insurance Personalization","permalink":"/ai-kb/blog/2021/10/01/insurance-personalization"}},"content":"![/img/content-blog-raw-blog-image-similarity-system-untitled.png](/img/content-blog-raw-blog-image-similarity-system-untitled.png)\\n\\n# Choice of variables\\n\\n### Image Encoder\\n\\nWe can select any pre-trained image classification model. These models are commonly known as encoders because their job is to encode an image into a feature vector. I analyzed four encoders named 1) MobileNet, 2) EfficientNet, 3) ResNet and 4) [BiT](https://tfhub.dev/google/bit/m-r152x4/1). After basic research, I decided to select BiT model because of its performance and state-of-the-art nature. I selected the BiT-M-50x3 variant of model which is of size 748 MB. More details about this architecture can be found on the official page [here](https://tfhub.dev/google/bit/m-r50x3/1). \\n\\n### Vector Similarity System\\n\\nImages are represented in a fixed-length feature vector format. For the given input vector, we need to find the TopK most similar vectors, keeping the memory efficiency and real-time retrival objective in mind. I explored the most popular techniques and listed down five of them: Annoy, Cosine distance, L1 distance, Locally Sensitive Hashing (LSH) and Image Deep Ranking. I selected Annoy because of its fast and efficient nature. More details about Annoy can be found on the official page [here](https://github.com/spotify/annoy).\\n\\n### Dataset\\n\\nI listed down 3 datasets from Kaggle that were best fitting the criteria of this use case: 1) [Fashion Product Images (Small)](https://www.kaggle.com/bhaskar2443053/fashion-small?), 2) [Food-11 image dataset](https://www.kaggle.com/trolukovich/food11-image-dataset?) and 3) [Caltech 256 Image Dataset](https://www.kaggle.com/jessicali9530/caltech256?). I selected Fashion dataset and Foods dataset.\\n\\n# Literature review\\n\\n- Determining Image similarity with Quasi-Euclidean Metric [arxiv](https://arxiv.org/abs/2006.14644v1)\\n- CatSIM: A Categorical Image Similarity Metric [arxiv](https://arxiv.org/abs/2004.09073v1)\\n- Central Similarity Quantization for Efficient Image and Video Retrieval [arxiv](https://arxiv.org/abs/1908.00347v5)\\n- Improved Deep Hashing with Soft Pairwise Similarity for Multi-label Image Retrieval [arxiv](https://arxiv.org/abs/1803.02987v3)\\n- Model-based Behavioral Cloning with Future Image Similarity Learning [arxiv](https://arxiv.org/abs/1910.03157v1)\\n- Why do These Match? Explaining the Behavior of Image Similarity Models [arxiv](https://arxiv.org/abs/1905.10797v1)\\n- Learning Non-Metric Visual Similarity for Image Retrieval [arxiv](https://arxiv.org/abs/1709.01353v2)\\n\\n# Process Flow\\n\\n### Step 1: Data Acquisition\\n\\nDownload the raw image dataset into a directory. Categorize these images into their respective category directories. Make sure that images are of the same type, JPEG recommended. We will also process the metadata and store it in a serialized file, CSV recommended. \\n\\n### Step 2: Encoder Fine-tuning\\n\\nDownload the pre-trained image model and add two additional layers on top of that: the first layer is a feature vector layer and the second layer is the classification layer. We will only train these 2 layers on our data and after training, we will select the feature vector layer as the output of our fine-tuned encoder. After fine-tuning the model, we will save the feature extractor for later use.\\n\\n![Fig: a screenshot of encoder fine-tuning process](/img/content-blog-raw-blog-image-similarity-system-untitled-1.png)\\n\\nFig: a screenshot of encoder fine-tuning process\\n\\n### Step 3: Image Vectorization\\n\\nNow, we will use the encoder (prepared in step 2) to encode the images (prepared in step 1). We will save feature vector of each image as an array in a directory. After processing, we will save these embeddings for later use.\\n\\n### Step 4: Metadata and Indexing\\n\\nWe will assign a unique id to each image and create dictionaries to locate information of this image: 1) Image id to Image name dictionary, 2) Image id to image feature vector dictionary, and 3) (optional) Image id to metadata product id dictionary. We will also create an image id to image feature vector indexing. Then we will save these dictionaries and index object for later use.\\n\\n### Step 5: API Call\\n\\nWe will receive an image from user, encode it with our image encoder, find TopK similar vectors using Indexing object, and retrieve the image (and metadata) using dictionaries. We send these images (and metadata) back to the user.\\n\\n# Deployment\\n\\nThe API was deployed on AWS cloud infrastructure using AWS Elastic Beanstalk service.\\n\\n![/img/content-blog-raw-blog-image-similarity-system-untitled-2.png](/img/content-blog-raw-blog-image-similarity-system-untitled-2.png)"},{"id":"/2021/10/01/insurance-personalization","metadata":{"permalink":"/ai-kb/blog/2021/10/01/insurance-personalization","source":"@site/blog/2021-10-01-insurance-personalization.mdx","title":"Insurance Personalization","description":"Author: Alexsoft","date":"2021-10-01T00:00:00.000Z","formattedDate":"October 1, 2021","tags":[{"label":"insurance","permalink":"/ai-kb/blog/tags/insurance"},{"label":"personalization","permalink":"/ai-kb/blog/tags/personalization"}],"readingTime":9.355,"truncated":false,"authors":[{"name":"Sparsh Agarwal","title":"Principal Developer","url":"https://github.com/sparsh-ai","imageURL":"https://avatars.githubusercontent.com/u/62965911?v=4","key":"sparsh"}],"frontMatter":{"title":"Insurance Personalization","authors":"sparsh","tags":["insurance","personalization"]},"prevItem":{"title":"Image Similarity System","permalink":"/ai-kb/blog/2021/10/01/image-similarity-system"},"nextItem":{"title":"Name & Address Parsing","permalink":"/ai-kb/blog/2021/10/01/name-&-address-parsing"}},"content":"Author: [Alexsoft](https://www.altexsoft.com/blog/personalized-insurance/)\\n\\nIn a hyper-connected world, where advanced analytics and smart devices constantly re-assess and monitor risks, the traditional once-a-year insurance policy looks increasingly irrelevant and static. Insurance will become a breathing and living thing that shrinks and scales with time to accommodate the changing risks in the clients\u2019 daily lives. As technology continues to expand, real-time data from connected devices and predictive analysis from AIs and machine learning will enhance personalized insurance to benefit the client and insurer.\\n\\nTo satisfy the expectations of clients, insurers may need to go beyond the personalization of marketing communication and start personalizing product bundles for individuals.\\n\\n## What is personalized insurance?\\n\\n**Personalized insurance**\xa0is the process of reaching insurance customers with targeted pricing, offers, and messages at the right time. Personalization spans across various types of insurance services, from health to property insurance.\\n\\nSome insurers are already defining themselves as trusted advisors aiding people in navigating, anticipating, and eliminating risks rather than just paying the compensation when things go wrong.\\n\\nFor example, these companies use customer data from wearable and smart devices to monitor the user\u2019s lifestyle. If the user\u2019s data indicate the emergence of a serious medical condition, they can send the customer content designed to change their detrimental lifestyle or recommend immediate treatment. When the customer stays fit, healthy and does not carry out risky activities, their insurance cost will be decreased.\\n\\n![/img/content-blog-raw-blog-insurance-personalization-untitled.png](/img/content-blog-raw-blog-insurance-personalization-untitled.png)\\n\\nInsurers can provide personalization to customers at different levels:\\n\\n- **Personalized product bundles.**\xa0The insurer offers a wide range of products such as health, car, life, and property insurance. So, clients can choose the specific products they want and group them in a bundle.\\n- **Personalized communications.**\xa0Insurers use data collected from smart devices to notify customers about harmful activities and lifestyles. They also send recommendations on lifestyle changes. Some insurers take a step further to provide clients with incentives for a healthy lifestyle.\\n- **Personalized insurance quote.**\xa0Customers are able to adjust the price of their insurance premiums by turning off the ones they don\u2019t need at any time. Some insurers enable automatic quote adjustments depending on customer\u2019s behavior (e.g., driving habits) or lifestyle choices (e.g., exercising).\\n\\n### Why is it important?\\n\\nCollecting and analyzing user data is vital in personalizing products based on individual behavior and preferences. In addition, insurers should use this data to enhance external relationships with their customers and guide their internal processes. This will eventually lead to delightful customer experiences and efficient operations.\\n\\nPersonalized insurance is important for many reasons:\\n\\n**Customers expect personalized treatment.**\xa0Every customer wants to feel special, and the personalization of your services and products will do just that. It will make them stay loyal to you. Moreover, customers are open for personalization. According to the\xa0[Accenture study](https://www.accenture.com/_acnmedia/PDF-95/Accenture-2019-Global-Financial-Services-Consumer-Study.pdf#zoom=50), 95 percent of new customers are ready to share their data in exchange for personalized insurance services. And about 58 percent of conservative users would be willing to do so.\\n\\n**Driving more effective sales and increasing revenue.**\xa0Personalization benefits your sales and income in two ways. First, lots of people are ready to share their data with you in exchange for incentives and reduced premiums. Secondly, having access to clients\u2019 data gives you the ability to target people who are already interested in your product, thereby increasing sales and revenue at a lower cost. You will be able to reach your customers at the right time and with the product they need.\\n\\n**Streamlining operations and working with customers more accurately.**\xa0Having an insight into customer preferences and behavior is crucial if you want to provide personalized services. Data obtained from social media activity, fitness trackers, GPS, and other tech can help you serve customers better.\\n\\n## Success stories\\n\\n### Lemonade\\n\\nUse of AI and chatbots to personalize communications. \\n\\nLemonade is a US insurance company that uses Maya \u2013 an AI-powered bot, to collect and analyze customer data. Maya acts as a virtual assistant that gets information, provides quotes, and handles payments. It also has the ability to provide customized answers to user\u2019s questions and even help them make changes to existing policies. Lemonade uses Natural Action Synthesis and Natural Language Processing to ensure that Maya gets smarter the more it chats. This is possible because their machine learning model is retrained almost daily.\\n\\n![/img/content-blog-raw-blog-insurance-personalization-untitled-1.png](/img/content-blog-raw-blog-insurance-personalization-untitled-1.png)\\n\\nOn top of that, the company uses big data analytics to quantify losses and predict risks by placing the client into a risk group and quoting a relevant premium. Customers are grouped according to their risk behaviors. The groups are created using algorithms that collect extensive customer data, such as health conditions.\\n\\n### Cover\\n\\n**[Cover](https://cover.com/)**\xa0is a US-based insurance metasearch company that notifies its clients of price drops for their premiums. Their technology works by scanning the market, looking for discounted and lowered prices of insurance premiums for their clients. Cover blends automation, mobile technology, and expert advice to provide customers with high-quality insurance protection at the best prices.\\n\\nCover compares with policy data and prices from over 30 different insurers. From the start, the customers need to provide answers to some questions, which will be used to match the client with a policy that suits their needs.\\n\\n### Oscar\\n\\n**[Oscar](https://www.hioscar.com/)**\xa0is a health insurer that provides its clients with a concierge team of medical professionals who give health advice and help them know if they see the best specialist for their specific health condition. They also help with finding the best doctors that accept Oscar insurance and manage and treat chronic conditions. Also, they set aside a separate concierge team in cases of emergencies that helps with the patient\u2019s discharge and follow-up care.\\n\\nOscar\u2019s mobile app acts as an intermediary between the user and the health system. The platform facilitates the customer\u2019s interaction with their healthcare professionals. Clients can receive their lab reports, medical records, physician recommendations, and virtual care from the app. Oscar has also improved its high-touch services, including telemedicine and an \u201cAsk your concierge\u201d feature that connects users with a health insurance advice team.\\n\\n![/img/content-blog-raw-blog-insurance-personalization-untitled-2.png](/img/content-blog-raw-blog-insurance-personalization-untitled-2.png)\\n\\n### Alllstate\\n\\nAllstate is an auto insurance company that offers personalized car insurance to its customers using telematics programs called Drivewise and Milewise. Drivewise is offered through a mobile app that monitors the customers driving behavior and provides feedback after each drive. Customers also receive incentives for safe driving. From the app interface, clients can check their rewards and driving behavior for the last 100 trips. The customer\u2019s premium is then calculated based on factors like speeding, abrupt braking, and time of the trip. One of the nice things about Drivewise is that even those who do not have an Allstate care insurance policy can participate in this program. Their Milewise program, as the name suggests, lets customers pay insurance based on the miles covered. So, the app monitors the distance covered by the car, and low-mileage drivers can save on insurance.\\n\\n![/img/content-blog-raw-blog-insurance-personalization-untitled-3.png](/img/content-blog-raw-blog-insurance-personalization-untitled-3.png)\\n\\n## How to approach personalization?\\n\\n![/img/content-blog-raw-blog-insurance-personalization-untitled-4.png](/img/content-blog-raw-blog-insurance-personalization-untitled-4.png)\\n\\nBefore fully investing in personalization, you need to carefully plan your approach. This will ensure you have all the pieces for success, and it will help you follow through with your plan.\\n\\n### Explore existing data\\n\\nHaving customer data is the minimum requirement to provide personalized services. First, you need to envision the type of personalization you want to offer. Then, make sure you have data collection channels that provide you with relevant data needed for your tasks. For instance, some of your documents may contain the required information, and you have to digitize, structure those, or extract specific details for that. So, you should audit your current information and data collection mechanisms to estimate whether you\u2019ll need any additional effort to gather this data. For instance, you may want to use\xa0[intelligent document processing](https://www.altexsoft.com/blog/intelligent-document-processing/).\\n\\n### Engage data scientists to make the proof of concept and carry out A/B tests\\n\\nYour vision on personalization may not work for every business model. Or your data quality may be low to reach project feasibility. We\u2019ve talked about that while explaining how to approach\xa0[ROI\xa0calculations\xa0with machine learning projects](https://www.altexsoft.com/blog/business/how-to-estimate-roi-and-costs-for-machine-learning-and-data-science-projects/). So, you need to present the data you have to a data science team to run several experiments and build prototypes. Once they are ready, you can roll out your new algorithms for a subset of customers to run A/B tests. Their results may show that the conventional approaches work better for you or help iterate on your assumptions.\\n\\n### Invest in data infrastructure\\n\\nIf the A/B tests show that personalization will work for your business model, that is where automation comes into play. You can start investing in data infrastructure and\xa0[analytical pipelines](https://www.altexsoft.com/blog/data-pipeline-components-and-types/)\xa0to automate data collection and analysis mechanisms.\\n\\nYou\u2019ll need a\xa0[data engineering team](https://www.altexsoft.com/blog/datascience/what-is-data-engineering-explaining-data-pipeline-data-warehouse-and-data-engineer-role/)\xa0for that. These specialists set up connections with data sources, such as mobile, IoT, and telematics devices, enable automatic data preparation, configure storages, and integrate your infrastructure with business intelligence software that helps explore and visualize data.\\n\\n### Continuously learn your customers\u2019 preferences and needs\\n\\nThe data you collect is only as good as the insights gained from it. That is why it is vital to have a\xa0[comprehensive analytic solution](https://www.altexsoft.com/blog/business/complete-guide-to-business-intelligence-and-analytics-strategy-steps-processes-and-tools/). A high-quality analytic software will transform the data into your most valuable asset. This data will be used to improve product development, make more accurate decisions, and provide personalized services to your customers.\\n\\n### Iterate on your infrastructure and algorithms\\n\\nPersonalization isn\u2019t a one-time project. Whether you apply machine learning or build personalization based on rule-based systems, you still have to revisit your technology, continuously gather new data, and adapt your workflows.\\n\\n### Ensure a personalized cross-channel experience\\n\\nSince the data collected from IoT devices and other tech is vital for personalization, it is important to make the customer experience seamless across different communication channels. Therefore, the customer should always be provided with the same level of personalization regardless of the touchpoint.\\n\\n## Challenges\\n\\n**Personalization is financially intensive.**\xa0The ability of insurers to personalize insurance differs only marginally between marketing communications and products. Most of them, especially startups, do not have the funds to implement advanced technologies like machine learning needed for personalized insurance. However, insurers do not need to start with all the levels of personalization. They can often start by customizing their customer service, gathering data and insights, and then gradually developing towards more complex systems.\\n\\n**Complex process involving multiple parties.**\xa0Also, it is difficult to balance personalization with financial targets, especially when establishing a price for risk. In-depth personalization of insurance must use data analytics from different sources to ensure that personalized offers reflect the client\u2019s needs as well as the profitability and risks implications for the company.\\n\\n**Customer data is heavily regulated.**\xa0Customer data from different sources are subject to industry regulations and privacy concerns. It is often a difficult task to obtain approval from regulators to use this data. Also, customers are becoming more aware of how companies are using their data and approve strict regulations. That is why laws such as General Data Protection Regulation (GDPR) and California Consumer Privacy Act (CCPA) have been passed, which gives customers more control over their data. Insurers can address this barrier by explaining to people how their systems work and how personal data is used. Read more on\xa0[explainable machine learning](https://www.altexsoft.com/blog/interpretability-machine-learning/)\xa0in our dedicated article. Besides being open, insurers can provide clients with incentives and other services for free in exchange for access to personal data."},{"id":"/2021/10/01/name-&-address-parsing","metadata":{"permalink":"/ai-kb/blog/2021/10/01/name-&-address-parsing","source":"@site/blog/2021-10-01-name-&-address-parsing.mdx","title":"Name & Address Parsing","description":"/img/content-blog-raw-blog-name-&-address-parsing-untitled.png","date":"2021-10-01T00:00:00.000Z","formattedDate":"October 1, 2021","tags":[{"label":"app","permalink":"/ai-kb/blog/tags/app"},{"label":"flask","permalink":"/ai-kb/blog/tags/flask"},{"label":"ner","permalink":"/ai-kb/blog/tags/ner"},{"label":"nlp","permalink":"/ai-kb/blog/tags/nlp"}],"readingTime":3.8,"truncated":false,"authors":[{"name":"Sparsh Agarwal","title":"Principal Developer","url":"https://github.com/sparsh-ai","imageURL":"https://avatars.githubusercontent.com/u/62965911?v=4","key":"sparsh"}],"frontMatter":{"title":"Name & Address Parsing","authors":"sparsh","tags":["app","flask","ner","nlp"]},"prevItem":{"title":"Insurance Personalization","permalink":"/ai-kb/blog/2021/10/01/insurance-personalization"},"nextItem":{"title":"Object Detection Hands-on Exercises","permalink":"/ai-kb/blog/2021/10/01/object-detection-hands-on-exercises"}},"content":"![/img/content-blog-raw-blog-name-&-address-parsing-untitled.png](/img/content-blog-raw-blog-name-&-address-parsing-untitled.png)\\n\\n# Introduction\\n\\nCreate an API that can parse and classify names and addresses given a string. We tried [probablepeople](https://github.com/datamade/probablepeople) and [usaddress](https://github.com/datamade/usaddress). These work well separately but need the functionality of these packages combined, and better accuracy than what probablepeople provides.\\nFor the API, I\'d like to mimic [this](https://parserator.datamade.us/api-docs/) with some minor modifications.\\nA few examples: \\n\\n- \\"KING JOHN A 5643 ROUTH CREEK PKWY #1314 RICHARDSON TEXAS 750820146 UNITED STATES OF AMERICA\\" would return type: person; first_name: JOHN; last_name: KING; middle: A; street_address: 5643 ROUTH CREEK PKWY #1314; city: RICHARDSON; state: TEXAS; zip: 75082-0146; country: UNITED STATES OF AMERICA.\\n- \\"THRM NGUYEN LIVING TRUST 2720 SUMMERTREE CARROLLTON HOUSTON TEXAS 750062646 UNITED STATES OF AMERICA\\" would return type: entity; name: THRM NGUYEN LIVING TRUST; street_address: 2720 SUMMERTREE CARROLLTON; state: TEXAS; city: HOUSTON; zip: 75006-2646; country: UNITED STATES OF AMERICA.\\n\\n# Modeling Approach\\n\\n### List of Entities\\n\\nList of Entities A - Person, Household, Corporation\\n\\nList of Entities B - Person First name, Person Middle name, Person Last name, Street address, City, State, Pincode, Country, Company name\\n\\n### Endpoint Configuration\\n\\n**OOR Endpoint**\\n\\nInput Instance: ANDERSON, EARLINE 1423 NEW YORK AVE FORT WORTH, TX 76104 7522\\n\\n```\\nOutput Tags:-\\n<Type> - Person/Household/Corporation\\n<GivenName>, <MiddleName>, <Surname> - if Type Person/Household\\n<Name> - Full Name - if Type Person \\n<Name> - Household - if Type Household\\n<Name> - Corporation - If Type Corporation\\n<Address> - Full Address\\n<StreetAddress>, <City>, <State>, <Zipcode>, <Country>\\n~~NameConfidence, AddrConfidence~~\\n```\\n\\n**Name Endpoint**\\n\\nInput Instance: ANDERSON, EARLINE\\n\\n```\\nOutput Tags:-\\n\\n- <Type> - Person/Household/Corporation\\n- <GivenName>, <MiddleName>, <Surname> - if Type Person/Household\\n- <Name> - Full Name - if Type Person\\n- <Name> - Household - if Type Household\\n- <Name> - Corporation - If Type Corporation\\n- ~~NameConfidence~~\\n```\\n\\n**Address Endpoint**\\n\\nInput Instance: 1423 NEW YORK AVE FORT WORTH, TX 76104 7522\\n\\n```\\nOutput Tags:-\\n\\n- <Address> - Full Address\\n- <StreetAddress>, <City>, <State>, <Zipcode>, <Country>\\n- ~~AddrConfidence~~\\n```\\n\\n### Process Flow\\n\\n- Pytorch Flair NER model\\n- Pre trained word embeddings\\n- Additional parsing models on top of name tags\\n- Tagging of 1000+ records to create training data\\n- Deployment as REST api with 3 endpoints - name parse, address parse and whole string parse\\n\\n# Framework\\n\\n![/img/content-blog-raw-blog-name-&-address-parsing-untitled-1.png](/img/content-blog-raw-blog-name-&-address-parsing-untitled-1.png)\\n\\n![/img/content-blog-raw-blog-name-&-address-parsing-untitled-2.png](/img/content-blog-raw-blog-name-&-address-parsing-untitled-2.png)\\n\\n# Tagging process\\n\\nI used Doccano ([https://github.com/doccano/doccano](https://github.com/doccano/doccano)) for labeling the dataset. This tool is open-source and free to use. I deployed it with a one-click Heroku service (fig 1). After launching the app, log in with the provided credentials, and create a project (fig 2). Create the labels and upload the dataset (fig 3). Start the annotation process (fig 4). Now after enough annotations (you do not need complete all annotations in one go), go back to projects > edit section and export the data (fig 5). Bring the exported JSON file in python and run the model training code. The whole model will automatically get trained on the new annotations. To make the training faster, you can use Nvidia GPU support.\\n\\n![fig 1: screenshot taken from Doccano\'s github page](/img/content-blog-raw-blog-name-&-address-parsing-untitled-3.png)\\n\\nfig 1: screenshot taken from Doccano\'s github page\\n\\n![fig 2: Doccano\'s deployed app homepage](/img/content-blog-raw-blog-name-&-address-parsing-untitled-4.png)\\n\\nfig 2: Doccano\'s deployed app homepage\\n\\n![fig 3: create the labels. I defined these labels for my project](/img/content-blog-raw-blog-name-&-address-parsing-untitled-5.png)\\n\\nfig 3: create the labels. I defined these labels for my project\\n\\n![fig 5: export the annotations](/img/content-blog-raw-blog-name-&-address-parsing-untitled-6.png)\\n\\nfig 5: export the annotations\\n\\n# Model\\n\\nI first tried the Spacy NER blank model but it was not giving high-quality results. So I moved to the PyTorch Flair NER model. This model was a way faster (5 min training because of GPU compatibility comparing to 1-hour Spacy training time) and also much more accurate. F1 results for all tags were near perfect (score of 1).  This score will increase further with more labeled data. This model is production-ready.\\n\\n# Inference\\n\\nFor OOR, I directly used the model\'s output for core tagging and created the aggregated tags like recipient (aggregation of name tags) and address (aggregation of address tags like city and state) using simple conditional concatenation. For only Name and only Address inference, I added the dummy address in name text and dummy name in address text. This way, I passed the text in same model and later on filtered the required tags as output. \\n\\n### API\\n\\nI used Flask REST framework in Python to build the API with 3 endpoints. This API is production-ready.\\n\\n# Results and Discussion\\n\\n- 0.99 F1 score on 6 out of 8 tags & 0.95+ F1 score on other 2 tags\\n- API inference time of less than 1 second on single CPU"},{"id":"/2021/10/01/object-detection-hands-on-exercises","metadata":{"permalink":"/ai-kb/blog/2021/10/01/object-detection-hands-on-exercises","source":"@site/blog/2021-10-01-object-detection-hands-on-exercises.mdx","title":"Object Detection Hands-on Exercises","description":"We are going to discuss the following 4 use cases:","date":"2021-10-01T00:00:00.000Z","formattedDate":"October 1, 2021","tags":[{"label":"object detection","permalink":"/ai-kb/blog/tags/object-detection"},{"label":"vision","permalink":"/ai-kb/blog/tags/vision"}],"readingTime":3.165,"truncated":false,"authors":[{"name":"Sparsh Agarwal","title":"Principal Developer","url":"https://github.com/sparsh-ai","imageURL":"https://avatars.githubusercontent.com/u/62965911?v=4","key":"sparsh"}],"frontMatter":{"title":"Object Detection Hands-on Exercises","authors":"sparsh","tags":["object detection","vision"]},"prevItem":{"title":"Name & Address Parsing","permalink":"/ai-kb/blog/2021/10/01/name-&-address-parsing"},"nextItem":{"title":"Object detection with OpenCV","permalink":"/ai-kb/blog/2021/10/01/object-detection-with-opencv"}},"content":"We are going to discuss the following 4 use cases:\\n\\n1. Detect faces, eyes, pedestrians, cars, and number plates using OpenCV haar cascade classifiers\\n2. Streamlit app for MobileNet SSD Caffe Pre-trained model\\n3. Streamlit app for various object detection models and use cases\\n4. Detect COCO-80 class objects in videos using TFHub MobileNet SSD model\\n\\n### Use Case 1 -  **Object detection with OpenCV**\\n\\n**Face detection** - We will use the frontal face Haar cascade classifier model to detect faces in the given image. The following function first passes the given image into the classifier model to detect a list of face bounding boxes and then runs a loop to draw a red rectangle box around each detected face in the image:\\n\\n```python\\ndef detect_faces(fix_img):\\n    face_rects = face_classifier.detectMultiScale(fix_img)\\n    for (x, y, w, h) in face_rects:\\n        cv2.rectangle(fix_img,\\n                     (x,y),\\n                     (x+w, y+h),\\n                     (255,0,0),\\n                     10)\\n    return fix_img\\n```\\n\\n**Eyes detection** - The process is almost similar to the face detection process. Instead of frontal face Haar cascade, we will use the eye detection Haar cascade model.\\n\\n![Input image](/img/content-blog-raw-blog-object-detection-with-opencv-untitled.png)\\n\\nInput image\\n\\n![detected faces and eyes in the image](/img/content-blog-raw-blog-object-detection-with-opencv-untitled-1.png)\\n\\ndetected faces and eyes in the image\\n\\n**Pedestrian detection** - We will use the full-body Haar cascade classifier model for pedestrian detection. We will apply this model to a video this time. The following function will run the model on each frame of the video to detect the pedestrians:\\n\\n```python\\n# While Loop\\nwhile cap.isOpened():\\n    # Read the capture\\n\\t\\tret, frame = cap.read()\\n    # Pass the Frame to the Classifier\\n\\t\\tbodies = body_classifier.detectMultiScale(frame, 1.2, 3)\\n    # if Statement\\n\\t\\tif ret == True:\\n        # Bound Boxes to Identified Bodies\\n\\t\\t\\t\\tfor (x,y,w,h) in bodies:\\n            cv2.rectangle(frame,\\n                         (x,y),\\n                         (x+w, y+h),\\n                         (25,125,225),\\n                         5)\\n            cv2.imshow(\'Pedestrians\', frame) \\n        # Exit with Esc button\\n\\t\\t\\t\\tif cv2.waitKey(1) == 27:\\n            break  \\n    # else Statement\\n\\t\\telse:\\n        break\\n    \\n# Release the Capture & Destroy All Windows\\ncap.release()\\ncv2.destroyAllWindows()\\n```\\n\\n**Car detection** - The process is almost similar to the pedestrian detection process. Again, we will use this model on a video. Instead of people Haar cascade, we will use the car cascade model.\\n\\n**Car number plate detection** - The process is almost similar to the face and eye detection process. We will use the car number plate cascade model.\\n\\n*You can find the code [here](https://github.com/sparsh-ai/0D7ACA15) on Github.*\\n\\n### Use Case 2 - MobileNet SSD Caffe Pre-trained model\\n\\n*You can play with the live app [here](https://share.streamlit.io/sparsh-ai/streamlit-5a407279/app.py). Souce code is available* [here](https://github.com/sparsh-ai/streamlit-489fbbb7) *on Github.*\\n\\n### Use Case 3 - YOLO Object Detection App\\n\\n*You can play with the live app* [*here](https://share.streamlit.io/sparsh-ai/streamlit-489fbbb7/app.py). Source code is available [here](https://github.com/sparsh-ai/streamlit-5a407279/tree/master) on Github.*\\n\\nThis app can detect COCO 80-classes using three different models - Caffe MobileNet SSD, Yolo3-tiny, and Yolo3. It can also detect faces using two different models - SSD Res10 and OpenCV face detector.  Yolo3-tiny can also detect fires.\\n\\n![/img/content-blog-raw-blog-object-detection-with-yolo3-untitled.png](/img/content-blog-raw-blog-object-detection-with-yolo3-untitled.png)\\n\\n![/img/content-blog-raw-blog-object-detection-with-yolo3-untitled-1.png](/img/content-blog-raw-blog-object-detection-with-yolo3-untitled-1.png)\\n\\n### Use Case 4 - TFHub MobileNet SSD on Videos\\n\\nIn this section, we will use the MobileNet SSD object detection model from TFHub. We will apply it to videos. We can load the model using the following command:\\n\\n```python\\nmodule_handle = \\"https://tfhub.dev/google/openimages_v4/ssd/mobilenet_v2/1\\"\\ndetector = hub.load(module_handle).signatures[\'default\']\\n```\\n\\nAfter loading the model, we will capture frames using OpenCV video capture method, and pass each frame through the detection model:\\n\\n```python\\ncap = cv2.VideoCapture(\'/content/Spectre_opening_highest_for_a_James_Bond_film_in_India.mp4\')\\nfor i in range(1,total_frames,200):\\n    cap.set(cv2.CAP_PROP_POS_FRAMES,i)\\n    ret,frame = cap.read()\\n    frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\\n    run_detector(detector,frame)\\n```\\n\\nHere are some detected objects in frames: \\n\\n![/img/content-blog-raw-blog-object-detection-hands-on-exercises-untitled.png](/img/content-blog-raw-blog-object-detection-hands-on-exercises-untitled.png)\\n\\n![/img/content-blog-raw-blog-object-detection-hands-on-exercises-untitled-1.png](/img/content-blog-raw-blog-object-detection-hands-on-exercises-untitled-1.png)\\n\\n![/img/content-blog-raw-blog-object-detection-hands-on-exercises-untitled-2.png](/img/content-blog-raw-blog-object-detection-hands-on-exercises-untitled-2.png)\\n\\n*You can find the code [here](https://gist.github.com/sparsh-ai/32ff6fe8c073f6be5d893029e4dc2960) on Github.*\\n\\n---\\n\\n*Congrats! In the next post of this series, we will cover 5 exciting use cases - 1) detectron 2 object detection fine-tuning on custom class, 2) Tensorflow Object detection API inference, fine-tuning, and few-shot learning, 3) Inference with 6 pre-trained models, 4) Mask R-CNN object detection app, and 5) Logo detection app deployment as a Rest API using AWS elastic Beanstalk.*"},{"id":"/2021/10/01/object-detection-with-opencv","metadata":{"permalink":"/ai-kb/blog/2021/10/01/object-detection-with-opencv","source":"@site/blog/2021-10-01-object-detection-with-opencv.mdx","title":"Object detection with OpenCV","description":"Face detection","date":"2021-10-01T00:00:00.000Z","formattedDate":"October 1, 2021","tags":[{"label":"object detection","permalink":"/ai-kb/blog/tags/object-detection"},{"label":"opencv","permalink":"/ai-kb/blog/tags/opencv"},{"label":"vision","permalink":"/ai-kb/blog/tags/vision"}],"readingTime":1.565,"truncated":false,"authors":[{"name":"Sparsh Agarwal","title":"Principal Developer","url":"https://github.com/sparsh-ai","imageURL":"https://avatars.githubusercontent.com/u/62965911?v=4","key":"sparsh"}],"frontMatter":{"title":"Object detection with OpenCV","authors":"sparsh","tags":["object detection","opencv","vision"]},"prevItem":{"title":"Object Detection Hands-on Exercises","permalink":"/ai-kb/blog/2021/10/01/object-detection-hands-on-exercises"},"nextItem":{"title":"Object detection with YOLO3","permalink":"/ai-kb/blog/2021/10/01/object-detection-with-yolo3"}},"content":"## **Face detection**\\n\\nWe will use the frontal face Haar cascade classifier model to detect faces in the given image. The following function first passes the given image into the classifier model to detect a list of face bounding boxes and then runs a loop to draw a red rectangle box around each detected face in the image:\\n\\n```python\\ndef detect_faces(fix_img):\\n    face_rects = face_classifier.detectMultiScale(fix_img)\\n    for (x, y, w, h) in face_rects:\\n        cv2.rectangle(fix_img,\\n                     (x,y),\\n                     (x+w, y+h),\\n                     (255,0,0),\\n                     10)\\n    return fix_img\\n```\\n\\n## **Eyes detection**\\n\\nThe process is almost similar to the face detection process. Instead of frontal face Haar cascade, we will use the eye detection Haar cascade model.\\n\\n![Input image](/img/content-blog-raw-blog-object-detection-with-opencv-untitled.png)\\n\\nInput image\\n\\n![detected faces and eyes in the image](/img/content-blog-raw-blog-object-detection-with-opencv-untitled-1.png)\\n\\ndetected faces and eyes in the image\\n\\n## **Pedestrian detection**\\n\\nWe will use the full-body Haar cascade classifier model for pedestrian detection. We will apply this model to a video this time. The following function will run the model on each frame of the video to detect the pedestrians:\\n\\n```python\\n# While Loop\\nwhile cap.isOpened():\\n    # Read the capture\\n\\t\\tret, frame = cap.read()\\n    # Pass the Frame to the Classifier\\n\\t\\tbodies = body_classifier.detectMultiScale(frame, 1.2, 3)\\n    # if Statement\\n\\t\\tif ret == True:\\n        # Bound Boxes to Identified Bodies\\n\\t\\t\\t\\tfor (x,y,w,h) in bodies:\\n            cv2.rectangle(frame,\\n                         (x,y),\\n                         (x+w, y+h),\\n                         (25,125,225),\\n                         5)\\n            cv2.imshow(\'Pedestrians\', frame) \\n        # Exit with Esc button\\n\\t\\t\\t\\tif cv2.waitKey(1) == 27:\\n            break  \\n    # else Statement\\n\\t\\telse:\\n        break\\n    \\n# Release the Capture & Destroy All Windows\\ncap.release()\\ncv2.destroyAllWindows()\\n```\\n\\n## **Car detection**\\n\\nThe process is almost similar to the pedestrian detection process. Again, we will use this model on a video. Instead of people Haar cascade, we will use the car cascade model.\\n\\n## **Car number plate detection**\\n\\nThe process is almost similar to the face and eye detection process. We will use the car number plate cascade model.\\n\\n*You can find the code [here](https://github.com/sparsh-ai/0D7ACA15) on Github.*"},{"id":"/2021/10/01/object-detection-with-yolo3","metadata":{"permalink":"/ai-kb/blog/2021/10/01/object-detection-with-yolo3","source":"@site/blog/2021-10-01-object-detection-with-yolo3.mdx","title":"Object detection with YOLO3","description":"Live app","date":"2021-10-01T00:00:00.000Z","formattedDate":"October 1, 2021","tags":[{"label":"app","permalink":"/ai-kb/blog/tags/app"},{"label":"streamlit","permalink":"/ai-kb/blog/tags/streamlit"},{"label":"vision","permalink":"/ai-kb/blog/tags/vision"}],"readingTime":1.975,"truncated":false,"authors":[{"name":"Sparsh Agarwal","title":"Principal Developer","url":"https://github.com/sparsh-ai","imageURL":"https://avatars.githubusercontent.com/u/62965911?v=4","key":"sparsh"}],"frontMatter":{"title":"Object detection with YOLO3","authors":"sparsh","tags":["app","streamlit","vision"]},"prevItem":{"title":"Object detection with OpenCV","permalink":"/ai-kb/blog/2021/10/01/object-detection-with-opencv"},"nextItem":{"title":"OCR experiments","permalink":"/ai-kb/blog/2021/10/01/ocr-experiments"}},"content":"## Live app\\n\\nThis app can detect COCO 80-classes using three different models - Caffe MobileNet SSD, Yolo3-tiny, and Yolo3. It can also detect faces using two different models - SSD Res10 and OpenCV face detector.  Yolo3-tiny can also detect fires.\\n\\n![/img/content-blog-raw-blog-object-detection-with-yolo3-untitled.png](/img/content-blog-raw-blog-object-detection-with-yolo3-untitled.png)\\n\\n![/img/content-blog-raw-blog-object-detection-with-yolo3-untitled-1.png](/img/content-blog-raw-blog-object-detection-with-yolo3-untitled-1.png)\\n\\n## Code\\n\\n```python\\nimport streamlit as st\\nimport cv2\\nfrom PIL import Image\\nimport numpy as np\\nimport os\\n\\nfrom tempfile import NamedTemporaryFile\\nfrom tensorflow.keras.preprocessing.image import img_to_array, load_img\\n\\ntemp_file = NamedTemporaryFile(delete=False)\\n\\nDEFAULT_CONFIDENCE_THRESHOLD = 0.5\\nDEMO_IMAGE = \\"test_images/demo.jpg\\"\\nMODEL = \\"model/MobileNetSSD_deploy.caffemodel\\"\\nPROTOTXT = \\"model/MobileNetSSD_deploy.prototxt.txt\\"\\n\\nCLASSES = [\\n    \\"background\\",\\n    \\"aeroplane\\",\\n    \\"bicycle\\",\\n    \\"bird\\",\\n    \\"boat\\",\\n    \\"bottle\\",\\n    \\"bus\\",\\n    \\"car\\",\\n    \\"cat\\",\\n    \\"chair\\",\\n    \\"cow\\",\\n    \\"diningtable\\",\\n    \\"dog\\",\\n    \\"horse\\",\\n    \\"motorbike\\",\\n    \\"person\\",\\n    \\"pottedplant\\",\\n    \\"sheep\\",\\n    \\"sofa\\",\\n    \\"train\\",\\n    \\"tvmonitor\\",\\n]\\nCOLORS = np.random.uniform(0, 255, size=(len(CLASSES), 3))\\n\\n@st.cache\\ndef process_image(image):\\n    blob = cv2.dnn.blobFromImage(\\n        cv2.resize(image, (300, 300)), 0.007843, (300, 300), 127.5\\n    )\\n    net = cv2.dnn.readNetFromCaffe(PROTOTXT, MODEL)\\n    net.setInput(blob)\\n    detections = net.forward()\\n    return detections\\n\\n@st.cache\\ndef annotate_image(\\n    image, detections, confidence_threshold=DEFAULT_CONFIDENCE_THRESHOLD\\n):\\n    # loop over the detections\\n    (h, w) = image.shape[:2]\\n    labels = []\\n    for i in np.arange(0, detections.shape[2]):\\n        confidence = detections[0, 0, i, 2]\\n\\n        if confidence > confidence_threshold:\\n            # extract the index of the class label from the `detections`,\\n            # then compute the (x, y)-coordinates of the bounding box for\\n            # the object\\n            idx = int(detections[0, 0, i, 1])\\n            box = detections[0, 0, i, 3:7] * np.array([w, h, w, h])\\n            (startX, startY, endX, endY) = box.astype(\\"int\\")\\n\\n            # display the prediction\\n            label = f\\"{CLASSES[idx]}: {round(confidence * 100, 2)}%\\"\\n            labels.append(label)\\n            cv2.rectangle(image, (startX, startY), (endX, endY), COLORS[idx], 2)\\n            y = startY - 15 if startY - 15 > 15 else startY + 15\\n            cv2.putText(\\n                image, label, (startX, y), cv2.FONT_HERSHEY_SIMPLEX, 0.5, COLORS[idx], 2\\n            )\\n    return image, labels\\n\\ndef main():\\n  selected_box = st.sidebar.selectbox(\\n    \'Choose one of the following\',\\n    (\'Welcome\', \'Object Detection\')\\n    )\\n    \\n  if selected_box == \'Welcome\':\\n      welcome()\\n  if selected_box == \'Object Detection\':\\n      object_detection() \\n\\ndef welcome():\\n  st.title(\'Object Detection using Streamlit\')\\n  st.subheader(\'A simple app for object detection\')\\n  st.image(\'test_images/demo.jpg\',use_column_width=True)\\n\\ndef object_detection():\\n  \\n  st.title(\\"Object detection with MobileNet SSD\\")\\n\\n  confidence_threshold = st.sidebar.slider(\\n    \\"Confidence threshold\\", 0.0, 1.0, DEFAULT_CONFIDENCE_THRESHOLD, 0.05)\\n\\n  st.sidebar.multiselect(\\"Select object classes to include\\",\\n  options=CLASSES,\\n  default=CLASSES\\n  )\\n\\n  img_file_buffer = st.file_uploader(\\"Upload an image\\", type=[\\"png\\", \\"jpg\\", \\"jpeg\\"])\\n\\n  if img_file_buffer is not None:\\n      temp_file.write(img_file_buffer.getvalue())\\n      image = load_img(temp_file.name)\\n      image = img_to_array(image)\\n      image = image/255.0\\n\\n  else:\\n      demo_image = DEMO_IMAGE\\n      image = np.array(Image.open(demo_image))\\n\\n  detections = process_image(image)\\n  image, labels = annotate_image(image, detections, confidence_threshold)\\n\\n  st.image(\\n      image, caption=f\\"Processed image\\", use_column_width=True,\\n  )\\n\\n  st.write(labels)\\n\\nmain()\\n```\\n\\n*You can play with the live app* [*here](https://share.streamlit.io/sparsh-ai/streamlit-489fbbb7/app.py). Source code is available [here](https://github.com/sparsh-ai/streamlit-5a407279/tree/master) on Github.*"},{"id":"/2021/10/01/ocr-experiments","metadata":{"permalink":"/ai-kb/blog/2021/10/01/ocr-experiments","source":"@site/blog/2021-10-01-ocr-experiments.mdx","title":"OCR experiments","description":"/img/content-blog-raw-blog-ocr-experiments-untitled.png","date":"2021-10-01T00:00:00.000Z","formattedDate":"October 1, 2021","tags":[{"label":"ocr","permalink":"/ai-kb/blog/tags/ocr"},{"label":"vision","permalink":"/ai-kb/blog/tags/vision"}],"readingTime":1.155,"truncated":false,"authors":[{"name":"Sparsh Agarwal","title":"Principal Developer","url":"https://github.com/sparsh-ai","imageURL":"https://avatars.githubusercontent.com/u/62965911?v=4","key":"sparsh"}],"frontMatter":{"title":"OCR experiments","authors":"sparsh","tags":["ocr","vision"]},"prevItem":{"title":"Object detection with YOLO3","permalink":"/ai-kb/blog/2021/10/01/object-detection-with-yolo3"},"nextItem":{"title":"PDF to Wordcloud via Mail","permalink":"/ai-kb/blog/2021/10/01/pdf-to-wordcloud-via-mail"}},"content":"![/img/content-blog-raw-blog-ocr-experiments-untitled.png](/img/content-blog-raw-blog-ocr-experiments-untitled.png)\\n\\n## 1. Tesseract\\n\\nTesseract is an open-source text recognition engine that is available under the Apache 2.0 license and its development has been sponsored by Google since 2006.\\n\\n[Notebook on nbviewer](https://nbviewer.jupyter.org/gist/sparsh-ai/2d1f533048a3655de625298c3dd32d47)\\n\\n## 2. EasyOCR\\n\\nReady-to-use OCR with 70+ languages supported including Chinese, Japanese, Korean and Thai. EasyOCR is built with Python and Pytorch deep learning library, having a GPU could speed up the whole process of detection. The detection part is using the CRAFT algorithm and the Recognition model is CRNN. It is composed of 3 main components, feature extraction (we are currently using Resnet), sequence labelling (LSTM) and decoding (CTC). EasyOCR doesn\u2019t have much software dependencies, it can directly be used with its API.\\n\\n[Notebook on nbviewer](https://nbviewer.jupyter.org/gist/sparsh-ai/12359606ee4127513c66fc3b4ff18e5b)\\n\\n## 3. KerasOCR\\n\\nThis is a slightly polished and packaged version of the Keras CRNN implementation and the published CRAFT text detection model. It provides a high-level API for training a text detection and OCR pipeline and out-of-the-box OCR models, and an end-to-end training pipeline to build new OCR models.\\n\\n[Notebook on nbviewer](https://nbviewer.jupyter.org/gist/sparsh-ai/2fcb764619baf5f56cf7122b1b2c527c)\\n\\n## 4. ArabicOCR\\n\\nIt is an OCR system for the Arabic language that converts images of typed text to machine-encoded text. It currently supports only letters (29 letters).  ArabicOCR aims to solve a simpler problem of OCR with images that contain only Arabic characters (check the dataset link below to see a sample of the images).\\n\\n[Notebook on nbviewer](https://nbviewer.jupyter.org/gist/sparsh-ai/26df76b78f8cd2018a068b284b7cfe56)"},{"id":"/2021/10/01/pdf-to-wordcloud-via-mail","metadata":{"permalink":"/ai-kb/blog/2021/10/01/pdf-to-wordcloud-via-mail","source":"@site/blog/2021-10-01-pdf-to-wordcloud-via-mail.mdx","title":"PDF to Wordcloud via Mail","description":"/img/content-blog-raw-blog-pdf-to-wordcloud-via-mail-untitled.png","date":"2021-10-01T00:00:00.000Z","formattedDate":"October 1, 2021","tags":[],"readingTime":1.015,"truncated":false,"authors":[{"name":"Sparsh Agarwal","title":"Principal Developer","url":"https://github.com/sparsh-ai","imageURL":"https://avatars.githubusercontent.com/u/62965911?v=4","key":"sparsh"}],"frontMatter":{"title":"PDF to Wordcloud via Mail","authors":"sparsh"},"prevItem":{"title":"OCR experiments","permalink":"/ai-kb/blog/2021/10/01/ocr-experiments"},"nextItem":{"title":"Personalized Unexpectedness in  Recommender Systems","permalink":"/ai-kb/blog/2021/10/01/personalized-unexpectedness-in-recommender-systems"}},"content":"![/img/content-blog-raw-blog-pdf-to-wordcloud-via-mail-untitled.png](/img/content-blog-raw-blog-pdf-to-wordcloud-via-mail-untitled.png)\\n\\n## Objective\\n\\nIntegrating PDF, Text, Wordcloud and Email functionalities in Python\\n\\n## Process Flow\\n\\n- Step 1 - I use PyPDF2 library to read PDF text in Python\\n- Step 2 - Import the supporting libraries\\n- Step 3 - Count No. of Pages for this pdf and extract text for each page using loop\\n- Step 4 - Build Text corpus by simply attaching text of next page to all the previous ones\\n- Step 5 - Creating word frequency dataframe by first splitting text into words and counting the frequency of each word\\n- Step 6.1 - Pre-process text i.e. removing stopwords (using nltk library), grouping common words.\\n- Step 6.2 - used regex to extract alphabets only, lower all chracters, and sorting as per decreasing order of frequency.\\n- Step 7 - Creating Wordcloud using matplotlib and wordcloud libraries\\n- Step 8 - Importing required libraries like smtplib, MIME, win32 for sending the mail\\n- Step 9 - Create outlook mail object with supporting data like filepath attachment, recepient address, mail body etc.\\n- Step 10 - Sending the mail with required wordcloud image file attached and checking if mail is received or not!\\n\\n## Code\\n\\n[Notebook on nbviewer](https://nbviewer.jupyter.org/gist/sparsh-ai/f1de48fd4fac199bcc95e1d136fbdfd0)"},{"id":"/2021/10/01/personalized-unexpectedness-in-recommender-systems","metadata":{"permalink":"/ai-kb/blog/2021/10/01/personalized-unexpectedness-in-recommender-systems","source":"@site/blog/2021-10-01-personalized-unexpectedness-in-recommender-systems.mdx","title":"Personalized Unexpectedness in  Recommender Systems","description":"Classical recommender systems typically provides familier items, which not only bores customer after some time, but create a critical bias problem also, generally known as filter bubble or echo chamber problem.","date":"2021-10-01T00:00:00.000Z","formattedDate":"October 1, 2021","tags":[{"label":"personalization","permalink":"/ai-kb/blog/tags/personalization"}],"readingTime":2.96,"truncated":false,"authors":[{"name":"Sparsh Agarwal","title":"Principal Developer","url":"https://github.com/sparsh-ai","imageURL":"https://avatars.githubusercontent.com/u/62965911?v=4","key":"sparsh"}],"frontMatter":{"title":"Personalized Unexpectedness in  Recommender Systems","authors":"sparsh","tags":["personalization"]},"prevItem":{"title":"PDF to Wordcloud via Mail","permalink":"/ai-kb/blog/2021/10/01/pdf-to-wordcloud-via-mail"},"nextItem":{"title":"Predicting Electronics Resale Price","permalink":"/ai-kb/blog/2021/10/01/predicting-electronics-resale-price"}},"content":"Classical recommender systems typically provides familier items, which not only bores customer after some time, but create a critical bias problem also, generally known as *filter bubble* or *echo chamber problem*. \\n\\nTo address this issue, instead of recommending best matching product all the time, we intentionally recommend a random product. For example, if a user subscribed to Netflix one month ago and watching action movies all the time. If we recommend another action movie, there is a high probability that user will click but keeping in mind the long-term user satisfaction and to address the filter bubble bias, we would recommend a comedy movie. Surprisingly, this strategy works!!\\n\\nThe most common metric is ***diversity*** factor but diversity only measures dispersion among recommended items. The better alternative is ***unexpectedness*** factor. It measures deviations of recommended items from user expectations and thus captures the concept of user surprise and allows recommender systems to break from the filter bubble. The goal is to provide novel, surprising and satisfying recommendations. \\n\\nIncluding session-based information into the design of an unexpected recommender system is beneficial. For example, it is more reasonable to recommend the next episode of a TV series to the user who has just finished the first episode, instead of recommending new types of videos to that person. On the other hand, if the user has been binge-watching the same TV series in one night, it is better to recommend something different to him or her.\\n\\n### Model\\n\\n![/img/content-blog-raw-blog-personalized-unexpectedness-in-recommender-systems-untitled.png](/img/content-blog-raw-blog-personalized-unexpectedness-in-recommender-systems-untitled.png)\\n\\n*Overview of the proposed PURS model. The base model estimates the click-through rate of certain user-item pairs, while the unexpected model captures the unexpectedness of the new recommendation as well as user perception towards unexpectedness.*\\n\\n### Offline Experiment Results\\n\\n![/img/content-blog-raw-blog-personalized-unexpectedness-in-recommender-systems-untitled-1.png](/img/content-blog-raw-blog-personalized-unexpectedness-in-recommender-systems-untitled-1.png)\\n\\n### Online A/B Test Results\\n\\nAuthors conducted the online A/B test at Alibaba-Youku, a major video recommendation platform from 2019-11 to 2019-12. During the testing period, they compared the proposed PURS model with the latest production model in the company. They measured the performance using standard business metrics: **VV** (Video View, average video viewed by each user), **TS** (Time Spent, average time that each user spends on the platform), **ID** (Impression Depth, average impression through one session) and **CTR** (Click-Through-Rate, the percentage of user clicking on the recommended video). They also measure the novelty of the recommended videos using the unexpectedness and coverage measures.\\n\\n![Represents statistical significance at the 0.95 level.](/img/content-blog-raw-blog-personalized-unexpectedness-in-recommender-systems-untitled-2.png)\\n\\nRepresents statistical significance at the 0.95 level.\\n\\n### Code Walkthrough\\n\\n> Note: PURS is *implemented in Tensorflow 1.x*\\n\\n**Unexpected attention ([model.py](https://github.com/lpworld/PURS/blob/master/model.py))**\\n\\n```python\\ndef unexp_attention(self, querys, keys, keys_id):\\n        \\"\\"\\"\\n        Same Attention as in the DIN model\\n        queries:     [Batchsize, 1, embedding_size]\\n        keys:        [Batchsize, max_seq_len, embedding_size]  max_seq_len is the number of keys(e.g. number of clicked creativeid for each sample)\\n        keys_id:     [Batchsize, max_seq_len]\\n        \\"\\"\\"\\n        querys = tf.expand_dims(querys, 1)\\n        keys_length = tf.shape(keys)[1] # padded_dim\\n        embedding_size = querys.get_shape().as_list()[-1]\\n        keys = tf.reshape(keys, shape=[-1, keys_length, embedding_size])\\n        querys = tf.reshape(tf.tile(querys, [1, keys_length, 1]), shape=[-1, keys_length, embedding_size])\\n\\n        net = tf.concat([keys, keys - querys, querys, keys*querys], axis=-1)\\n        for units in [32,16]:\\n            net = tf.layers.dense(net, units=units, activation=tf.nn.relu)\\n        att_wgt = tf.layers.dense(net, units=1, activation=tf.sigmoid)        # shape(batch_size, max_seq_len, 1)\\n        outputs = tf.reshape(att_wgt, shape=[-1, 1, keys_length], name=\\"weight\\")  #shape(batch_size, 1, max_seq_len)\\n        scores = outputs\\n        scores = scores / (embedding_size ** 0.5)       # scale\\n        scores = tf.nn.softmax(scores)\\n        outputs = tf.matmul(scores, keys)    #(batch_size, 1, embedding_size)\\n        outputs = tf.reduce_sum(outputs, 1, name=\\"unexp_embedding\\")   #(batch_size, embedding_size)\\n        return outputs\\n```\\n\\n**Unexpected metric calculation ([train.py](https://github.com/lpworld/PURS/blob/master/train.py))**\\n\\n```python\\ndef unexpectedness(sess, model, test_set):\\n    unexp_list = []\\n    for _, uij in DataInput(test_set, batch_size):\\n        score, label, user, item, unexp = model.test(sess, uij)\\n        for index in range(len(score)):\\n            unexp_list.append(unexp[index])\\n    return np.mean(unexp_list)\\n```\\n\\n### References\\n\\n1. [https://arxiv.org/pdf/2106.02771v1.pdf](https://arxiv.org/pdf/2106.02771v1.pdf)\\n2. [https://github.com/lpworld/PURS](https://github.com/lpworld/PURS)"},{"id":"/2021/10/01/predicting-electronics-resale-price","metadata":{"permalink":"/ai-kb/blog/2021/10/01/predicting-electronics-resale-price","source":"@site/blog/2021-10-01-predicting-electronics-resale-price.mdx","title":"Predicting Electronics Resale Price","description":"/img/content-blog-raw-blog-predicting-electronics-resale-price-untitled.png","date":"2021-10-01T00:00:00.000Z","formattedDate":"October 1, 2021","tags":[{"label":"regression","permalink":"/ai-kb/blog/tags/regression"}],"readingTime":1.875,"truncated":false,"authors":[{"name":"Sparsh Agarwal","title":"Principal Developer","url":"https://github.com/sparsh-ai","imageURL":"https://avatars.githubusercontent.com/u/62965911?v=4","key":"sparsh"}],"frontMatter":{"title":"Predicting Electronics Resale Price","authors":"sparsh","tags":["regression"]},"prevItem":{"title":"Personalized Unexpectedness in  Recommender Systems","permalink":"/ai-kb/blog/2021/10/01/personalized-unexpectedness-in-recommender-systems"},"nextItem":{"title":"Real-time news personalization with Flink","permalink":"/ai-kb/blog/2021/10/01/real-time-news-personalization-with-flink"}},"content":"![/img/content-blog-raw-blog-predicting-electronics-resale-price-untitled.png](/img/content-blog-raw-blog-predicting-electronics-resale-price-untitled.png)\\n\\n# Objective\\n\\nPredict the resale price based on brand, part id and purchase quantity\\n\\n# Milestones\\n\\n- Data analysis and discovery - What is the acceptable variance the model needs to meet in terms of similar part number and quantity?\\n- Model research and validation - Does the model meet the variance requirement? (Variance of the model should meet or be below the variance of the sales history)\\n- Model deployment - Traffic will increase 10 fold. So, model needs to be containerized or dockerized\\n- Training - Model needs to be trainable on new sales data. Methodology to accept or reject the variance of the newly trained model documented.\\n\\n# Deliverables\\n\\n1. Data Analysis and Discovery (identify target variance for pricing model in terms of similar part numbers and quantities). Analysis should be done on the 12 following quantity ranges: 1-4, 5-9, 10-24, 25-49, 50-99, 100-249, 250-499, 500-999, 1000-2499, 2500-4999, 5000-9999, 10000+.\\n\\n2. ModelA Training (Resale Value Estimation [$] (Brand+PartNo.+Quantity)\\n\\n3. ModelA Validation (variance analysis and comparison with sales history variance in terms of similar part numbers and quantities)\\n\\n4. ModelA Containerization\\n\\n5. ModelA re-training based on new sales data\\n\\n6. ScriptA to calculate variance for new sales data (feedback for training results)\\n\\n7. Documentation for re-training\\n\\n8. ModelA deployment and API\\n\\n# Modeling Approach\\n\\n### Framework\\n\\n- Fully connected regression neural network\\n- NLP feature extraction from part id\\n- Batch generator to feed large data in batches\\n- Hyperparameter tuning to find the best model fit\\n\\n### List of Variables\\n\\n- 2 years of sales history\\n- PRC\\n- PARTNO\\n- ORDER_NUMBER\\n- ORIG_ORDER_QTY\\n- UNIT_COST\\n- UNIT_REASLE\\n- UOM (UNIT OF MEASUREMENT)\\n\\n# Bucket of Ideas\\n\\n1. Increase n-gram range; e.g. in part_id ABC-123-23, these are 4-grams: ABC-, BC-1, C-12, -123, 123-, 23-2, 3-23; Idea is to see if increasing this range further will increase the model\'s performance\\n2. Employ Char-level LSTM to capture sequence information; e.g. in same part_id ABC-123-23, currently we are not maintaining sequence of grams, we don\'t know if 3-23 is coming at first or last; here, the idea is to see if lstm model can be employed to capture this sequence information to improve model\'s performance\\n3. New Loss function - including cost based loss"},{"id":"/2021/10/01/real-time-news-personalization-with-flink","metadata":{"permalink":"/ai-kb/blog/2021/10/01/real-time-news-personalization-with-flink","source":"@site/blog/2021-10-01-real-time-news-personalization-with-flink.mdx","title":"Real-time news personalization with Flink","description":"Overview","date":"2021-10-01T00:00:00.000Z","formattedDate":"October 1, 2021","tags":[{"label":"personalization","permalink":"/ai-kb/blog/tags/personalization"},{"label":"realtime","permalink":"/ai-kb/blog/tags/realtime"}],"readingTime":9.82,"truncated":false,"authors":[{"name":"Sparsh Agarwal","title":"Principal Developer","url":"https://github.com/sparsh-ai","imageURL":"https://avatars.githubusercontent.com/u/62965911?v=4","key":"sparsh"}],"frontMatter":{"title":"Real-time news personalization with Flink","authors":"sparsh","tags":["personalization","realtime"]},"prevItem":{"title":"Predicting Electronics Resale Price","permalink":"/ai-kb/blog/2021/10/01/predicting-electronics-resale-price"},"nextItem":{"title":"Semantic Similarity","permalink":"/ai-kb/blog/2021/10/01/semantic-similarity"}},"content":"## Overview\\n\\nNews recommendation system has a high degree of real-time because there will be a large number of news and hot spots at any time. Incremental updating, online learning, local updating and even reinforcement learning can make the recommender system quickly respond to the user\u2018s new behavior, and the premise of these updating strategies is that the sample itself has enough real-time information. In news recommendation system, the typical training sample is the user\u2019s click behavior data.\\n\\n### Why is the real-time nature of the recommendation system important?\\n\\nIntuitively, when users use personalized news applications, users expect to find articles that match their interests faster; when using short video services, they expect to \\"flash\\" content that they are interested in faster; when doing online shopping, I also hope to find the products that I like, faster. All recommendations highlight the word \\"fast\\", which is an intuitive manifestation of the \\"real-time\\" role of the recommendation system.\\n\\nFrom a professional point of view, the real-time performance of the recommendation system is also crucial, which is mainly reflected in the following two aspects:\\n\\n1. **The faster the update speed of the recommendation system is, the more it can reflect the user\'s recent user habits, and the more time-sensitive it can make recommendations to the user.**\\n2. **The faster the recommendation system is updated, the easier it is for the model to find the latest popular data patterns, and the more it can make the model react to find the latest fashion trends.**\\n\\n### The real-time nature of the \\"feature\\" of the recommendation system\\n\\nSuppose a user has watched a 10-minute \\"badminton teaching\\" video in its entirety. Then there is no doubt that the user is interested in the subject of \\"badminton\\". The system hopes to continue to recommend \\"badminton\\" related videos when the user turns the page next time. However, due to the lack of real-time features of the system, the user\u2019s viewing history cannot be fed back to the recommendation system in real time. As a result, the recommendation system learned that the user had watched the video \\"Badminton Teaching\\". It was already half an hour later. Has left the app. This is an example of recommendation failure caused by poor real-time performance of the recommendation system.\\n\\nIt is true that the next time the user opens the application, the recommendation system can use the last user behavior history to recommend \\"badminton\\" related videos, but the recommendation system undoubtedly loses what is most likely to increase user viscosity and increase user retention. opportunity.\\n\\n### The real-time nature of the \\"model\\" of the recommender system\\n\\nNo matter how strong the real-time feature is, the scope of influence is limited to the current user. Compared with the real-time nature of \\"features\\", the real-time nature of the recommendation system model is often considered from a more global perspective . The real-time nature of the feature attempts to describe a person with more accurate features, so that the recommendation system can give a recommendation result that is more in line with the person. The real-time nature of the model hopes to capture new data patterns at the global level faster and discover new trends and relevance.\\n\\nTake, for example, a large number of promotional activities on Double Eleven on an e-commerce website. The real-time nature of the feature will quickly discover the products that the user may be interested in based on the user\'s recent behavior, but will never find the latest preferences of similar users, the latest correlation information between the products, and the trend information of new activities.\\n\\nTo discover such global data changes, the model needs to be updated faster. The most important factor affecting the real-time performance of the model is the training method of the model.\\n\\n1. **Full update -** The most common way of model training is full update.\xa0The model will use all training samples in a certain period of time for retraining, and then replace the \\"outdated\\" model with the new trained model. However, the full update requires a large amount of training samples, so the training time required is longer; and the full update is often performed on offline big data platforms, such as spark+tensorflow, so the data delay is also longer, which leads to the full update It is the worst \\"real-time\\" model update method. In fact, for a model that has been trained, it is enough to learn only the newly added incremental samples, which is called incremental update.\\n2. **Incremental update (Incremental Learning)** - Incremental update only feeds newly added samples to the model for incremental learning . Technically, deep learning models often use stochastic gradient descent (SGD) and its variants for learning. The model\'s learning of incremental samples is equivalent to continuing to input incremental samples for gradient descent on the basis of the original samples. Therefore, based on the deep learning model, it is not difficult to change from full update to incremental update. But everything in engineering is a tradeoff, there is never a perfect solution, and incremental updates are no exception. Since only incremental samples are used for learning, the model also converges to the best point of the new sample after multiple epochs, and it is difficult to converge to the global best point of all the original samples + incremental samples. Therefore, in the actual recommendation system, the incremental update and the global update are often combined . After several rounds of incremental update, the global update is performed in a time window with a small business volume, and the model is corrected after the incremental update process. Accumulated errors in. Make trade-offs and trade-offs between \\"real-time performance\\" and \\"global optimization\\".\\n3. **Online learning** - \\"Online learning\\" is a further improvement of \\"incremental update\\", \\"incremental update\\" is to perform incremental update when a batch of new samples is obtained, and online learning is to update the model in real time every time a new sample is obtained. Online learning can also be implemented technically through SGD. But if you use the general SGD method, online learning will cause a very serious problem, that is, the sparsity of the model is very poor, opening too many \\"fragmented\\" unimportant features. We pay attention to the \\"sparseness\\" of the model in a sense that is also an engineering consideration. For example, in a model with an input vector of several million dimensions, if the sparsity of the model is good, the effect of the model can be maintained without affecting the model. , Only make the corresponding weight of the input vector of a very small part of the dimension non-zero, that is to say, when the model is online, the volume of the model is very small, which is undoubtedly beneficial to the entire model serving process. Both the memory space required to store the model and the speed of online inference will benefit from the sparsity of the model. If the SGD method is used to update the model, it is easier to generate a large number of features with small weights than the batch method, which increases the difficulty of model deployment and update. So in order to take into account the training effect and model sparsity in the online learning process, there are a lot of related researches. The most famous ones include Microsoft\'s RDA, Google\'s FOBOS and the most famous FTRL, etc.\\n4. **Partial model update** - Another improvement direction to improve the real-time performance of the model is to perform a partial update of the model. The general idea is to reduce the update frequency of the part with low training efficiency and increase the update frequency of the part with high training efficiency . This approach is representative of the GBDT+LR model of Facebook.\\n\\n![/img/content-blog-raw-blog-real-time-news-personalization-with-flink-untitled.png](/img/content-blog-raw-blog-real-time-news-personalization-with-flink-untitled.png)\\n\\n## Data pipeline of a typical news recommendation system\\n\\nWhen a user is exposed with a list of news articles, a page view events are sent to the backend server and when that user clicks on the news of interest, the action events are also sent to the backend server. After receiving these 2 event streams (page view and clicks), the backend server will send these user behaviour events to the message queue. And message queue finally stores these messages into the distributed file system, such as HDFS.\\n\\nFor model training, we need a training sample. The most common sampling technique is negative sampling. In this, we generate \'n\' negative samples for each positive event that we receive. Users will only generate behavior for some exposed news samples, which are positive samples, and the remaining exposure samples without behavior are negative samples. After generating positive and negative samples, the model can be trained.\\n\\nThe recommendation system with low real-time requirements can use batch processing technology (APACHE spark is a typical tool) to generate samples, as shown in the left figure. Set a timing task, and read the user behavior log and exposure log in the time window from HDFS every other period of time, such as one hour, to perform join operation, generate training samples, and then write the training samples back to HDFS, Then start the training update of the model.\\n\\n![/img/content-blog-raw-blog-real-time-news-personalization-with-flink-untitled-1.png](/img/content-blog-raw-blog-real-time-news-personalization-with-flink-untitled-1.png)\\n\\n### Problems\\n\\nOne obvious problem with batch processing is **latency**. The typical cycle of running batch tasks regularly is one hour, which means that there is a delay of at least one hour from sample generation to model training. Sometimes, if the batch platform is overloaded and the tasks need to be queued, the delay will be greater.\\n\\nAnother problem is the **boundary** problem. If page view (PV) data is generated at the end of the log time window selected by the batch task, the corresponding action data may fall into the next time window of the batch task, resulting in join failure and false negative samples.\\n\\nA related problem to this is the time synchronization problem. When a news item is exposed to the user, the user may click immediately after the PV data stream is generated, or the user may act after a few minutes, more than ten minutes, or even several hours. This means that after the PV data stream arrives, it needs to wait for a period of time to join with the action data stream. If the waiting time is too long, some samples (positive samples) that should have user behavior will be wrongly marked as negative samples because the user behavior has no time to return. Too long waiting time will damage and increase the system delay. Offline analysis of the delay distribution between the actual action data stream and PV data stream is a very typical exponential distribution.\\n\\n![/img/content-blog-raw-blog-real-time-news-personalization-with-flink-untitled-2.png](/img/content-blog-raw-blog-real-time-news-personalization-with-flink-untitled-2.png)\\n\\n## Apache Flink to the rescue\\n\\n### How Apache Flink solves the latency problem?\\n\\nIn order to enhance the real-time performance, we use Apache Flink framework to rewrite the sample generation logic with stream processing technology. As shown in the right figure above, after the user exposure and behavior logs generated by online services are written into the message queue, instead of waiting for them to drop to HDFS, we directly consume these message flows with Flink. At the same time, Flink reads the necessary feature information from the redis cache and generates the sample message stream directly. The sample message flow is written back to the Kafka queue, and downstream tensorflow can directly consume the message flow for model training.\\n\\n### How Apache Flink solved the boundary and synchronization problem?\\n\\nAs per the exponential distribution (analyzed on a private dataset of a news recommender app), most of the user behavior has reflow within a few minutes. And if few minutes is an acceptable delay, a simple solution is to set a time window with a compromise size. Flink provides window join to implement this logic.\\n\\n## References\\n\\n1. [https://developpaper.com/flink-streaming-processing-and-real-time-sample-generation-in-recommender-system/](https://developpaper.com/flink-streaming-processing-and-real-time-sample-generation-in-recommender-system/)\\n2. [https://zhuanlan.zhihu.com/p/74813776](https://zhuanlan.zhihu.com/p/74813776)\\n3. [https://zhuanlan.zhihu.com/p/75597761](https://zhuanlan.zhihu.com/p/75597761)"},{"id":"/2021/10/01/semantic-similarity","metadata":{"permalink":"/ai-kb/blog/2021/10/01/semantic-similarity","source":"@site/blog/2021-10-01-semantic-similarity.mdx","title":"Semantic Similarity","description":"/img/content-blog-raw-blog-semantic-similarity-untitled.png","date":"2021-10-01T00:00:00.000Z","formattedDate":"October 1, 2021","tags":[{"label":"nlp","permalink":"/ai-kb/blog/tags/nlp"},{"label":"similarity","permalink":"/ai-kb/blog/tags/similarity"}],"readingTime":1.67,"truncated":false,"authors":[{"name":"Sparsh Agarwal","title":"Principal Developer","url":"https://github.com/sparsh-ai","imageURL":"https://avatars.githubusercontent.com/u/62965911?v=4","key":"sparsh"}],"frontMatter":{"title":"Semantic Similarity","authors":"sparsh","tags":["nlp","similarity"]},"prevItem":{"title":"Real-time news personalization with Flink","permalink":"/ai-kb/blog/2021/10/01/real-time-news-personalization-with-flink"},"nextItem":{"title":"Short-video Background Music Recommender","permalink":"/ai-kb/blog/2021/10/01/short-video-background-music-recommender"}},"content":"![/img/content-blog-raw-blog-semantic-similarity-untitled.png](/img/content-blog-raw-blog-semantic-similarity-untitled.png)\\n\\n# Introduction\\n\\nDeliverable - Two paragraph-level distance outputs for L and Q, each has 35 columns. \\n\\nFor each paragraph, we need to calculate the L1 distance of consecutive sentences in this paragraph, and then generate the mean and standard deviation of all these distances for this paragraph. For example, say the paragraph 1 starts from sentence1 and ends with sentence 5. First, calculate the L1 distances for L1(1,2), L1(2,3), L1(3,4) and L1(4,5) and then calculate the mean and standard deviation of the 4 distances. In the end we got two measures for this paragraph: L1_m and L1_std. Similarly, we need to calculate the mean and standard deviation using L2 distance, plus a simple mean and deviation of the distances. We use 6 different embeddings: all dimensions of BERT embeddings, 100,200 and 300 dimensions of PCA Bert embeddings (PCA is a dimension reduction technique \\n\\nIn the end, we will have 35 columns for each paragraph : Paragraph ID +#sentences in the paragraph +(cosine_m, cosine_std,cossimillarity_m, cosimmilarity_std, L1_m, L1_std, L2_m, L2_std ) \u2013 by- ( all, 100, 200, 300)= 3+8*4. \\n\\nNote: for paragraph that only has 1 sentence, the std measures are empty.\\n\\n# Modeling Approach\\n\\n### Process Flow for Use Case 1\\n\\n1. Splitting paragraphs into sentences using 1) NLTK Sentence Tokenizer, 2) Spacy Sentence Tokenizer and, on two additional symbols `:` and `...`\\n2. Text Preprocessing: Lowercasing, Removing Non-alphanumeric characters, Removing Null records, Removing sentence records (rows) having less than 3 words.\\n3. TF-IDF vectorization\\n4. LSA over document-term matrix\\n5. Cosine distance calculation of adjacent sentences (rows)\\n\\n### Process Flow for Use Case 2\\n\\n- Split paragraphs into sentences\\n- Text cleaning\\n- BERT Sentence Encoding\\n- BERT PCA 100\\n- BERT PCA 200\\n- BERT PCA 300\\n- Calculate distance between consecutive sentences in the paragraph\\n- Distances: L1, L2 and Cosine and Cosine similarity\\n- Statistics: Mean, SD\\n\\n# Experimental Setup\\n\\n1. #IncrementalPCA\\n2. GPU to speed up\\n3. Data chunking\\n4. Calculate BERT for a chunk and store in disk"},{"id":"/2021/10/01/short-video-background-music-recommender","metadata":{"permalink":"/ai-kb/blog/2021/10/01/short-video-background-music-recommender","source":"@site/blog/2021-10-01-short-video-background-music-recommender.mdx","title":"Short-video Background Music Recommender","description":"Matching micro-videos with suitable background music can help uploaders better convey their contents and emotions, and increase the click-through rate of their uploaded videos. However, manually selecting the background music becomes a painstaking task due to the voluminous and ever-growing pool of candidate music. Therefore, automatically recommending background music to videos becomes an important task.","date":"2021-10-01T00:00:00.000Z","formattedDate":"October 1, 2021","tags":[{"label":"recsys","permalink":"/ai-kb/blog/tags/recsys"}],"readingTime":2.17,"truncated":false,"authors":[{"name":"Sparsh Agarwal","title":"Principal Developer","url":"https://github.com/sparsh-ai","imageURL":"https://avatars.githubusercontent.com/u/62965911?v=4","key":"sparsh"}],"frontMatter":{"title":"Short-video Background Music Recommender","authors":"sparsh","tags":["recsys"]},"prevItem":{"title":"Semantic Similarity","permalink":"/ai-kb/blog/2021/10/01/semantic-similarity"},"nextItem":{"title":"The progression of analytics in enterprises","permalink":"/ai-kb/blog/2021/10/01/the-progression-of-analytics-in-enterprises"}},"content":"Matching micro-videos with suitable background music can help uploaders better convey their contents and emotions, and increase the click-through rate of their uploaded videos. However, manually selecting the background music becomes a painstaking task due to the voluminous and ever-growing pool of candidate music. Therefore, automatically recommending background music to videos becomes an important task.\\n\\nIn [this](https://arxiv.org/pdf/2107.07268.pdf) paper, Zhu et. al. shared their approach to solve this task. They first collected ~3,000 background music from popular TikTok videos and also ~150,000 video clips that used some kind of background music. They named this dataset `TT-150K`.\\n\\n![An exemplar subset of videos and their matched background music in the established TT-150k dataset](/img/content-blog-raw-blog-short-video-background-music-recommender-untitled.png)\\n\\nAn exemplar subset of videos and their matched background music in the established TT-150k dataset\\n\\nAfter building the dataset, they worked on modeling and proposed the following architecture:\\n\\n![Proposed CMVAE (Cross-modal Variational Auto-encoder) framework](/img/content-blog-raw-blog-short-video-background-music-recommender-untitled-1.png)\\n\\nProposed CMVAE (Cross-modal Variational Auto-encoder) framework\\n\\nThe goal is to represent videos (`users` in recsys terminology) and music (`items`) in a shared latent space. To achieve this, CMVAE use pre-trained models to extract features from unstructured data - `vggish` model for audio2vec, `resnet` for video2vec and `bert-multilingual` for text2vec.  Text and video vectors are then fused using product-of-expert approach. \\n\\nIt uses the reconstruction power of variational autoencoders to 1) reconstruct video from music latent vector and, 2) reconstruct music from video latent vector. In layman terms, we are training a neural network that will try to guess the video activity just by listening background music, and also try to guess the background music just by seeing the video activities. \\n\\nThe joint training objective is $\\\\mathcal{L}_{(z_m,z_v)} = \\\\beta \\\\cdot\\\\mathcal{L}_{cross\\\\_recon} - \\\\mathcal{L}_{KL} + \\\\gamma \\\\cdot \\\\mathcal{L}_{matching}$, where $\\\\beta$ and $\\\\gamma$ control the weight of the cross reconstruction loss and the matching loss, respectively.\\n\\nAfter training the model, they compared the model\'s performance with existing baselines and the results are as follows:\\n\\n![/img/content-blog-raw-blog-short-video-background-music-recommender-untitled-2.png](/img/content-blog-raw-blog-short-video-background-music-recommender-untitled-2.png)\\n\\n**Conclusion**: I don\'t make short videos myself but can easily imagine the difficulty in finding the right background music. If I have to do this task manually, I will try out 5-6 videos and select one that I like. But here, I will be assuming that my audience would also like this music. Moreover, feedback is not actionable because it will create kind of an implicit sub-conscious effect (because when I see a video, I mostly judge it at overall level and rarely notice that background music is the problem). So, this kind of recommender system will definitely help me in selecting a better background music. Excited to see this feature soon in TikTok, Youtube Shorts and other similar services."},{"id":"/2021/10/01/the-progression-of-analytics-in-enterprises","metadata":{"permalink":"/ai-kb/blog/2021/10/01/the-progression-of-analytics-in-enterprises","source":"@site/blog/2021-10-01-the-progression-of-analytics-in-enterprises.mdx","title":"The progression of analytics in enterprises","description":"An organization\u2019s analytics strategy is how its\xa0people, processes, tools, and data\xa0work together to collect, store, and analyze data. Processes\xa0refers to\xa0how\xa0analytics are produced, consumed, and maintained. A more modern approach to analytics is intended to support greater business agility at scale. This requires faster data preparation from a wider variety of sources, rapid prototyping and analytics model building, and cross-team collaboration processes. Tools, or technologies, are the raw programs and applications used to prepare for and perform analyses, such as the provisioning, flow, and automation of tasks and resources. As an analytics strategy matures, the technologies used to implement it tend to move from monolithic structures to composable microservices. The last element is\xa0data. A modern analytics architecture supports a growing volume and variety of data sources, which may include data from data warehouses and data lakes\u2014streaming data, relational databases, graph databases, unstructured or semi-structured data, text data, and images.","date":"2021-10-01T00:00:00.000Z","formattedDate":"October 1, 2021","tags":[{"label":"insight","permalink":"/ai-kb/blog/tags/insight"}],"readingTime":12.09,"truncated":false,"authors":[{"name":"Sparsh Agarwal","title":"Principal Developer","url":"https://github.com/sparsh-ai","imageURL":"https://avatars.githubusercontent.com/u/62965911?v=4","key":"sparsh"}],"frontMatter":{"title":"The progression of analytics in enterprises","authors":"sparsh","tags":["insight"]},"prevItem":{"title":"Short-video Background Music Recommender","permalink":"/ai-kb/blog/2021/10/01/short-video-background-music-recommender"},"nextItem":{"title":"Tools for building recommender systems","permalink":"/ai-kb/blog/2021/10/01/tools-for-building-recommender-systems"}},"content":"An organization\u2019s analytics strategy is how its\xa0*people, processes, tools, and data*\xa0work together to collect, store, and analyze data. *Processes*\xa0refers to\xa0*how*\xa0analytics are produced, consumed, and maintained. A more modern approach to analytics is intended to support greater business agility at scale. This requires faster data preparation from a wider variety of sources, rapid prototyping and analytics model building, and cross-team collaboration processes. *Tools*, or technologies, are the raw programs and applications used to prepare for and perform analyses, such as the provisioning, flow, and automation of tasks and resources. As an analytics strategy matures, the technologies used to implement it tend to move from monolithic structures to composable microservices. The last element is\xa0*data.* A modern analytics architecture supports a growing volume and variety of data sources, which may include data from data warehouses and data lakes\u2014streaming data, relational databases, graph databases, unstructured or semi-structured data, text data, and images.\\n\\n### Analytics Past, Present, and Future\\n\\n|  | Past | Present | Future |\\n| --- | --- | --- | --- |\\n|  | This refers to an era of analytics starting in the 1990s and running through the mid-2000s. During this phase, organizations were able to consolidate mostly transactional data into a unified system, often a\xa0data warehouse, which limited end users\u2019 ability to interact directly with the data due to technical and governance requirements. | Starting in the late 2000s, organizations were forced to rethink how they used analytics, in no small part due to the explosion of data during this time. This was the era of \u201cBig Data\u201d and its infamous\xa0\u201c V\u2019s\u201d:\xa0volume, velocity, and variety.4\xa0As organizations shifted their approach during this period, they unlocked\xa0diagnostic analytics, or the capability to answer \u201cWhy did it happen?\u201d | The Future of Analytics Is Converged. Converged analytics unifies advances in AI, streaming data, and related technologies into a seamless analytics experience for all users. This arrangement unlocks\xa0prescriptive\xa0analytics across an organization, allowing anyone to make data-driven decisions that answer important questions. |\\n| People | IT professionals were needed to kick off any data-based work by extracting data from a centralized, difficult-to-use source. This process could take multiple days, and the number of query requests could easy exceed the IT team\u2019s ability to fulfill those requests\u2014and the opportune time for new insights.If some change was needed to data collection or storage methods, it could easily take IT months to perform. The data analysis and modeling work could take nearly as long. Rank-and-file domain experts did have\xa0some\xa0access to data, through so-called\xa0self-service business intelligence (BI)\xa0features. However, due to the same speed and accessibility issues that technical professionals faced, it was often difficult for domain experts like line of business leaders to truly lead with data for decision making. | It\u2019s no coincidence that around the same time as Big Data emerged, so did the role of the\xa0data scientist. Compared with earlier roles like researcher or statistician, the data scientist blends quantitative and domain expertise with a greater degree of computational thinking. These skills became necessary both to handle the greater variety and volume of data sources and to update and deploy data and analytics models without the assistance of IT professionals.Whereas IT in the past sought to meticulously catalog and structure data to enter into a data warehouse, they no longer needed to always clean the data before collecting it; these analytics teams could focus on ease of use and speed to governed access.With these new workflows and organization structures in place, domain leaders are better able to lead with data: both via self-service BI tools and from frequent collaboration with data analysts, data scientists, and other data specialists. | Statisticians and IT served information to business users at the inception of a wider analytics adoption; further into maturity, data analysts and scientists built systems where business users could self-serve insights. In a converged architecture, not only is the business user at the center, but their decision making is augmented by automation. Given this arrangement, there is more collaboration, more automation, and greater scale for data-driven insights as a result of the convergence of teams and workstreams. Teams can work cross-functionally and in parallel across different domains iterating the system to their needs with the raw time and human resources needed to create and maintain analytics products such as dashboards and models. |\\n| Processes | IT professionals spent long periods of time gathering requirements for analytics projects before they could build or deploy solutions. The team meticulously catalogued sources of data used across the organization, from financial or point-of-sale systems to frequently used external datasets. As part of the warehousing process, it was decided\xa0which\xa0of these data sources to store and\xa0how\xa0to store them.Once deployed, data passed into the warehouse through an\xa0extract-transform-load\xa0process (ETL), where the data was copied from these various sources, cleaned and reshaped into the defined structure of the data warehouse, then inserted into production. In other words, data went through rigorous cleaning and preprocessing\xa0before\xa0use.To reach this data, users needed to write time-consuming ad hoc queries. Alternatively, particular data segments or summaries that were frequently requested by business users could be delivered via scheduled automation to reports, dashboards, and scorecards. | As opposed to earlier analytics strategies, IT professionals now seek to collect data as is from any possible source of value. This data can be in a variety of formats, so few predefined rules or relationships are established for ingestion. Depending on the data size, data is processed in batch over discrete time periods, or in streams and events near real time. Because data cleaning is the last step, this process is sometimes referred to as\xa0extract-load-transform\xa0(ELT), as opposed to the ETL of earlier architectures. For data scientists and other technical professionals, faster access to more\xa0and more dynamic\xa0data better enables the rapid development of training sets of data for machine learning models. The ELT process allows for the construction of\xa0machine learning\xa0models, where computers are able to improve performance as more data is passed to them.As more data is collected and put into production, the importance of a\xa0data governance\xa0process typically grows, describing who has authority over data and how that data should be used. Similar approaches are necessary to audit how models are put into production and how they work. | While perhaps using different\xa0means, the\xa0ends\xa0of older analytics approaches were the same: insights, whether historic or in support of future decisions, using governed data and processes. In the methods for doing so, however, infrastructure tended to bloat, either from fragile data storage jobs or increasingly complex data pipelines.Given the volume, velocity, and variety of data needed for prescriptive analytics, such monolithic, centralized approaches are less than optimal. Using the tools discussed in the next section, a converged architecture offers a more nimble approach for providing the right insights at the right time to users of all technical levels.Such democratization relies on quick deployment and adjustment of data products; optimizing production, for example, requires bringing more machine learning models to production faster and at scale. The practice of\xa0ModelOps\xa0is used to institute and govern such rapid production. These processes have become a necessity in rapidly changing business conditions; for example, as the COVID-19 pandemic made structural changes to the economy, many models lost their predictive edge in the face of fundamentally different data. |\\n| Tools | Data warehouses implemented some new technologies relative to the traditional relational database model. Importantly, the data warehouse separated data into\xa0fact\xa0tables, where measurements were stored, and\xa0dimension\xa0tables, which contained descriptive attributes. Business users interacted with the data via reporting software to view static data summaries. These tended to rely on overnight batch jobs to update.In a more sophisticated architecture, analysts could take advantage of\xa0online analytical processing\xa0(OLAP) cubes. Usually relying on a star schema, OLAP let users query the data across dimensions during interactive sessions. For example, they could \u201cslice and dice\u201d or \u201croll up and drill down\u201d on the data.By this point, end users had some autonomy in how they looked at and acted upon the data. Automated processes to inform business activities through data were also put into place, such as alerts when inventory or sales dropped below some threshold. Basic what-if analyses also helped business users evaluate decisions and plan for the future.That said, given the limited sources of data from the data warehouse, there were limited ways to customize and work with the data. While reporting and basic analytics were automated, end users operated largely without the assistance of models developed by\xa0statisticians. Although business intelligence and operations research seek to create value from data, too often these complementary tools were siloed. | In 2011, James Dixon, then chief technology officer of Pentaho, coined the term\xa0data lake\xa0as the architecture needed to support the next level of analytics maturity.\xa0Dixon argued that because of the inherently rigid structures of data warehouses, getting value from the increasing volume and variety of data associated with Big Data was difficult. A data lake, \u201ca repository of data stored in its natural/raw format,\u201d was a better approach. In particular, this arrangement wasn\u2019t suited to operate or capitalize on the expanding volume and variety of Big Data.The data lake is often powered by cloud computing for the benefits of reliability, redundancy, and scalability. Dominant cloud service providers include Amazon Web Services (AWS), Microsoft Azure, and Google Cloud Platform (GCP). Open source technologies like Hadoop and Spark are used to process and store massive datasets using parallel computing and distributed storage. Because this data is often unstructured, it may be stored in graph, document, or other non-relational databases.With the increasing volume and velocity of data, and the use of data lakes along with data warehouses to enable data-driven decisions, businesses needed better ways to scale and share business intelligence. One such path was through interactive, immersive exploration and visualization of the data, as pioneered with Spotfire. Other paths were through visual reports and dashboards, as used by not just Spotfire, but by Jaspersoft, Power BI, WebFOCUS, and many others. As BI tools matured, self-service capabilities and automation for end users also matured. | If maintaining legacy analytics is like raising a thoroughbred, then developing converged analytics is like cultivating a school of goldfish. That is, the backend provisioning is no longer served by monolithic systems but rather by composable groups of\xa0microservices. This arrangement supports elastic and scalable analytics; composability makes it easier to adapt to changes driven in part by a growing volume and variety of data sources. In previous analytics approaches, the distinction between backward-facing BI and prediction-focused data science was clear. Under convergence, analytics at the edge is possible\u2014automating analytic computations so they can be performed on non-centralized data generated by sensors, switches, and similar. With converged analytics, individuals no longer need to wait for data science teams to provide ad hoc deeper insights. They have all the data-driven insights at their fingertips, assisted by AI to quickly explore and make decisions. This isn\u2019t just the case for back-office analysts: frontline workers can, for example, adjust how they interact with a customer given data retrieved about that customer at the time of that interaction. |\\n| Data | During this period, data tended to be\xa0transactional, or related to sales and purchases. Take a point-of-sales (POS) system, for example. Each time a sale is made, information about what was sold,\xa0possibly\xa0to whom, is recorded in the POS system. Those records can be compiled into tables and ultimately processed into a data\xa0warehouse.Under this process, data is gathered from prespecified sources at prespecified times, such as a nightly POS extract. Not all data made its way to the data warehouse, especially in the earlier days of analytics\u2014either because it was judged unimportant, or because it was not prioritized. | Contemporary analytics expands the variety of data available and used: both\xa0structured\xa0tables and\xa0unstructured\xa0sources like natural language and images are available. On account of stream processing, refreshes of this data are available in minutes or even less. In particular, the data lake can accommodate real-time events such as IoT sensor readings, GPS signals, and online transactions as they\xa0happen. | A primary feature of converged analytics is the blending of historical and real-time data. According to a study by Seagate and International Data Corporation (IDC), 30% of all data will be real time by 2025. In particular, IoT sensor readings, GPS signals, and online transactions as they happen are available for immediate analysis and modeling. |\\n| Agility | The relatively rigid nature of the data warehouse made changes to the collection and dissemination of data difficult. Subsequently, business agility was limited. Business users could get historic data about the business through static reports (descriptive analytics). Through OLAP cubes, they could possibly even dig into the data to parse out cause and effect (diagnostic analytics). But without more immediate access to broader data, it was difficult to advance to\xa0predictive analytics, or the ability to ask: \u201cWhat is\xa0going\xa0to happen?\u201d | This next phase in the evolution of analytics gets data-driven insights into the hands of end users quickly, with technology allowing them to interact with it on a deeper level. Data scientists are able to build machine learning systems that improve with more data. Using drag-and-drop tools, business users can process and analyze data without technical assistance. With cloud, automation, and streaming technologies, organizations have been better able to adapt to and plan for changing circumstances. That said, machine learning works only so long in production before the algorithm struggles to account for changes to the business and needs intervention. While data scientists undertake these predictive challenges, BI professionals and domain experts tend to operate solely in analyzing current or past data. The next generation of analytics architecture will further reflect organizational needs for greater collaboration among data scientists, BI and analytics teams, and business users and consumers of analytics insights. | Earlier analytics tended to isolate skills and processes: technical versus highly technical roles, data collection versus deployment versus modeling, and so forth. Converged analytics promotes close collaboration between teams to rapidly model, deploy, and act on data. As data operations become decentralized, teams and individuals can rapidly mine and act on the analytics.In particular, the marriage of real-time data with machine learning and AI-infused BI allows any user to magnify their own domain knowledge with data-driven insights. These features square precisely with the definition of business agility as \u201cinnovation via collaboration to be able to anticipate challenges and opportunities before they occur.\u201d With the support of converged analytics, any professional can detect and act on both challenges and opportunities at the moment of impact, rather than months later. |\\n\\n---\\n\\n\xa9\ufe0f2021, RecoHut."},{"id":"/2021/10/01/tools-for-building-recommender-systems","metadata":{"permalink":"/ai-kb/blog/2021/10/01/tools-for-building-recommender-systems","source":"@site/blog/2021-10-01-tools-for-building-recommender-systems.mdx","title":"Tools for building recommender systems","description":"/img/content-blog-raw-blog-tools-for-building-recommender-systems-untitled.png","date":"2021-10-01T00:00:00.000Z","formattedDate":"October 1, 2021","tags":[{"label":"recsys","permalink":"/ai-kb/blog/tags/recsys"},{"label":"tool","permalink":"/ai-kb/blog/tags/tool"}],"readingTime":11.025,"truncated":false,"authors":[{"name":"Sparsh Agarwal","title":"Principal Developer","url":"https://github.com/sparsh-ai","imageURL":"https://avatars.githubusercontent.com/u/62965911?v=4","key":"sparsh"}],"frontMatter":{"title":"Tools for building recommender systems","authors":"sparsh","tags":["recsys","tool"]},"prevItem":{"title":"The progression of analytics in enterprises","permalink":"/ai-kb/blog/2021/10/01/the-progression-of-analytics-in-enterprises"},"nextItem":{"title":"Vehicle Suggestions","permalink":"/ai-kb/blog/2021/10/01/vehicle-suggestions"}},"content":"![/img/content-blog-raw-blog-tools-for-building-recommender-systems-untitled.png](/img/content-blog-raw-blog-tools-for-building-recommender-systems-untitled.png)\\n\\n## Recombee - Recommendation as a service API\\n\\nRecombee is a Recommender as a Service with easy integration and Admin UI. It can be used in many domains, for example in media (VoD, news \u2026), e-commerce, job boards, aggregators or classifieds. Basically, it can be used in any domain with a catalog of\xa0**items**\xa0that can be interacted by\xa0**users**. The users can interact with the items in many ways: for example\xa0view them,\xa0rate them,\xa0bookmark them,\xa0purchase them, etc. Both items and users can have\xa0various properties\xa0(metadata) that are also used by the recommendation models.\\n\\n![/img/content-blog-raw-blog-tools-for-building-recommender-systems-untitled-1.png](/img/content-blog-raw-blog-tools-for-building-recommender-systems-untitled-1.png)\\n\\n[Here](https://docs.recombee.com/tutorial.html) is the official tutorial series to get started. \\n\\n## Amazon Personalize - Self-service Platform to build and serve recommenders\\n\\nAmazon Personalize is a fully managed machine learning service that goes beyond rigid static rule based recommendation systems and trains, tunes, and deploys custom ML models to deliver highly customized recommendations to customers across industries such as retail and media and entertainment.\\n\\n![/img/content-blog-raw-blog-tools-for-building-recommender-systems-untitled-2.png](/img/content-blog-raw-blog-tools-for-building-recommender-systems-untitled-2.png)\\n\\nIt covers 6 use-cases:\\n\\n![Popular Use-cases](/img/content-blog-raw-blog-tools-for-building-recommender-systems-untitled-3.png)\\n\\nPopular Use-cases\\n\\nFollowing are the hands-on tutorials:\\n\\n1. [Data Science on AWS Workshop - Personalize Recommendations**p**](https://github.com/data-science-on-aws/workshop/tree/937f6e4fed53fcc6c22bfac42c2c18a687317995/oreilly_book/02_usecases/personalize_recommendations)\\n2. [https://aws.amazon.com/blogs/machine-learning/creating-a-recommendation-engine-using-amazon-personalize/](https://aws.amazon.com/blogs/machine-learning/creating-a-recommendation-engine-using-amazon-personalize/)\\n3. [https://aws.amazon.com/blogs/machine-learning/omnichannel-personalization-with-amazon-personalize/](https://aws.amazon.com/blogs/machine-learning/omnichannel-personalization-with-amazon-personalize/)\\n4. [https://aws.amazon.com/blogs/machine-learning/using-a-b-testing-to-measure-the-efficacy-of-recommendations-generated-by-amazon-personalize/](https://aws.amazon.com/blogs/machine-learning/using-a-b-testing-to-measure-the-efficacy-of-recommendations-generated-by-amazon-personalize/)\\n\\nAlso checkout these resources:\\n\\n1. [https://www.youtube.com/playlist?list=PLN7ADELDRRhiQB9QkFiZolioeJZb3wqPE](https://www.youtube.com/playlist?list=PLN7ADELDRRhiQB9QkFiZolioeJZb3wqPE)\\n\\n## Azure Personalizer - An API based service with Reinforcement learning capability\\n\\nAzure Personalizer is a cloud-based API service that helps developers create rich, personalized experiences for each user of your app. It learns from customer\'s real-time behavior, and uses reinforcement learning to select the best item (action) based on collective behavior and reward scores across all users. Actions are the content items, such as news articles, specific movies, or products. It takes a list of items (e.g. list of drop-down choices) and their context (e.g. Report Name, User Name, Time Zone) as input and returns the ranked list of items for the given context. While doing that, it also allows feedback submission regarding the relevance and efficiency of the ranking results returned by the service. The feedback (reward score) can be automatically calculated and submitted to the service based on the given personalization use case.\\n\\n![/img/content-blog-raw-blog-tools-for-building-recommender-systems-untitled-4.png](/img/content-blog-raw-blog-tools-for-building-recommender-systems-untitled-4.png)\\n\\nYou can use the Personalizer service to determine what product to suggest to shoppers or to figure out the optimal position for an advertisement. After the content is shown to the user, your application monitors the user\'s reaction and reports a reward score back to the Personalizer service. This ensures continuous improvement of the machine learning model, and Personalizer\'s ability to select the best content item based on the contextual information it receives. \\n\\nFollowing are some of the interesting use cases of Azure Personalizer:\\n\\n1. Blog Recommender [[Video tutorial](https://youtu.be/fsn7hTOKXsY?list=PLN7ADELDRRhhHRu1tS3gmdeUfeQkG82k_&t=1145), [GitHub](https://github.com/georgiakalyva/azure-personalizer-service)]\\n2. Food Personalizer [[Video tutorial](https://youtu.be/A-8OfoWySHQ?list=PLN7ADELDRRhhHRu1tS3gmdeUfeQkG82k_&t=1758), [Slideshare](https://www.slideshare.net/SetuChokshi/introduction-to-reinforcement-learning-with-azure-personalizer-233272693), [Code Blog](https://pipinstall.me/introduction_to_azure_personalizer/)]\\n3. Coffee Personalizer [[GitHub](https://github.com/Azure-Samples/cognitive-services-personalizer-samples/tree/master/samples/azurenotebook), [Video tutorial](https://youtu.be/vkbIhX7xhcE?list=PLN7ADELDRRhhHRu1tS3gmdeUfeQkG82k_)]\\n4. News Recommendation\\n5. Movie Recommendation\\n6. Product Recommendation\\n7. **Intent clarification & disambiguation**: help your users have a better experience when their intent is not clear by providing an option that is personalized.\\n8. **Default suggestions**\xa0for menus & options: have the bot suggest the most likely item in a personalized way as a first step, instead of presenting an impersonal menu or list of alternatives.\\n9. **Bot traits & tone**: for bots that can vary tone, verbosity, and writing style, consider varying these traits.\\n10. **Notification & alert content**: decide what text to use for alerts in order to engage users more.\\n11. **Notification & alert timing**: have personalized learning of when to send notifications to users to engage them more.\\n12. Dropdown Options - Different users of an application with manager privileges would see a list of reports that they can run. Before Personalizer was implemented, the list of dozens of reports was displayed in alphabetical order, requiring most of the managers to scroll through the lengthy list to find the report they needed. This created a poor user experience for daily users of the reporting system, making for a good use case for Personalizer. The tooling learned from the user behavior and began to rank frequently run reports on the top of the dropdown list. Frequently run reports would be different for different users, and would change over time for each manager as they get assigned to different projects. This is exactly the situation where Personalizer\u2019s reward score-based learning models come into play.\\n13. Projects in Timesheet - Every employee in the company logs a daily timesheet listing all of the projects the user is assigned to. It also lists other projects, such as overhead. Depending upon the employee project allocations, his or her timesheet table could have few to a couple of dozen active projects listed. Even though the employee is assigned to several projects, particularly at lead and manager levels, they don\u2019t log time in more than 2 to 3 projects for a few weeks to months.\\n    1. Reward Score Calculation\\n\\n## Google Recommendation - Recommender Service from Google\\n\\n![https://cloudx-bricks-prod-bucket.storage.googleapis.com/6a0d4afb1778e55d54cb7d66382a4b25f8748a50a93f3c3403d2a835aa166f3d.svg](https://cloudx-bricks-prod-bucket.storage.googleapis.com/6a0d4afb1778e55d54cb7d66382a4b25f8748a50a93f3c3403d2a835aa166f3d.svg)\\n\\n## [Abacus.ai](http://abacus.ai) - Self-service Platform at cheaper price\\n\\nIt uses multi-objective, real-time recommendations models and provides 4 use-cases for fasttrack train-&-deploy process - Personalized recommendations, personalized search, related items and real-time feed recommendations.\\n\\n![/img/content-blog-raw-blog-tools-for-building-recommender-systems-untitled-5.png](/img/content-blog-raw-blog-tools-for-building-recommender-systems-untitled-5.png)\\n\\nHere is the hands-on video tutorial:\\n\\n[https://youtu.be/7hTKL73f2yA](https://youtu.be/7hTKL73f2yA)\\n\\n## Nvidia Merlin - Toolkit with GPU capabilities\\n\\nMerlin empowers data scientists, machine learning engineers, and researchers to build high-performing recommenders at scale. Merlin includes tools that democratize building deep learning recommenders by addressing common ETL, training, and inference challenges. Each stage of the Merlin pipeline is optimized to support hundreds of terabytes of data, all accessible through easy-to-use APIs. With Merlin, better predictions than traditional methods and increased click-through rates are within reach.\\n\\n![End-to-end recommender system architecture. FE: feature engineering; PP: preprocessing; ETL: extract-transform-load.](/img/content-blog-raw-blog-tools-for-building-recommender-systems-untitled-6.png)\\n\\nEnd-to-end recommender system architecture. FE: feature engineering; PP: preprocessing; ETL: extract-transform-load.\\n\\n![/img/content-blog-raw-blog-tools-for-building-recommender-systems-untitled-7.png](/img/content-blog-raw-blog-tools-for-building-recommender-systems-untitled-7.png)\\n\\n## TFRS - Open-source Recommender library built on top of Tensorflow\\n\\nBuilt with TensorFlow 2.x, TFRS makes it possible to:\\n\\n- Build and evaluate flexible\xa0**[candidate nomination models](https://research.google/pubs/pub48840/)**;\\n- Freely incorporate item, user, and context\xa0**[information](https://tensorflow.org/recommenders/examples/featurization)**\xa0into recommendation models;\\n- Train\xa0**[multi-task models](https://tensorflow.org/recommenders/examples/multitask)**\xa0that jointly optimize multiple recommendation objectives;\\n- Efficiently serve the resulting models using\xa0**[TensorFlow Serving](https://www.tensorflow.org/tfx/guide/serving)**.\\n\\n![/img/content-blog-raw-blog-tools-for-building-recommender-systems-untitled-8.png](/img/content-blog-raw-blog-tools-for-building-recommender-systems-untitled-8.png)\\n\\nFollowing is a series of official tutorial notebooks:-\\n\\n[TensorFlow Recommenders: Quickstart](https://www.tensorflow.org/recommenders/examples/quickstart)\\n\\n## Elliot - An end-to-end framework good for recommender system experiments\\n\\n[Elliot](https://elliot.readthedocs.io/en/latest/) is a comprehensive recommendation framework that aims to run and reproduce an entire experimental pipeline by processing a simple configuration file. The framework loads, filters, and splits the data considering a vast set of strategies (13 splitting methods and 8 filtering approaches, from temporal training-test splitting to nested K-folds Cross-Validation). Elliot optimizes hyperparameters (51 strategies) for several recommendation algorithms (50), selects the best models, compares them with the baselines providing intra-model statistics, computes metrics (36) spanning from accuracy to beyond-accuracy, bias, and fairness, and conducts statistical analysis (Wilcoxon and Paired t-test). The aim is to provide the researchers with a tool to ease (and make them reproducible) all the experimental evaluation phases, from data reading to results collection.\\n\\n![/img/content-blog-raw-blog-tools-for-building-recommender-systems-untitled-9.png](/img/content-blog-raw-blog-tools-for-building-recommender-systems-untitled-9.png)\\n\\n## RecBole - Another framework good for recommender system model experiments\\n\\nRecBole is developed based on Python and PyTorch for reproducing and developing recommendation algorithms in a unified, comprehensive and efficient framework for research purpose. It can be installed from pip, Conda and source, and easy to use. It includes 65 recommendation algorithms, covering four major categories: General Recommendation, Sequential Recommendation, Context-aware Recommendation, and Knowledge-based Recommendation, which can support the basic research in recommender systems.\\n\\n![/img/content-blog-raw-blog-tools-for-building-recommender-systems-untitled-10.png](/img/content-blog-raw-blog-tools-for-building-recommender-systems-untitled-10.png)\\n\\nFeatures:\\n\\n- **General and extensible data structure**We deign general and extensible data structures to unify the formatting and usage of various recommendation datasets.\\n- **Comprehensive benchmark models and datasets**We implement 65 commonly used recommendation algorithms, and provide the formatted copies of 28 recommendation datasets.\\n- **Efficient GPU-accelerated execution**We design many tailored strategies in the GPU environment to enhance the efficiency of our library.\\n- **Extensive and standard evaluation protocols**We support a series of commonly used evaluation protocols or settings for testing and comparing recommendation algorithms.\\n\\n## Microsoft Recommenders - A powerful set of tools for building high-quality recommender system at low-cost *(highly recommended)*\\n\\nThe Microsoft Recommenders repository is an open source collection of python utilities and Jupyter notebooks to help accelerate the process of designing, evaluating, and deploying recommender systems. The repository was initially formed by data scientists at Microsoft to consolidate common tools and best practices developed from working on recommender systems in various industry settings. The goal of the tools and notebooks is to show examples of how to effectively build, compare, and then deploy the best recommender solution for a given scenario. Contributions from the community have brought in new algorithm implementations and code examples covering multiple aspects of working with recommendation algorithms.\\n\\n![/img/content-blog-raw-blog-tools-for-building-recommender-systems-untitled-11.png](/img/content-blog-raw-blog-tools-for-building-recommender-systems-untitled-11.png)\\n\\n## Surprise - An open-source library with easy api and powerful models\\n\\n[Surprise](http://surpriselib.com/)\xa0is a Python\xa0[scikit](https://www.scipy.org/scikits.html)\xa0for building and analyzing recommender systems that deal with explicit rating data.\\n\\n[Surprise](http://surpriselib.com/)\xa0**was designed with the following purposes in mind**:\\n\\n- Give users perfect control over their experiments. To this end, a strong emphasis is laid on\xa0[documentation](http://surprise.readthedocs.io/en/stable/index.html), which we have tried to make as clear and precise as possible by pointing out every detail of the algorithms.\\n- Alleviate the pain of\xa0[Dataset handling](http://surprise.readthedocs.io/en/stable/getting_started.html#load-a-custom-dataset). Users can use both\xa0*built-in*\xa0datasets ([Movielens](http://grouplens.org/datasets/movielens/),\xa0[Jester](http://eigentaste.berkeley.edu/dataset/)), and their own\xa0*custom*\xa0datasets.\\n- Provide various ready-to-use\xa0[prediction algorithms](http://surprise.readthedocs.io/en/stable/prediction_algorithms_package.html)\xa0such as\xa0[baseline algorithms](http://surprise.readthedocs.io/en/stable/basic_algorithms.html),\xa0[neighborhood methods](http://surprise.readthedocs.io/en/stable/knn_inspired.html), matrix factorization-based (\xa0[SVD](http://surprise.readthedocs.io/en/stable/matrix_factorization.html#surprise.prediction_algorithms.matrix_factorization.SVD),\xa0[PMF](http://surprise.readthedocs.io/en/stable/matrix_factorization.html#unbiased-note),\xa0[SVD++](http://surprise.readthedocs.io/en/stable/matrix_factorization.html#surprise.prediction_algorithms.matrix_factorization.SVDpp),\xa0[NMF](http://surprise.readthedocs.io/en/stable/matrix_factorization.html#surprise.prediction_algorithms.matrix_factorization.NMF)), and\xa0[many others](http://surprise.readthedocs.io/en/stable/prediction_algorithms_package.html). Also, various\xa0[similarity measures](http://surprise.readthedocs.io/en/stable/similarities.html)\xa0(cosine, MSD, pearson\u2026) are built-in.\\n- Make it easy to implement\xa0[new algorithm ideas](http://surprise.readthedocs.io/en/stable/building_custom_algo.html).\\n- Provide tools to\xa0[evaluate](http://surprise.readthedocs.io/en/stable/model_selection.html),\xa0[analyse](http://nbviewer.jupyter.org/github/NicolasHug/Surprise/tree/master/examples/notebooks/KNNBasic_analysis.ipynb/)\xa0and\xa0[compare](http://nbviewer.jupyter.org/github/NicolasHug/Surprise/blob/master/examples/notebooks/Compare.ipynb)\xa0the algorithms\u2019 performance. Cross-validation procedures can be run very easily using powerful CV iterators (inspired by\xa0[scikit-learn](http://scikit-learn.org/)\xa0excellent tools), as well as\xa0[exhaustive search over a set of parameters](http://surprise.readthedocs.io/en/stable/getting_started.html#tune-algorithm-parameters-with-gridsearchcv).\\n\\n## Spotlight - Another open-source library\\n\\nSpotlight uses PyTorch to build both deep and shallow recommender models. By providing both a slew of building blocks for loss functions (various pointwise and pairwise ranking losses), representations (shallow factorization representations, deep sequence models), and utilities for fetching (or generating) recommendation datasets, it aims to be a tool for rapid exploration and prototyping of new recommender models.\\n\\n![/img/content-blog-raw-blog-tools-for-building-recommender-systems-untitled-12.png](/img/content-blog-raw-blog-tools-for-building-recommender-systems-untitled-12.png)\\n\\n[Here](https://github.com/maciejkula/spotlight/tree/master/examples) is a series of hands-on tutorials to get started.\\n\\n## Vowpal Wabbit - library with reinforcement learning features\\n\\nVowpal Wabbit is an open source machine learning library, extensively used by industry, and is the first public terascale learning system. It provides fast, scalable machine learning and has unique capabilities such as learning to search, active learning, contextual memory, and extreme multiclass learning. It has a focus on reinforcement learning and provides production ready implementations of Contextual Bandit algorithms. It was developed originally at Yahoo! Research, and currently at Microsoft Research. Vowpal Wabbit sees significant innovation as a research to production vehicle for Microsoft Research.\\n\\n![/img/content-blog-raw-blog-tools-for-building-recommender-systems-untitled-13.png](/img/content-blog-raw-blog-tools-for-building-recommender-systems-untitled-13.png)\\n\\nFor most applications, collaborative filtering yields satisfactory results for item recommendations; there are however several issues that arise that might make it difficult to scale up a recommender system.\\n\\n- The number of features can grow quite large, and given the usual sparsity of consumption datasets, collaborative filtering needs every single feature and datapoint available.\\n- For new data points, the whole model has to be re-trained\\n\\nVowpal Wabbit\u2019s matrix factorization capabilities can be used to build a recommender that is similar in spirit to collaborative filtering but that avoids the pitfalls that we mentioned before.\\n\\nFollowing are the three introductory hands-on tutorials on building recommender systems with vowpal wabbit:\\n\\n1. [Vowpal Wabbit Deep Dive - A Content-based Recommender System using Microsoft Recommender Library](https://github.com/microsoft/recommenders/blob/main/examples/02_model_content_based_filtering/vowpal_wabbit_deep_dive.ipynb)\\n2. [Simulating Content Personalization with Contextual Bandits](https://vowpalwabbit.org/tutorials/cb_simulation.html)\\n3. [Vowpal Wabbit, The Magic Recommender System!](https://samuel-guedj.medium.com/vowpal-wabbit-the-magic-58b7f1d8e39c)\\n\\n## DLRM - An open-source scalable model from Facebook\'s AI team, build on top of PyTorch\\n\\nDLRM advances on other models by combining principles from both collaborative filtering and predictive analytics-based approaches, which enables it to work efficiently with production-scale data and provide state-of-art results.\\n\\nIn the DLRM model, categorical features are processed using embeddings, while continuous features are processed with a bottom multilayer perceptron (MLP). Then, second-order interactions of different features are computed explicitly. Finally, the results are processed with a top MLP and fed into a sigmoid function in order to give a probability of a click.\\n\\n![/img/content-blog-raw-blog-tools-for-building-recommender-systems-untitled-14.png](/img/content-blog-raw-blog-tools-for-building-recommender-systems-untitled-14.png)\\n\\nFollowing are the hands-on tutorials:\\n\\n1. [https://nbviewer.jupyter.org/github/gotorehanahmad/Recommendation-Systems/blob/master/dlrm/dlrm_main.ipynb](https://nbviewer.jupyter.org/github/gotorehanahmad/Recommendation-Systems/blob/master/dlrm/dlrm_main.ipynb)\\n2. [Training Facebook\'s DLRM on the digix dataset](https://nbviewer.jupyter.org/github/mabeckers/dlrm/blob/new_dataset/Train_DLRM_Digix.ipynb)\\n\\n## References\\n\\n1. [https://elliot.readthedocs.io/en/latest/](https://elliot.readthedocs.io/en/latest/)\\n2. [https://vowpalwabbit.org/index.html](https://vowpalwabbit.org/index.html)\\n3. [https://abacus.ai/user_eng](https://abacus.ai/user_eng)\\n4. [https://azure.microsoft.com/en-in/services/cognitive-services/personalizer/](https://azure.microsoft.com/en-in/services/cognitive-services/personalizer/)\\n5. [https://aws.amazon.com/personalize/](https://aws.amazon.com/personalize/)\\n6. [https://github.com/facebookresearch/dlrm](https://github.com/facebookresearch/dlrm)\\n7. [https://www.tensorflow.org/recommenders](https://www.tensorflow.org/recommenders)\\n8. [https://magento.com/products/product-recommendations](https://magento.com/products/product-recommendations)\\n9. [https://cloud.google.com/recommendations](https://cloud.google.com/recommendations)\\n10. [https://www.recombee.com/](https://www.recombee.com/)\\n11. [https://recbole.io/](https://recbole.io/)\\n12. [https://github.com/microsoft/recommenders](https://github.com/microsoft/recommenders)\\n13. [http://surpriselib.com/](http://surpriselib.com/)\\n14. [https://github.com/maciejkula/spotlight](https://github.com/maciejkula/spotlight)\\n15. https://vowpalwabbit.org/tutorials/contextual_bandits.html\\n16. https://github.com/VowpalWabbit/vowpal_wabbit/wiki\\n17. https://vowpalwabbit.org/tutorials/cb_simulation.html\\n18. https://vowpalwabbit.org/rlos/2021/projects.html\\n19. https://vowpalwabbit.org/rlos/2020/projects.html\\n20. https://getstream.io/blog/recommendations-activity-streams-vowpal-wabbit/\\n21. https://samuel-guedj.medium.com/vowpal-wabbit-the-magic-58b7f1d8e39c\\n22. https://vowpalwabbit.org/neurips2019/\\n23. https://github.com/VowpalWabbit/neurips2019\\n24. https://getstream.io/blog/introduction-contextual-bandits/\\n25. https://www.youtube.com/watch?v=CeOcNK1xSSA&t=72s\\n26. https://vowpalwabbit.org/blog/rlos-fest-2021.html\\n27. https://github.com/VowpalWabbit/workshop\\n28. https://github.com/VowpalWabbit/workshop/tree/master/aiNextCon2019\\n29. [Blog post by Nasir Mirza. Azure Cognitive Services Personalizer: Part One. Oct, 2019.](https://www.ais.com/azure-cognitive-services-personalizer-part-one/)\\n30. [Blog post by Nasir Mirza. Azure Cognitive Services Personalizer: Part Two. Oct, 2019.](https://www.ais.com/azure-cognitive-services-personalizer-part-two/)\\n31. [Blog post by Nasir Mirza. Azure Cognitive Services Personalizer: Part Three. Dec, 2019.](https://www.ais.com/azure-cognitive-services-personalizer-part-three/)\\n32. [Microsoft Azure Personalizer Official Documentation. Oct, 2020.](https://docs.microsoft.com/en-us/azure/cognitive-services/personalizer/what-is-personalizer)\\n33. [Personalizer demo.](https://personalizationdemo.azurewebsites.net/)\\n34. [Official Page.](https://azure.microsoft.com/en-in/services/cognitive-services/personalizer/#faqs)\\n35. [Blog Post by Jake Wong. Get hands on with the Azure Personalizer API. Aug, 2019.](https://www.linkedin.com/pulse/get-hands-azure-personalizer-api-jake-wang/)\\n36. [Medium Post.](https://enefitit.medium.com/we-tested-azure-personalizer-heres-what-you-can-expect-8c5ec074a28e)\\n37. [Blog Post.](https://www.valoremreply.com/post/azure-personalizer/)\\n38. [Git Repo.](https://github.com/Azure-Samples/cognitive-services-personalizer-samples)\\n39. [https://youtu.be/7hTKL73f2yA](https://youtu.be/7hTKL73f2yA)\\n40. [Deep-Learning Based Recommendation Systems\u200a\u2014\u200aLearning AI](https://abacus.ai/blog/2020/03/31/deep-learning-based-recommendation-systems/#:~:text=Deep%2DLearning%20Based%20Recommendation%20Systems%20%E2%80%94%20Learning%20AI,-By%20Abacus.AI&text=Deep%20Learning%20(DL)%20has%20had,of%20Recommender%20Systems%20(RS).)\\n41. [Evaluating Deep Learning Models with Abacus.AI \u2013 Recommendation Systems](https://abacus.ai/blog/2020/12/11/evaluating-deep-learning-models-recommender-systems/)\\n42. https://aws.amazon.com/blogs/machine-learning/pioneering-personalized-user-experiences-at-stockx-with-amazon-personalize/\\n43. https://aws.amazon.com/blogs/machine-learning/category/artificial-intelligence/amazon-personalize/\\n44. https://d1.awsstatic.com/events/reinvent/2019/REPEAT_1_Build_a_content-recommendation_engine_with_Amazon_Personalize_AIM304-R1.pdf\\n45. https://aws.amazon.com/blogs/aws/amazon-personalize-real-time-personalization-and-recommendation-for-everyone/\\n46. https://d1.awsstatic.com/events/reinvent/2019/REPEAT_1_Accelerate_experimentation_with_personalization_models_AIM424-R1.pdf\\n47. https://d1.awsstatic.com/events/reinvent/2019/REPEAT_1_Personalized_user_engagement_with_machine_learning_AIM346-R1.pdf\\n48. https://github.com/aws-samples/amazon-personalize-samples\\n49. https://github.com/aws-samples/amazon-personalize-automated-retraining\\n50. https://github.com/aws-samples/amazon-personalize-ingestion-pipeline\\n51. https://github.com/aws-samples/amazon-personalize-monitor\\n52. https://github.com/aws-samples/amazon-personalize-data-conversion-pipeline\\n53. https://github.com/james-jory/segment-personalize-workshop\\n54. https://github.com/aws-samples/amazon-personalize-samples/tree/master/next_steps/workshops/POC_in_a_box\\n55. https://github.com/Imagination-Media/aws-personalize-magento2\\n56. https://github.com/awslabs/amazon-personalize-optimizer-using-amazon-pinpoint-events\\n57. https://github.com/aws-samples/amazon-personalize-with-aws-glue-sample-dataset\\n58. https://github.com/awsdocs/amazon-personalize-developer-guide\\n59. https://github.com/chrisking/NetflixPersonalize\\n60. https://github.com/aws-samples/retail-demo-store\\n61. https://github.com/aws-samples/personalize-data-science-sdk-workflow\\n62. https://github.com/apac-ml-tfc/personalize-poc\\n63. https://github.com/dalacan/personalize-batch-recommendations\\n64. https://github.com/harunobukameda/Amazon-Personalize-Handson\\n65. https://www.sagemakerworkshop.com/personalize/\\n66. https://github.com/lmorri/vodpocinabox\\n67. https://github.com/awslabs/unicornflix\\n68. https://www.youtube.com/watch?v=r9J3UZmddC4&t=966s\\n69. https://www.youtube.com/watch?v=kTufCK76Yus&t=1436s\\n70. https://www.youtube.com/watch?v=hY_XzglTkak&t=66s\\n71. [https://business.adobe.com/lv/summit/2020/adobe-sensei-powers-magento-product-recommendations.html](https://business.adobe.com/lv/summit/2020/adobe-sensei-powers-magento-product-recommendations.html)\\n72. https://magento.com/products/product-recommendations\\n73. https://docs.magento.com/user-guide/marketing/product-recommendations.html\\n74. https://vod.webqem.com/detail/videos/magento-commerce/video/6195503645001/magento-commerce---product-recommendations?autoStart=true&page=1\\n75. https://blog.adobe.com/en/publish/2020/11/23/new-ai-capabilities-for-magento-commerce-improve-retail.html#gs.yw6mtq\\n76. https://developers.google.com/recommender/docs/reference/rest\\n77. https://www.youtube.com/watch?v=nY5U0uQZRyU&t=6s"},{"id":"/2021/10/01/vehicle-suggestions","metadata":{"permalink":"/ai-kb/blog/2021/10/01/vehicle-suggestions","source":"@site/blog/2021-10-01-vehicle-suggestions.mdx","title":"Vehicle Suggestions","description":"/img/content-blog-raw-blog-vehicle-suggestions-untitled.png","date":"2021-10-01T00:00:00.000Z","formattedDate":"October 1, 2021","tags":[{"label":"nlp","permalink":"/ai-kb/blog/tags/nlp"},{"label":"similarity","permalink":"/ai-kb/blog/tags/similarity"},{"label":"vision","permalink":"/ai-kb/blog/tags/vision"}],"readingTime":13.92,"truncated":false,"authors":[{"name":"Sparsh Agarwal","title":"Principal Developer","url":"https://github.com/sparsh-ai","imageURL":"https://avatars.githubusercontent.com/u/62965911?v=4","key":"sparsh"}],"frontMatter":{"title":"Vehicle Suggestions","authors":"sparsh","tags":["nlp","similarity","vision"]},"prevItem":{"title":"Tools for building recommender systems","permalink":"/ai-kb/blog/2021/10/01/tools-for-building-recommender-systems"},"nextItem":{"title":"Web Scraping using Scrapy, BS4, and Selenium","permalink":"/ai-kb/blog/2021/10/01/web-scraping-using-scrapy-bs4-and-selenium"}},"content":"![/img/content-blog-raw-blog-vehicle-suggestions-untitled.png](/img/content-blog-raw-blog-vehicle-suggestions-untitled.png)\\n\\n# Introduction\\n\\nThe customer owns a franchise store for selling Tesla Automobiles. The objective is to predict user preferences using social media data.\\n\\nTask 1 - Suggest the best vehicle for the given description\\n\\nTask 2 - Suggest the best vehicle for the given social media id of the user\\n\\n## Customer queries\\n\\n```json\\n// car or truck or no mention of vehicle type means Cyber Truck\\n// SUV mention means Model X\\nconst one = \\"I\'m looking for a fast suv that I can go camping without worrying about recharging\\".;\\nconst two = \\"cheap red car that is able to go long distances\\";\\nconst three = \\"i am looking for a daily driver that i can charge everyday, do not need any extras\\";\\nconst four = \\"i like to go offroading a lot on my jeep and i want to do the same with the truck\\";\\nconst five = \\"i want the most basic suv possible\\";\\nconst six = \\"I want all of the addons\\";\\n// mentions of large family or many people means model x\\nconst seven = \\"I have a big family and want to be able to take them around town and run errands without worrying about charging\\";\\n```\\n\\n- Expected output\\n    \\n    ```json\\n    const oneJson = {\\n    vehicle: \'Model X\',\\n    trim : \'adventure\',\\n    exteriorColor: \'whiteExterior\',\\n    wheels: \\"22Performance\\",\\n    tonneau: \\"powerTonneau\\",\\n    packages: \\"\\",\\n    interiorAddons: \\"\\",\\n    interiorColor: \\"blackInterior\\",\\n    range: \\"extendedRange\\",\\n    software: \\"\\",\\n    }\\n    \\n    const twoJSON = {\\n    vehicle: \'Cyber Truck\',\\n    trim : \'base\',\\n    exteriorColor: \'whiteExterior\',\\n    wheels: \\"21AllSeason\\",\\n    tonneau: \\"powerTonneau\\",\\n    packages: \\"\\",\\n    interiorAddons: \\"\\",\\n    interiorColor: \\"blackInterior\\",\\n    range: \\"extendedRange\\",\\n    software: \\"\\",\\n    }\\n    \\n    const threeJSON = {\\n    vehicle: \'Cyber Truck\',\\n    trim : \'base\',\\n    exteriorColor: \'whiteExterior\',\\n    wheels: \\"21AllSeason\\",\\n    tonneau: \\"powerTonneau\\",\\n    packages: \\"\\",\\n    interiorAddons: \\"\\",\\n    interiorColor: \\"blackInterior\\",\\n    range: \\"standardRange\\",\\n    software: \\"\\",\\n    }\\n    \\n    const fourJSON = {\\n    vehicle: \'Cyber Truck\',\\n    trim : \'adventure\',\\n    exteriorColor: \'whiteExterior\',\\n    wheels: \\"20AllTerrain\\",\\n    tonneau: \\"powerTonneau\\",\\n    packages: \\"offroadPackage,matchingSpareTire\\",\\n    interiorAddons: \\"\\",\\n    interiorColor: \\"blackInterior\\",\\n    range: \\"extendedRange\\",\\n    software: \\"\\",\\n    }\\n    \\n    const fiveJSON = {\\n    vehicle: \'Model X\',\\n    trim : \'base\',\\n    exteriorColor: \'whiteExterior\',\\n    wheels: \\"20AllTerrain\\",\\n    tonneau: \\"manualTonneau\\",\\n    packages: \\"\\",\\n    interiorAddons: \\"\\",\\n    interiorColor: \\"blackInterior\\",\\n    range: \\"standardRange\\",\\n    software: \\"\\",\\n    }\\n    \\n    const sixJSON = {\\n    vehicle: \'Cyber Truck\',\\n    trim : \'adventure\',\\n    exteriorColor: \'whiteExterior\',\\n    wheels: \\"20AllTerrain\\",\\n    tonneau: \\"powerTonneau\\",\\n    packages: \\"offroadPackage,matchingSpareTire\\",\\n    interiorAddons: \\"wirelessCharger\\",\\n    interiorColor: \\"blackInterior\\",\\n    range: \\"extendedRange\\",\\n    software: \\"selfDrivingPackage\\",\\n    }\\n    \\n    const sevenJSON = {\\n    vehicle: \'Model X\',\\n    trim : \'base\',\\n    exteriorColor: \'whiteExterior\',\\n    wheels: \\"21AllSeason\\",\\n    tonneau: \\"powerTonneau\\",\\n    packages: \\"\\",\\n    interiorAddons: \\"\\",\\n    interiorColor: \\"blackInterior\\",\\n    range: \\"mediumRange\\",\\n    software: \\"\\",\\n    }\\n    ```\\n    \\n- Vehicle model configurations\\n    \\n    ```json\\n    const configuration = {\\n    meta: {\\n    configurationId: \'???\',\\n    storeId: \'US_SALES\',\\n    country: \'US\',\\n    version: \'1.0\',\\n    effectiveDate: \'???\',\\n    currency: \'USD\',\\n    locale: \'en-US\',\\n    availableLocales: [\'en-US\'],\\n    },\\n    \\n    defaults: {\\n    basePrice: 50000,\\n    deposit: 1000,\\n    initialSelection: [\\n    \'adventure\',\\n    \'whiteExterior\',\\n    \'21AllSeason\',\\n    \'powerTonneau\',\\n    \'blackInterior\',\\n    \'mediumRange\',\\n    ],\\n    },\\n    \\n    groups: {\\n    trim: {\\n    name: { \'en-US\': \'Choose trim\' },\\n    multiselect: false,\\n    required: true,\\n    options: [\'base\', \'adventure\'],\\n    },\\n    exteriorColor: {\\n    name: { \'en-US\': \'Choose paint\' },\\n    multiselect: false,\\n    required: true,\\n    options: [\\n    \'whiteExterior\',\\n    \'blueExterior\',\\n    \'silverExterior\',\\n    \'greyExterior\',\\n    \'blackExterior\',\\n    \'redExterior\',\\n    \'greenExterior\',\\n    ],\\n    },\\n    wheels: {\\n    name: { \'en-US\': \'Choose wheels\' },\\n    multiselect: false,\\n    required: true,\\n    options: [\'21AllSeason\', \'20AllTerrain\', \'22Performance\'],\\n    },\\n    tonneau: {\\n    name: { \'en-US\': \'Choose tonneau cover\' },\\n    multiselect: false,\\n    required: true,\\n    options: [\'manualTonneau\', \'powerTonneau\'],\\n    },\\n    packages: {\\n    name: { \'en-US\': \'Choose upgrades\' },\\n    multiselect: true,\\n    required: false,\\n    options: [\'offroadPackage\', \'matchingSpareTire\'],\\n    },\\n    interiorColor: {\\n    name: { \'en-US\': \'Choose interior\' },\\n    multiselect: false,\\n    required: true,\\n    options: [\'greyInterior\', \'blackInterior\', \'greenInterior\'],\\n    },\\n    interiorAddons: {\\n    name: { \'en-US\': \'Choose upgrade\' },\\n    multiselect: true,\\n    required: false,\\n    options: [\'wirelessCharger\'],\\n    },\\n    range: {\\n    name: { \'en-US\': \'Choose range\' },\\n    multiselect: false,\\n    required: true,\\n    options: [\'standardRange\', \'mediumRange\', \'extendedRange\'],\\n    },\\n    software: {\\n    name: { \'en-US\': \'Choose upgrade\' },\\n    multiselect: true,\\n    required: false,\\n    options: [\'selfDrivingPackage\'],\\n    },\\n    specs: {\\n    name: { \'en-US\': \'Specs overview *\' },\\n    attrs: {\\n    description: {\\n    \'en-US\':\\n    \\"* Options, specs and pricing may change as we approach production. We\'ll contact you to review any updates to your preferred build.\\",\\n    },\\n    },\\n    multiselect: false,\\n    required: false,\\n    options: [\'acceleration\', \'power\', \'towing\', \'range\'],\\n    },\\n    },\\n    \\n    options: {\\n    base: {\\n    name: { \'en-US\': \'Base\' },\\n    attrs: {\\n    description: { \'en-US\': \'Production begins 2022\' },\\n    },\\n    visual: true,\\n    price: 0,\\n    },\\n    adventure: {\\n    name: { \'en-US\': \'Adventure\' },\\n    attrs: {\\n    description: { \'en-US\': \'Production begins 2021\' },\\n    },\\n    visual: true,\\n    price: 10000,\\n    },\\n    \\n    standardRange: {\\n    name: { \'en-US\': \'Standard\' },\\n    attrs: {\\n    description: { \'en-US\': \'230+ miles\' },\\n    },\\n    price: 0,\\n    },\\n    mediumRange: {\\n    name: { \'en-US\': \'Medium\' },\\n    attrs: {\\n    description: { \'en-US\': \'300+ miles\' },\\n    },\\n    price: 3000,\\n    },\\n    extendedRange: {\\n    name: { \'en-US\': \'Extended\' },\\n    attrs: {\\n    description: { \'en-US\': \'400+ miles\' },\\n    },\\n    price: 8000,\\n    },\\n    \\n    greenExterior: {\\n    name: { \'en-US\': \'Adirondack Green\' },\\n    attrs: {\\n    imageUrl: \'/public/images/configurationOptions/exteriorcolors/green.svg\',\\n    },\\n    visual: true,\\n    price: 2000,\\n    },\\n    blueExterior: {\\n    name: { \'en-US\': \'Trestles Blue\' },\\n    attrs: {\\n    imageUrl: \'/public/images/configurationOptions/exteriorcolors/blue.svg\',\\n    },\\n    visual: true,\\n    price: 1000,\\n    },\\n    whiteExterior: {\\n    name: { \'en-US\': \'Arctic White\' },\\n    attrs: {\\n    imageUrl: \'/public/images/configurationOptions/exteriorcolors/white.svg\',\\n    },\\n    visual: true,\\n    price: 0,\\n    },\\n    silverExterior: {\\n    name: { \'en-US\': \'Silver Gracier\' },\\n    attrs: {\\n    imageUrl: \'/public/images/configurationOptions/exteriorcolors/silver.svg\',\\n    },\\n    visual: true,\\n    price: 1000,\\n    },\\n    blackExterior: {\\n    name: { \'en-US\': \'Cosmic Black\' },\\n    attrs: {\\n    imageUrl: \'/public/images/configurationOptions/exteriorcolors/black.svg\',\\n    },\\n    visual: true,\\n    price: 1000,\\n    },\\n    redExterior: {\\n    name: { \'en-US\': \'Red Rocks\' },\\n    attrs: {\\n    imageUrl: \'/public/images/configurationOptions/exteriorcolors/red.svg\',\\n    },\\n    visual: true,\\n    price: 2000,\\n    },\\n    greyExterior: {\\n    name: { \'en-US\': \'Antracite Grey\' },\\n    attrs: {\\n    imageUrl: \'/public/images/configurationOptions/exteriorcolors/grey.svg\',\\n    },\\n    visual: true,\\n    price: 1000,\\n    },\\n    \\n    \'21AllSeason\': {\\n    name: { \'en-US\': \'21\\" Cast Wheel - All Season\' },\\n    attrs: {\\n    imageUrl: \'/public/images/configurationOptions/wheels/twentyone.svg\',\\n    },\\n    visual: true,\\n    price: 0,\\n    },\\n    \'20AllTerrain\': {\\n    name: { \'en-US\': \'20\\" Forged Wheel - All Terrain\' },\\n    attrs: {\\n    imageUrl: \'/public/images/configurationOptions/wheels/twenty.svg\',\\n    },\\n    visual: true,\\n    price: 0,\\n    },\\n    \'22Performance\': {\\n    name: { \'en-US\': \'22\\" Cast Wheel - Performance\' },\\n    attrs: {\\n    imageUrl: \'/public/images/configurationOptions/wheels/twentytwo.svg\',\\n    },\\n    visual: true,\\n    price: 2000,\\n    },\\n    \\n    manualTonneau: {\\n    name: { \'en-US\': \'Manual\' },\\n    attrs: {\\n    description: { \'en-US\': \'Description here\' },\\n    },\\n    price: 0,\\n    },\\n    powerTonneau: {\\n    name: { \'en-US\': \'Powered\' },\\n    attrs: {\\n    description: { \'en-US\': \'Description here\' },\\n    },\\n    price: 0,\\n    },\\n    \\n    blackInterior: {\\n    name: { \'en-US\': \'Black\' },\\n    attrs: {\\n    imageUrl: \'/public/images/configurationOptions/interiorcolors/black.svg\',\\n    },\\n    visual: true,\\n    price: 0,\\n    },\\n    greyInterior: {\\n    name: { \'en-US\': \'Grey\' },\\n    attrs: {\\n    imageUrl: \'/public/images/configurationOptions/interiorcolors/grey.svg\',\\n    },\\n    visual: true,\\n    price: 1000,\\n    },\\n    greenInterior: {\\n    name: { \'en-US\': \'Green\' },\\n    attrs: {\\n    imageUrl: \'/public/images/configurationOptions/interiorcolors/green.svg\',\\n    },\\n    visual: true,\\n    price: 2000,\\n    },\\n    \\n    offroadPackage: {\\n    name: { \'en-US\': \'Off-Road\' },\\n    attrs: {\\n    description: { \'en-US\': \'Lorem ipsum dolor sit amet.\' },\\n    imageUrl: \'/public/images/configurationOptions/packages/offroad.png\',\\n    },\\n    visual: true,\\n    price: 5000,\\n    },\\n    matchingSpareTire: {\\n    name: { \'en-US\': \'Matching Spare Tire\' },\\n    attrs: {\\n    description: { \'en-US\': \'Full sized tire\' },\\n    imageUrl: \'/public/images/configurationOptions/packages/spare.png\',\\n    },\\n    price: 500,\\n    },\\n    \\n    wirelessCharger: {\\n    name: { \'en-US\': \'Wireless charger\' },\\n    attrs: {\\n    description: { \'en-US\': \'Lorem ipsum dolor sit amet.\' },\\n    imageUrl: \'/public/images/configurationOptions/packages/wireless.png\',\\n    },\\n    price: 100,\\n    },\\n    selfDrivingPackage: {\\n    name: { \'en-US\': \'Autonomy\' },\\n    attrs: {\\n    description: { \'en-US\': \'Lorem ipsum dolor sit amet.\' },\\n    imageUrl: \'/public/images/configurationOptions/packages/autonomy.png\',\\n    },\\n    price: 7000,\\n    },\\n    \\n    acceleration: {\\n    name: { \'en-US\': \'0 - 60 mph\' },\\n    attrs: {\\n    units: { \'en-US\': \'sec\' },\\n    decimals: 1,\\n    },\\n    value: 3.4,\\n    },\\n    power: {\\n    name: { \'en-US\': \'Horsepower\' },\\n    attrs: {\\n    units: { \'en-US\': \'hp\' },\\n    },\\n    value: 750,\\n    },\\n    towing: {\\n    name: { \'en-US\': \'Towing\' },\\n    attrs: {\\n    units: { \'en-US\': \'lbs\' },\\n    },\\n    value: 10000,\\n    },\\n    range: {\\n    name: { \'en-US\': \'Range\' },\\n    attrs: {\\n    units: { \'en-US\': \'mi\' },\\n    },\\n    value: 400,\\n    },\\n    }\\n    };\\n    ```\\n    \\n\\n## Public datasets\\n\\n- Instagram: 16539 images from 972 Instagram influencers ([link](https://github.com/gvsi/instagram-like-predictor))\\n- TechCrunchPosts: ([link](https://www.kaggle.com/thibalbo/techcrunch-posts-compilation))\\n- Tweets: ([link](https://data.world/data-society/twitter-user-data))\\n\\nPrimary (available for academic use only, need university affiliation for access)\\n\\n- [A Dataset and Benchmarks for Multimedia Social Analysis](https://arxiv.org/abs/2006.08335)\\n\\nSecondary (low quality data, not sure if can be used at all)\\n\\n- [Hacker News Posts](https://www.kaggle.com/hacker-news/hacker-news-posts)\\n- [TechCrunch Posts Compilation](https://www.kaggle.com/thibalbo/techcrunch-posts-compilation)\\n- Instagram image data [HowTo](https://towardsdatascience.com/predict-the-number-of-likes-on-instagram-a7ec5c020203)\\n- Flikr Large with likes and comments\\n- [The Images of Groups Dataset](http://chenlab.ece.cornell.edu/people/Andy/ImagesOfGroups.html)\\n- [http://www.multimediaeval.org/datasets/](http://www.multimediaeval.org/datasets/)\\n- [The InstaCities1M Dataset](https://gombru.github.io/2018/08/01/InstaCities1M/)\\n- [Multimodal Meme Classification: Identifying Offensive Content in Image and Text](https://www.insight-centre.org/sites/default/files/publications/memes_classification_lrec_1.pdf)\\n- [Understanding Police Social Media Usage Through Posts and Tweets](https://dataverse.harvard.edu/dataset.xhtml?persistentId=doi:10.7910/DVN/NRPHLC)\\n- Topic clusters text\\n    - Model X\\n        - I like model X\\n        - I want to buy model X\\n        - Model X is my favorite car\\n        - Tesla Modelx is my dream\\n        - modelx tesla love\\n    - Cyber Truck\\n        - I like Cyber Truck\\n        - I want to buy Cyber Truck\\n        - Cyber Truck is my favorite car\\n        - Tesla Cyber Truck is my dream\\n        - CyberTruck tesla love\\n    - Adventure\\n        - I like adventure\\n        - sports i play\\n        - i went on trip\\n        - I travels a lot\\n        - car adventure\\n        \\n    - Exterior Color White\\n        - I like white color\\n        - White is my fav\\n        - white car love\\n        - I like white exterior\\n    - Exterior Color Black\\n        - I like Black color\\n        - Black is my fav\\n        - Black car love\\n        - I like Black exterior\\n    - Exterior Color Blue\\n        - I like Blue color\\n        - Blue is my fav\\n        - Blue car love\\n        - I like Blue exterior\\n    - Exterior Color Green\\n        - I like Green color\\n        - Green is my fav\\n        - Green car love\\n        - I like Green exterior\\n    - Exterior Color Red\\n        - I like Red color\\n        - Red is my fav\\n        - Red car love\\n        - I like Red exterior\\n    - Exterior Color Grey\\n        - I like Grey color\\n        - Grey is my fav\\n        - Grey car love\\n        - I like Grey exterior\\n    - Exterior Color Silver\\n        - I like Silver color\\n        - Silver is my fav\\n        - Silver car love\\n        - I like Silver exterior\\n    - Self driving\\n        - I like self driving technology\\n        - selfDrivingPackage\\n        - selfDrivingtech love\\n        - self drive is my fav\\n        - self driving car is amazing\\n- Celebs\\n    \\n    ![/img/content-blog-raw-blog-vehicle-suggestions-untitled-1.png](/img/content-blog-raw-blog-vehicle-suggestions-untitled-1.png)\\n    \\n\\n## Logical Reasoning\\n\\n- If I implicitly rate pictures of blue car, that means I might prefer a blue car.\\n- If I like posts of self-driving, that means I might prefer a self-driving option.\\n\\n# Scope\\n\\n### Scope 1\\n\\n![/img/content-blog-raw-blog-vehicle-suggestions-untitled-2.png](/img/content-blog-raw-blog-vehicle-suggestions-untitled-2.png)\\n\\n### Scope 2\\n\\nmedia content categories: text and images\\n\\nplatforms: facebook, twitter and instagram\\n\\nimplicit rating categories: like, comment, share\\n\\ncolumns: userid, timestamp, platform, type, content, rating\\n\\n# Model Framework\\n\\n### Model framework 1\\n\\n1. Convert user\'s natural language query into vector using Universal Sentence Embedding model\\n2. Create a product specs binary matrix based on different categories\\n3. Find TopK similar query vectors using cosine distance\\n4. For each TopK vector, Find TopM product specs using interaction table weights\\n5. For each TopM specification, find TopN similar specs using binary matrix\\n6. Show all the qualified product specifications\\n\\n### Model framework 2\\n\\n1. Seed data: 10 users with ground-truth persona, media content and implicit ratings\\n2. Inflated data: 10 users with media content and implicit ratings\\n3. media content \u2192 Implicit rating (A)\\n4. media content \u2192 feature vector (B) + (A) \u2192 weighted pooling \u2192 similar users (C)\\n5. media content \u2192 QA model \u2192 slot filling \u2192 global pooling \u2192 item associations (D)\\n6. (C) \u2192 content-based filtering \u2192 item recommendations \u2192 (D) \u2192 top-k recommendations\\n\\n**User selection**\\n\\n- People who are connected to social media community of electric vehicles\\n- Seed users are those who already have an electric vehicle\\n- Inflated users are those who doesn\'t own an EV but inclined to purchase\\n- Users having presense on all three sites or at least 2\\n- List of common users\\n    \\n    [https://www.facebook.com/gossman](https://www.facebook.com/gossman)\\n    \\n    [https://www.facebook.com/ryanm06](https://www.facebook.com/ryanm06)\\n    \\n    [https://www.facebook.com/chad.turner.7146](https://www.facebook.com/chad.turner.7146)\\n    \\n    [https://www.facebook.com/cjacobs05](https://www.facebook.com/cjacobs05)\\n    \\n    [https://www.facebook.com/MafiaAllen](https://www.facebook.com/MafiaAllen)\\n    \\n    [https://www.facebook.com/rahul.mii.33](https://www.facebook.com/rahul.mii.33)\\n    \\n    [https://www.facebook.com/francisco.chavira.547](https://www.facebook.com/francisco.chavira.547)\\n    \\n    [https://www.facebook.com/JayTheillest74](https://www.facebook.com/JayTheillest74)\\n    \\n    [https://www.facebook.com/michael.creighton20](https://www.facebook.com/michael.creighton20)\\n    \\n    [https://www.facebook.com/darryl.grigggardening](https://www.facebook.com/darryl.grigggardening)\\n    \\n    [https://www.facebook.com/4X4Aus/](https://www.facebook.com/4X4Aus/)\\n    \\n    [https://www.instagram.com/minnyrc/](https://www.instagram.com/minnyrc/)\\n    \\n    [https://www.instagram.com/warnerbu7lt/](https://www.instagram.com/warnerbu7lt/)\\n    \\n- List of celebs\\n    1. [https://en.wikipedia.org/wiki/List_of_most-followed_Instagram_accounts](https://en.wikipedia.org/wiki/List_of_most-followed_Instagram_accounts)\\n    2. [https://en.wikipedia.org/wiki/List_of_most-followed_Twitter_accounts](https://en.wikipedia.org/wiki/List_of_most-followed_Twitter_accounts)\\n    3. [https://en.wikipedia.org/wiki/List_of_most-followed_Facebook_pages](https://en.wikipedia.org/wiki/List_of_most-followed_Facebook_pages)\\n    \\n    [\'Jennifer Lopez\', \'Virat Kohli\', \'Ariana Grande\', \'Dwayne Johnson\', \'Kylie Jenner\', \'Lionel Messi\', \'LeBron James\', \'Beyonc\xe9\', \'Justin Bieber\', \'Akshay Kumar\', \'Demi Lovato\', \'Kendall Jenner\', \'Nicki Minaj\', \'Khlo\xe9 Kardashian\', \'Kim Kardashian\', \'Gigi Hadid\', \'Ellen DeGeneres\', \'Deepika Padukone\', \'Rihanna\', \'Shakira\', \'Cardi B\', \'Eminem\', \'Drake\', \'Chris Brown\', \'Maluma\', \'Vin Diesel\', \'Ronaldinho\', \'Kevin Hart\', \'Emma Watson\', \'Shawn Mendes\', \'Neymar\', \'Justin Timberlake\', \'Katy Perry\', \'Donald Trump\', \'Lady Gaga\', \'Amitabh Bachchan\', \'Selena Gomez\', \'Lil Wayne\', \'Elon Musk\', \'Britney Spears\', \'Jimmy Fallon\', \'Bill Gates\', \'Ariana Grande\', \'Miley Cyrus\', \'Oprah Winfrey\', \'Cristiano Ronaldo\', \'Salman Khan\', \'Shah Rukh Khan\', \'Niall Horan\']\\n    \\n\\n### Model framework 3\\n\\nUser-User Similarity (clustering)\\n\\n- User \u2192 Media content \u2192 Embedding \u2192 Average pooling\\n- Cosine Similarity of user\'s social vector with other user\'s social vector\\n\\nUser-Item Similarity (reranking)\\n\\n- **User \u2192 Implicit Rating on media content M \u2192 M\'s correlation with item features**\\n- Item features: familySize\\n- Cosine Similarity of user\'s social vector with item\'s feature vector\\n\\nUser-User Similarity (clustering)\\n\\n- User \u2192 Media content \u2192 Embedding \u2192 Average pooling\\n- Cosine Similarity of user\'s social vector with other user\'s social vector\\n\\nUser-Item Similarity (reranking)\\n\\n- **User \u2192 Implicit Rating on media content M \u2192 M\'s correlation with item features**\\n- Item features: familySize\\n- Cosine Similarity of user\'s social vector with item\'s feature vector\\n\\n### Model framework 4\\n\\n![/img/content-blog-raw-blog-vehicle-suggestions-untitled-3.png](/img/content-blog-raw-blog-vehicle-suggestions-untitled-3.png)\\n\\nText \u2192 Prepare \u2192 Vectorize \u2192 Average \u2192 Similar Users\\n\\nImage \u2192 Prepare \u2192 Vectorize \u2192 Average \u2192 Similar Users\\n\\nText \u2192 Prepare \u2192 QA \u2192 Slot filling\\n\\nImage \u2192 Prepare \u2192 VQA \u2192 Slot filling\\n\\nImage \u2192 Similar Image from users \u2192 Detailed enquiry\\n\\n### Model framework 5\\n\\n1. Topic Clusters Text\\n2. Topic Clusters Image\\n3. Fetch raw text and images\\n4. Combine, Clean and Store text in text dataframe\\n5. Vectorize Texts\\n6. Cosine similarities of texts with topic clusters\\n7. Vectorize Images\\n8. Cosine similarities of images with topic clusters\\n\\n# Experimental Setup\\n\\n- Experiment 1\\n    \\n    ```python\\n    import numpy as np\\n    import pandas as pd\\n    import tensorflow_hub as hub\\n    from itertools import product\\n    from sklearn.preprocessing import OneHotEncoder\\n    from sklearn.metrics.pairwise import cosine_similarity\\n    \\n    vehicle = [\'modelX\', \'cyberTruck\']\\n    trim = [\'adventure\', \'base\']\\n    exteriorColor = [\'whiteExterior\', \'blueExterior\', \'silverExterior\', \'greyExterior\', \'blackExterior\', \'redExterior\', \'greenExterior\']\\n    wheels = [\'20AllTerrain\', \'21AllSeason\', \'22Performance\']\\n    tonneau = [\'powerTonneau\', \'manualTonneau\']\\n    interiorColor = [\'blackInterior\', \'greyInterior\', \'greenInterior\']\\n    range = [\'standardRange\', \'mediumRange\', \'extendedRange\']\\n    packages = [\'offroadPackage\', \'matchingSpareTire\', \'offroadPackage,matchingSpareTire\', \'None\']\\n    interiorAddons = [\'wirelessCharger\', \'None\']\\n    software = [\'selfDrivingPackage\', \'None\']\\n    \\n    specs_cols = [\'vehicle\', \'trim\', \'exteriorColor\', \'wheels\', \'tonneau\', \'interiorColor\', \'range\', \'packages\', \'interiorAddons\', \'software\']\\n    specs = pd.DataFrame(list(product(vehicle, trim, exteriorColor, wheels, tonneau, interiorColor, range, packages, interiorAddons, software)),\\n                         columns=specs_cols)\\n    \\n    enc = OneHotEncoder(handle_unknown=\'error\', sparse=False)\\n    specs = pd.DataFrame(enc.fit_transform(specs))\\n    \\n    specs_ids = specs.index.tolist()\\n    \\n    query_list = [\\"I\'m looking for a fast suv that I can go camping without worrying about recharging\\",\\n                  \\"cheap red car that is able to go long distances\\",\\n                  \\"i am looking for a daily driver that i can charge everyday, do not need any extras\\",\\n                  \\"i like to go offroading a lot on my jeep and i want to do the same with the truck\\",\\n                  \\"i want the most basic suv possible\\",\\n                  \\"I want all of the addons\\", \\n                  \\"I have a big family and want to be able to take them around town and run errands without worrying about charging\\"]\\n    \\n    queries = pd.DataFrame(query_list, columns=[\'query\'])\\n    query_ids = queries.index.tolist()\\n    \\n    const_oneJSON = {\\n    \'vehicle\': \'modelX\',\\n    \'trim\' : \'adventure\',\\n    \'exteriorColor\': \'whiteExterior\',\\n    \'wheels\': \\"22Performance\\",\\n    \'tonneau\': \\"powerTonneau\\",\\n    \'packages\': \\"None\\",\\n    \'interiorAddons\': \\"None\\",\\n    \'interiorColor\': \\"blackInterior\\",\\n    \'range\': \\"extendedRange\\",\\n    \'software\': \\"None\\",\\n    }\\n    \\n    const_twoJSON = {\\n    \'vehicle\': \'cyberTruck\',\\n    \'trim\' : \'base\',\\n    \'exteriorColor\': \'whiteExterior\',\\n    \'wheels\': \\"21AllSeason\\",\\n    \'tonneau\': \\"powerTonneau\\",\\n    \'packages\': \\"None\\",\\n    \'interiorAddons\': \\"None\\",\\n    \'interiorColor\': \\"blackInterior\\",\\n    \'range\': \\"extendedRange\\",\\n    \'software\': \\"None\\",\\n    }\\n    \\n    const_threeJSON = {\\n    \'vehicle\': \'cyberTruck\',\\n    \'trim\' : \'base\',\\n    \'exteriorColor\': \'whiteExterior\',\\n    \'wheels\': \\"21AllSeason\\",\\n    \'tonneau\': \\"powerTonneau\\",\\n    \'packages\': \\"None\\",\\n    \'interiorAddons\': \\"None\\",\\n    \'interiorColor\': \\"blackInterior\\",\\n    \'range\': \\"standardRange\\",\\n    \'software\': \\"None\\",\\n    }\\n    \\n    const_fourJSON = {\\n    \'vehicle\': \'cyberTruck\',\\n    \'trim\' : \'adventure\',\\n    \'exteriorColor\': \'whiteExterior\',\\n    \'wheels\': \\"20AllTerrain\\",\\n    \'tonneau\': \\"powerTonneau\\",\\n    \'packages\': \\"offroadPackage,matchingSpareTire\\",\\n    \'interiorAddons\': \\"None\\",\\n    \'interiorColor\': \\"blackInterior\\",\\n    \'range\': \\"extendedRange\\",\\n    \'software\': \\"None\\",\\n    }\\n    \\n    const_fiveJSON = {\\n    \'vehicle\': \'modelX\',\\n    \'trim\' : \'base\',\\n    \'exteriorColor\': \'whiteExterior\',\\n    \'wheels\': \\"20AllTerrain\\",\\n    \'tonneau\': \\"manualTonneau\\",\\n    \'packages\': \\"None\\",\\n    \'interiorAddons\': \\"None\\",\\n    \'interiorColor\': \\"blackInterior\\",\\n    \'range\': \\"standardRange\\",\\n    \'software\': \\"None\\",\\n    }\\n    \\n    const_sixJSON = {\\n    \'vehicle\': \'cyberTruck\',\\n    \'trim\' : \'adventure\',\\n    \'exteriorColor\': \'whiteExterior\',\\n    \'wheels\': \\"20AllTerrain\\",\\n    \'tonneau\': \\"powerTonneau\\",\\n    \'packages\': \\"offroadPackage,matchingSpareTire\\",\\n    \'interiorAddons\': \\"wirelessCharger\\",\\n    \'interiorColor\': \\"blackInterior\\",\\n    \'range\': \\"extendedRange\\",\\n    \'software\': \\"selfDrivingPackage\\",\\n    }\\n    \\n    const_sevenJSON = {\\n    \'vehicle\': \'modelX\',\\n    \'trim\' : \'base\',\\n    \'exteriorColor\': \'whiteExterior\',\\n    \'wheels\': \\"21AllSeason\\",\\n    \'tonneau\': \\"powerTonneau\\",\\n    \'packages\': \\"None\\",\\n    \'interiorAddons\': \\"None\\",\\n    \'interiorColor\': \\"blackInterior\\",\\n    \'range\': \\"mediumRange\\",\\n    \'software\': \\"None\\",\\n    }\\n    \\n    historical_data = pd.DataFrame([const_oneJSON, const_twoJSON, const_threeJSON, const_fourJSON, const_fiveJSON, const_sixJSON, const_sevenJSON])\\n    \\n    input_vec = enc.transform([specs_frame.append(historical_data.iloc[0], sort=False).iloc[-1]])\\n    idx = np.argsort(-cosine_similarity(input_vec, specs.values))[0,:][:1]\\n    rslt = enc.inverse_transform([specs.iloc[idx]])\\n    \\n    interactions = pd.DataFrame(columns=[\'query_id\',\'specs_id\'])\\n    interactions[\'query_id\'] = queries.index.tolist()\\n    input_vecs = enc.transform(specs_frame.append(historical_data, sort=False).iloc[-len(historical_data):])\\n    interactions[\'specs_id\'] = np.argsort(-cosine_similarity(input_vecs, specs.values))[:,0]\\n    \\n    module_url = \\"https://tfhub.dev/google/universal-sentence-encoder/4\\" \\n    embed_model = hub.load(module_url)\\n    def embed(input):\\n      return embed_model(input)\\n    query_vecs = embed(queries[\'query\'].tolist()).numpy()\\n    \\n    _query = input(\'Please enter query: \') or \'i want the most basic suv possible\'\\n    _query_vec = embed([_query]).numpy()\\n    _match_qid = np.argsort(-cosine_similarity(_query_vec, query_vecs))[0,:][:1]\\n    _match_sid = interactions.loc[interactions[\'query_id\']==_match_qid[0], \'specs_id\'].values[0]\\n    input_vec = enc.transform([specs_frame.append(historical_data.iloc[0], sort=False).iloc[-1]])\\n    idx = np.argsort(-cosine_similarity([specs.iloc[_match_sid].values], specs.values))[0,:][:5]\\n    results = []\\n    for x in idx:\\n      results.append(enc.inverse_transform([specs.iloc[x]]))\\n    _temp = np.array(results).reshape(5,-1)\\n    _temp = pd.DataFrame(_temp, columns=specs_frame.columns)\\n    print(_temp)\\n    ```\\n    \\n\\n## Experiment 2\\n\\nCeleb Scraping\\n\\n### Facebook Scraping\\n\\n![/img/content-blog-raw-blog-vehicle-suggestions-untitled-4.png](/img/content-blog-raw-blog-vehicle-suggestions-untitled-4.png)\\n\\n### Twitter Scraping\\n\\n![/img/content-blog-raw-blog-vehicle-suggestions-untitled-5.png](/img/content-blog-raw-blog-vehicle-suggestions-untitled-5.png)\\n\\n### Dataframe\\n\\n![/img/content-blog-raw-blog-vehicle-suggestions-untitled-6.png](/img/content-blog-raw-blog-vehicle-suggestions-untitled-6.png)\\n\\n### Insta Image Grid\\n\\n![/img/content-blog-raw-blog-vehicle-suggestions-untitled-7.png](/img/content-blog-raw-blog-vehicle-suggestions-untitled-7.png)\\n\\n### User Text NER\\n\\n![/img/content-blog-raw-blog-vehicle-suggestions-untitled-8.png](/img/content-blog-raw-blog-vehicle-suggestions-untitled-8.png)\\n\\n## Experiment 3\\n\\nTopic model\\n\\n### Topic scores\\n\\n![/img/content-blog-raw-blog-vehicle-suggestions-untitled-9.png](/img/content-blog-raw-blog-vehicle-suggestions-untitled-9.png)\\n\\n### JSON rules\\n\\n![/img/content-blog-raw-blog-vehicle-suggestions-untitled-10.png](/img/content-blog-raw-blog-vehicle-suggestions-untitled-10.png)\\n\\n# Results and Discussion\\n\\n- API with 3 input fields - Facebook username, Twitter handle & Instagram username\\n- The system will automatically scrap the user\'s publicly available text and images from these 3 social media platforms and provide a list of recommendations from most to least preferred product"},{"id":"/2021/10/01/web-scraping-using-scrapy-bs4-and-selenium","metadata":{"permalink":"/ai-kb/blog/2021/10/01/web-scraping-using-scrapy-bs4-and-selenium","source":"@site/blog/2021-10-01-web-scraping-using-scrapy-bs4-and-selenium.mdx","title":"Web Scraping using Scrapy, BS4, and Selenium","description":"1. Handling single request & response by extracting a city\u2019s weather from a weather site using Scrapy","date":"2021-10-01T00:00:00.000Z","formattedDate":"October 1, 2021","tags":[{"label":"scraping","permalink":"/ai-kb/blog/tags/scraping"}],"readingTime":3.78,"truncated":false,"authors":[{"name":"Sparsh Agarwal","title":"Principal Developer","url":"https://github.com/sparsh-ai","imageURL":"https://avatars.githubusercontent.com/u/62965911?v=4","key":"sparsh"}],"frontMatter":{"title":"Web Scraping using Scrapy, BS4, and Selenium","authors":"sparsh","tags":["scraping"]},"prevItem":{"title":"Vehicle Suggestions","permalink":"/ai-kb/blog/2021/10/01/vehicle-suggestions"},"nextItem":{"title":"Web Scraping with Gazpacho","permalink":"/ai-kb/blog/2021/10/01/web-scraping-with-gazpacho"}},"content":"1. Handling single request & response by extracting a city\u2019s weather from a weather site using Scrapy\\n2. Handling multiple request & response by extracting book details from a dummy online book store using Scrapy\\n3. Scrape the cover images of all the books from the website [books.toscrape.com](http://books.toscrape.com/) using Scrapy\\n4. Logging into Facebook using Selenium\\n5. Extract PM2.5 data from [openaq.org](http://openaq.org) using Selenium\\n6. Extract PM2.5 data from [openaq.org](http://openaq.org) using Selenium Scrapy\\n\\n:::note Scrapy vs. Selenium\\n\\nSelenium is an automation tool for testing web applications. It uses a webdriver as an interface to control webpages through programming languages. So, this gives Selenium the capability to handle dynamic webpages effectively. Selenium is capable of extracting data on its own. It is true, but it has its caveats. Selenium cannot handle large data, but Scrapy can handle large data with ease. Also, Selenium is much slower when compared to Scrapy. So, the smart choice would be to use Selenium with Scrapy to scrape dynamic webpages containing large data, consuming less time. Combining Selenium with Scrapy is a simpler process. All that needs to be done is let Selenium render the webpage and once it is done, pass the webpage\u2019s source to create a Scrapy Selector object. And from here on, Scrapy can crawl the page with ease and effectively extract a large amount of data.\\n\\n:::\\n\\n```python\\n# SKELETON FOR COMBINING SELENIUM WITH SCRAPY\\nfrom scrapy import Selector\\n# Other Selenium and Scrapy imports\\n...\\ndriver = webdriver.Chrome()\\n# Selenium tasks and actions to render the webpage with required content\\nselenium_response_text = driver.page_source\\nnew_selector = Selector(text=selenium_response_text)\\n# Scrapy tasks to extract data from Selector\\n```\\n\\n## Project tree\\n\\n```html\\n.\\n\u251c\u2500\u2500 airQuality\\n\u2502   \u251c\u2500\u2500 countries_list.json\\n\u2502   \u251c\u2500\u2500 get_countries.py\\n\u2502   \u251c\u2500\u2500 get_pm_data.py\\n\u2502   \u251c\u2500\u2500 get_urls.py\\n\u2502   \u251c\u2500\u2500 openaq_data.json\\n\u2502   \u251c\u2500\u2500 openaq_scraper.py\\n\u2502   \u251c\u2500\u2500 README.md\\n\u2502   \u2514\u2500\u2500 urls.json\\n\u251c\u2500\u2500 airQualityScrapy\\n\u2502   \u251c\u2500\u2500 LICENSE\\n\u2502   \u251c\u2500\u2500 openaq\\n\u2502   \u2502   \u251c\u2500\u2500 countries_list.json\\n\u2502   \u2502   \u251c\u2500\u2500 openaq\\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 __init__.py\\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 items.py\\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 middlewares.py\\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 pipelines.py\\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 settings.py\\n\u2502   \u2502   \u2502   \u2514\u2500\u2500 spiders\\n\u2502   \u2502   \u251c\u2500\u2500 output.json\\n\u2502   \u2502   \u251c\u2500\u2500 README.md\\n\u2502   \u2502   \u251c\u2500\u2500 scrapy.cfg\\n\u2502   \u2502   \u2514\u2500\u2500 urls.json\\n\u2502   \u251c\u2500\u2500 performance_comparison\\n\u2502   \u2502   \u251c\u2500\u2500 performance_comparison\\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 __init__.py\\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 items.py\\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 middlewares.py\\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 pipelines.py\\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 settings.py\\n\u2502   \u2502   \u2502   \u2514\u2500\u2500 spiders\\n\u2502   \u2502   \u251c\u2500\u2500 README.md\\n\u2502   \u2502   \u251c\u2500\u2500 scrapy.cfg\\n\u2502   \u2502   \u251c\u2500\u2500 scrapy_output.json\\n\u2502   \u2502   \u2514\u2500\u2500 selenium_scraper\\n\u2502   \u2502       \u251c\u2500\u2500 bts_scraper.py\\n\u2502   \u2502       \u251c\u2500\u2500 selenium_output.json\\n\u2502   \u2502       \u2514\u2500\u2500 urls.json\\n\u2502   \u2514\u2500\u2500 README.md\\n\u251c\u2500\u2500 books\\n\u2502   \u251c\u2500\u2500 books\\n\u2502   \u2502   \u251c\u2500\u2500 __init__.py\\n\u2502   \u2502   \u251c\u2500\u2500 items.py\\n\u2502   \u2502   \u251c\u2500\u2500 middlewares.py\\n\u2502   \u2502   \u251c\u2500\u2500 pipelines.py\\n\u2502   \u2502   \u251c\u2500\u2500 settings.py\\n\u2502   \u2502   \u2514\u2500\u2500 spiders\\n\u2502   \u2502       \u251c\u2500\u2500 book_spider.py\\n\u2502   \u2502       \u251c\u2500\u2500 crawl_spider.py\\n\u2502   \u2502       \u2514\u2500\u2500 __init__.py\\n\u2502   \u251c\u2500\u2500 crawl_spider_output.json\\n\u2502   \u251c\u2500\u2500 README.md\\n\u2502   \u2514\u2500\u2500 scrapy.cfg\\n\u251c\u2500\u2500 booksCoverImage\\n\u2502   \u251c\u2500\u2500 booksCoverImage\\n\u2502   \u2502   \u251c\u2500\u2500 __init__.py\\n\u2502   \u2502   \u251c\u2500\u2500 items.py\\n\u2502   \u2502   \u251c\u2500\u2500 middlewares.py\\n\u2502   \u2502   \u251c\u2500\u2500 pipelines.py\\n\u2502   \u2502   \u251c\u2500\u2500 settings.py\\n\u2502   \u2502   \u2514\u2500\u2500 spiders\\n\u2502   \u2502       \u251c\u2500\u2500 image_crawl_spider.py\\n\u2502   \u2502       \u2514\u2500\u2500 __init__.py\\n\u2502   \u251c\u2500\u2500 output.json\\n\u2502   \u251c\u2500\u2500 path\\n\u2502   \u2502   \u2514\u2500\u2500 to\\n\u2502   \u2502       \u2514\u2500\u2500 store\\n\u2502   \u251c\u2500\u2500 README.md\\n\u2502   \u2514\u2500\u2500 scrapy.cfg\\n\u251c\u2500\u2500 etc\\n\u2502   \u2514\u2500\u2500 Selenium\\n\u2502       \u251c\u2500\u2500 chromedriver.exe\\n\u2502       \u251c\u2500\u2500 chromedriver_v87.exe\\n\u2502       \u2514\u2500\u2500 install.sh\\n\u251c\u2500\u2500 facebook\\n\u2502   \u2514\u2500\u2500 login.py\\n\u251c\u2500\u2500 gazpacho1\\n\u2502   \u251c\u2500\u2500 data\\n\u2502   \u2502   \u251c\u2500\u2500 media.html\\n\u2502   \u2502   \u251c\u2500\u2500 ocr.html\\n\u2502   \u2502   \u251c\u2500\u2500 page.html\\n\u2502   \u2502   \u251c\u2500\u2500 static\\n\u2502   \u2502   \u2502   \u2514\u2500\u2500 stheno.mp4\\n\u2502   \u2502   \u2514\u2500\u2500 table.html\\n\u2502   \u251c\u2500\u2500 media\\n\u2502   \u2502   \u251c\u2500\u2500 euryale.png\\n\u2502   \u2502   \u251c\u2500\u2500 medusa.mp3\\n\u2502   \u2502   \u251c\u2500\u2500 medusa.png\\n\u2502   \u2502   \u251c\u2500\u2500 stheno.mp4\\n\u2502   \u2502   \u2514\u2500\u2500 test.png\\n\u2502   \u251c\u2500\u2500 scrap_login.py\\n\u2502   \u251c\u2500\u2500 scrap_media.py\\n\u2502   \u251c\u2500\u2500 scrap_ocr.py\\n\u2502   \u251c\u2500\u2500 scrap_page.py\\n\u2502   \u2514\u2500\u2500 scrap_table.py\\n\u251c\u2500\u2500 houzzdotcom\\n\u2502   \u251c\u2500\u2500 houzzdotcom\\n\u2502   \u2502   \u251c\u2500\u2500 __init__.py\\n\u2502   \u2502   \u251c\u2500\u2500 items.py\\n\u2502   \u2502   \u251c\u2500\u2500 middlewares.py\\n\u2502   \u2502   \u251c\u2500\u2500 pipelines.py\\n\u2502   \u2502   \u251c\u2500\u2500 settings.py\\n\u2502   \u2502   \u2514\u2500\u2500 spiders\\n\u2502   \u2502       \u251c\u2500\u2500 crawl_spider.py\\n\u2502   \u2502       \u2514\u2500\u2500 __init__.py\\n\u2502   \u2514\u2500\u2500 scrapy.cfg\\n\u251c\u2500\u2500 media\\n\u2502   \u2514\u2500\u2500 test.png\\n\u251c\u2500\u2500 README.md\\n\u251c\u2500\u2500 scrapyPractice\\n\u2502   \u251c\u2500\u2500 scrapy.cfg\\n\u2502   \u2514\u2500\u2500 scrapyPractice\\n\u2502       \u251c\u2500\u2500 __init__.py\\n\u2502       \u251c\u2500\u2500 items.py\\n\u2502       \u251c\u2500\u2500 middlewares.py\\n\u2502       \u251c\u2500\u2500 pipelines.py\\n\u2502       \u251c\u2500\u2500 settings.py\\n\u2502       \u2514\u2500\u2500 spiders\\n\u2502           \u2514\u2500\u2500 __init__.py\\n\u2514\u2500\u2500 weather\\n    \u251c\u2500\u2500 output.json\\n    \u251c\u2500\u2500 README.md\\n    \u251c\u2500\u2500 scrapy.cfg\\n    \u2514\u2500\u2500 weather\\n        \u251c\u2500\u2500 __init__.py\\n        \u251c\u2500\u2500 items.py\\n        \u251c\u2500\u2500 middlewares.py\\n        \u251c\u2500\u2500 pipelines.py\\n        \u251c\u2500\u2500 settings.py\\n        \u2514\u2500\u2500 spiders\\n            \u251c\u2500\u2500 __init__.py\\n            \u2514\u2500\u2500 weather_spider.py\\n\\n35 directories, 98 files\\n```\\n\\n![For code, drop me a message on mail or LinkedIn.](/img/content-blog-raw-blog-web-scraping-using-scrapy-bs4-and-selenium-untitled.png)\\n\\nFor code, drop me a message on mail or LinkedIn."},{"id":"/2021/10/01/web-scraping-with-gazpacho","metadata":{"permalink":"/ai-kb/blog/2021/10/01/web-scraping-with-gazpacho","source":"@site/blog/2021-10-01-web-scraping-with-gazpacho.mdx","title":"Web Scraping with Gazpacho","description":"Using gazpacho to Download and Parse the Contents of a Website. Scrape the names of the three \\"Gorgons\\".","date":"2021-10-01T00:00:00.000Z","formattedDate":"October 1, 2021","tags":[{"label":"scraping","permalink":"/ai-kb/blog/tags/scraping"}],"readingTime":0.52,"truncated":false,"authors":[{"name":"Sparsh Agarwal","title":"Principal Developer","url":"https://github.com/sparsh-ai","imageURL":"https://avatars.githubusercontent.com/u/62965911?v=4","key":"sparsh"}],"frontMatter":{"title":"Web Scraping with Gazpacho","authors":"sparsh","tags":["scraping"]},"prevItem":{"title":"Web Scraping using Scrapy, BS4, and Selenium","permalink":"/ai-kb/blog/2021/10/01/web-scraping-using-scrapy-bs4-and-selenium"},"nextItem":{"title":"Wellness tracker chatbot","permalink":"/ai-kb/blog/2021/10/01/wellness-tracker-chatbot"}},"content":"### Using gazpacho to Download and Parse the Contents of a Website. Scrape the names of the three \\"Gorgons\\".\\n\\n![/img/content-blog-raw-blog-web-scraping-with-gazpacho-untitled.png](/img/content-blog-raw-blog-web-scraping-with-gazpacho-untitled.png)\\n\\n### Using gazpacho and pandas to Retrieve the Contents of an HTML Table. Scrape the creature and habitat columns.\\n\\n![/img/content-blog-raw-blog-web-scraping-with-gazpacho-untitled-1.png](/img/content-blog-raw-blog-web-scraping-with-gazpacho-untitled-1.png)\\n\\n### Using gazpacho and Selenium to Retrieve the Contents of a Password-Protected Web Page. Scrape the quote text behind the login form.\\n\\n![/img/content-blog-raw-blog-web-scraping-with-gazpacho-untitled-2.png](/img/content-blog-raw-blog-web-scraping-with-gazpacho-untitled-2.png)\\n\\n### Using gazpacho and pytesseract to Parse the Contents of \u201cNon-Text\u201d Text Data. Extract the embedded text.\\n\\n![/img/content-blog-raw-blog-web-scraping-with-gazpacho-untitled-3.png](/img/content-blog-raw-blog-web-scraping-with-gazpacho-untitled-3.png)\\n\\n### Using gazpacho and urllib to Retrieve and Download Images, Videos, and Audio Clippings. To download the Image, Audio and Video data.\\n\\n![/img/content-blog-raw-blog-web-scraping-with-gazpacho-untitled-4.png](/img/content-blog-raw-blog-web-scraping-with-gazpacho-untitled-4.png)"},{"id":"/2021/10/01/wellness-tracker-chatbot","metadata":{"permalink":"/ai-kb/blog/2021/10/01/wellness-tracker-chatbot","source":"@site/blog/2021-10-01-wellness-tracker-chatbot.mdx","title":"Wellness tracker chatbot","description":"/img/content-blog-raw-blog-wellness-tracker-chatbot-untitled.png","date":"2021-10-01T00:00:00.000Z","formattedDate":"October 1, 2021","tags":[{"label":"chatbot","permalink":"/ai-kb/blog/tags/chatbot"},{"label":"healthcare","permalink":"/ai-kb/blog/tags/healthcare"},{"label":"nlp","permalink":"/ai-kb/blog/tags/nlp"}],"readingTime":0.455,"truncated":false,"authors":[{"name":"Sparsh Agarwal","title":"Principal Developer","url":"https://github.com/sparsh-ai","imageURL":"https://avatars.githubusercontent.com/u/62965911?v=4","key":"sparsh"}],"frontMatter":{"title":"Wellness tracker chatbot","authors":"sparsh","tags":["chatbot","healthcare","nlp"]},"prevItem":{"title":"Web Scraping with Gazpacho","permalink":"/ai-kb/blog/2021/10/01/web-scraping-with-gazpacho"},"nextItem":{"title":"What is Livestream Ecommerce","permalink":"/ai-kb/blog/2021/10/01/what-is-livestream-ecommerce"}},"content":"![/img/content-blog-raw-blog-wellness-tracker-chatbot-untitled.png](/img/content-blog-raw-blog-wellness-tracker-chatbot-untitled.png)\\n\\n## Problem Statement\\n\\nA bot that logs daily wellness data to a spreadsheet (using the Airtable API), to help the user keep track of their health goals. Connect the assistant to a messaging channel\u2014Twilio\u2014so users can talk to the assistant via text message and Whatsapp.\\n\\n---\\n\\n## Proposed Solution\\n\\n- RASA chatbot with Forms and Custom actions\\n- Connect with Airtable API to log records in table database\\n- Connect with Whatsapp for user interaction\\n\\n---\\n\\n## Modeling\\n\\n![/img/content-blog-raw-blog-wellness-tracker-chatbot-untitled-1.png](/img/content-blog-raw-blog-wellness-tracker-chatbot-untitled-1.png)\\n\\n![/img/content-blog-raw-blog-wellness-tracker-chatbot-untitled-2.png](/img/content-blog-raw-blog-wellness-tracker-chatbot-untitled-2.png)\\n\\n![/img/content-blog-raw-blog-wellness-tracker-chatbot-untitled-3.png](/img/content-blog-raw-blog-wellness-tracker-chatbot-untitled-3.png)\\n\\n![/img/content-blog-raw-blog-wellness-tracker-chatbot-untitled-4.png](/img/content-blog-raw-blog-wellness-tracker-chatbot-untitled-4.png)\\n\\n---\\n\\n## Delivery\\n\\n[https://github.com/sparsh-ai/chatbots/tree/master/wellnessTracker](https://github.com/sparsh-ai/chatbots/tree/master/wellnessTracker)\\n\\n---\\n\\n## Reference\\n\\n[https://www.udemy.com/course/rasa-for-beginners/learn/lecture/20746878#overview](https://www.udemy.com/course/rasa-for-beginners/learn/lecture/20746878#overview)"},{"id":"/2021/10/01/what-is-livestream-ecommerce","metadata":{"permalink":"/ai-kb/blog/2021/10/01/what-is-livestream-ecommerce","source":"@site/blog/2021-10-01-what-is-livestream-ecommerce.mdx","title":"What is Livestream Ecommerce","description":"/img/content-blog-raw-blog-what-is-livestream-ecommerce-untitled.png","date":"2021-10-01T00:00:00.000Z","formattedDate":"October 1, 2021","tags":[{"label":"personalization","permalink":"/ai-kb/blog/tags/personalization"},{"label":"trend","permalink":"/ai-kb/blog/tags/trend"}],"readingTime":3.385,"truncated":false,"authors":[{"name":"Sparsh Agarwal","title":"Principal Developer","url":"https://github.com/sparsh-ai","imageURL":"https://avatars.githubusercontent.com/u/62965911?v=4","key":"sparsh"}],"frontMatter":{"title":"What is Livestream Ecommerce","authors":"sparsh","tags":["personalization","trend"]},"prevItem":{"title":"Wellness tracker chatbot","permalink":"/ai-kb/blog/2021/10/01/wellness-tracker-chatbot"},"nextItem":{"title":"Object detection with YOLO3","permalink":"/ai-kb/blog/2021/01/23/object-detection-with-yolo3"}},"content":"![/img/content-blog-raw-blog-what-is-livestream-ecommerce-untitled.png](/img/content-blog-raw-blog-what-is-livestream-ecommerce-untitled.png)\\n\\nRecent years witness the prosperity of online live streaming. With the development of mobile phones, cameras, and high-speed internet, more and more users are able to broadcast their experiences in live streams on various social platforms, such as Facebook Live and YouTube Live. There are a variety of live streaming applications, including knowledge share, video-gaming, and outdoor traveling.\\n\\nOne of the most important scenarios is live streaming commerce, a new form of online shopping becomes more and more popular, which combines live streaming with E-Commerce activity. The streamers introduce products and interact with their audiences, and hence greatly improve the performance of selling products.\\n\\n![/img/content-blog-raw-blog-what-is-livestream-ecommerce-untitled-1.png](/img/content-blog-raw-blog-what-is-livestream-ecommerce-untitled-1.png)\\n\\n> Livestream ecommerce is a business model in which retailers, influencers, or celebrities sell products and services via online video streaming where the presenter demonstrates and discusses the offering and answers audience questions in real-time.\\n> \\n\\n![/img/content-blog-raw-blog-what-is-livestream-ecommerce-untitled-2.png](/img/content-blog-raw-blog-what-is-livestream-ecommerce-untitled-2.png)\\n\\n### Examples\\n\\n[https://media.nngroup.com/media/editor/2021/02/16/tiktok_livestream_compressed.mp4](https://media.nngroup.com/media/editor/2021/02/16/tiktok_livestream_compressed.mp4)\\n\\n*During a livestream event hosted by Walmart on TikTok, users watched an influencer presenting various products such as a pair of jeans. Those interested in the jeans could tap the product listing shown at the bottom of the screen. They could also browse the list of products promoted during the livestream and purchase them without leaving the TikTok app. Viewers\u2019 real-time comments appeared along the left-hand side of the livestream feed.*\\n\\n### Advantages\\n\\n- Livestreams allow users to see products in detail and get their questions answered in real time\\n- During livestream sessions, the hosts can show product details in close-up (left), give instructions of use for products like essential oils and cosmetic face masks (middle), or even show how a particular product, like the tea they\u2019re selling, is made (right)\\n    \\n    ![/img/content-blog-raw-blog-what-is-livestream-ecommerce-untitled-3.png](/img/content-blog-raw-blog-what-is-livestream-ecommerce-untitled-3.png)\\n    \\n- Greatly shorten the decision-making time of consumers and provoke the sales volume\\n- The expert streamers introduce and promote the products in a live streaming manner, which makes the shopping process more interesting and convincing\\n- Rich and real-time interactions between streamers and their audiences, which makes live streaming a new medium and a powerful marketing tool for E-Commerce\\n- Viewers not only can watch the showing for product\u2019s looks and functions, but also can ask the streamers to show different or individual perspectives of the products in real-time\\n\\n### Market\\n\\nLivestream ecommerce has been surging dramatically in China. According to Forbes, this industry is estimated to earn $60 billion annually. In 2019, about 37 percent of the online shoppers in China (265 million people) made livestream purchases. On Taobao\u2019s 2020 annual Single-Day Global Shopping Festival (November 11th), livestreams accounted for $6 billion in sales (twice the amount from the prior year).\\n\\nAmazon has also launched its live platform, where influencers promote items and chat with potential customers. And Facebook and Instagram are exploring the integration between ecommerce and social media. For instance, the new Shop feature on Instagram allows users to browse products and place orders directly within Instagram \u2014 a form of social commerce.\\n\\nThe total GMV driven by live streaming achieved $6 Billion USD. Some quantitative research results show that adopting live streaming in sales can achieve a 21.8% increase in online sales volume.\\n\\n![/img/content-blog-raw-blog-what-is-livestream-ecommerce-untitled-4.png](/img/content-blog-raw-blog-what-is-livestream-ecommerce-untitled-4.png)\\n\\n### The Anatomy of a Livestream Session\\n\\n![/img/content-blog-raw-blog-what-is-livestream-ecommerce-untitled-5.png](/img/content-blog-raw-blog-what-is-livestream-ecommerce-untitled-5.png)\\n\\nA typical livestream session has the following basic components:\\n\\n1. **The video stream,**\xa0where the host shows the products, talks about them, and answers questions from the audience. In the Amazon Live case, the stream occupies the most of the screen space.\\n2. **The list of products being promoted**, with the product currently being shown highlighted. This list appears at the bottom of the Amazon video stream.\\n3. **A chat area,**\xa0where viewers can type questions and comments to interact with the host and other viewers. The chat area is at the right of the live stream on Amazon Live.\\n4. **A reaction button, that users**\xa0can use to send reactions, displayed as animated emojis. The reaction button shows up as a little star icon at the bottom right of the video stream on Amazon.\\n\\n### References\\n\\n1. [Features of Livestream ecommerce: What We Can Learn from China](https://www.nngroup.com/articles/livestream-ecommerce-china/)\\n2. [Top Live Streaming E-Commerce Startups](https://tracxn.com/d/trending-themes/Startups-in-Live-Streaming-E-Commerce)"},{"id":"/2021/01/23/object-detection-with-yolo3","metadata":{"permalink":"/ai-kb/blog/2021/01/23/object-detection-with-yolo3","source":"@site/blog/2021-01-23-object-detection-with-yolo3.mdx","title":"Object detection with YOLO3","description":"Live app","date":"2021-01-23T00:00:00.000Z","formattedDate":"January 23, 2021","tags":[{"label":"app","permalink":"/ai-kb/blog/tags/app"},{"label":"vision","permalink":"/ai-kb/blog/tags/vision"},{"label":"streamlit","permalink":"/ai-kb/blog/tags/streamlit"}],"readingTime":1.975,"truncated":false,"authors":[{"name":"Sparsh Agarwal","title":"Principal Developer","url":"https://github.com/sparsh-ai","imageURL":"https://avatars.githubusercontent.com/u/62965911?v=4","key":"sparsh"}],"frontMatter":{"title":"Object detection with YOLO3","authors":["sparsh"],"tags":["app","vision","streamlit"]},"prevItem":{"title":"What is Livestream Ecommerce","permalink":"/ai-kb/blog/2021/10/01/what-is-livestream-ecommerce"},"nextItem":{"title":"MobileNet SSD Caffe Pre-trained model","permalink":"/ai-kb/blog/2020/01/19/mobilenet-ssd-caffe-pre-trained-model"}},"content":"## Live app\\n\\nThis app can detect COCO 80-classes using three different models - Caffe MobileNet SSD, Yolo3-tiny, and Yolo3. It can also detect faces using two different models - SSD Res10 and OpenCV face detector.  Yolo3-tiny can also detect fires.\\n\\n![/img/content-blog-raw-blog-object-detection-with-yolo3-untitled.png](/img/content-blog-raw-blog-object-detection-with-yolo3-untitled.png)\\n\\n![/img/content-blog-raw-blog-object-detection-with-yolo3-untitled-1.png](/img/content-blog-raw-blog-object-detection-with-yolo3-untitled-1.png)\\n\\n## Code\\n\\n```python\\nimport streamlit as st\\nimport cv2\\nfrom PIL import Image\\nimport numpy as np\\nimport os\\n\\nfrom tempfile import NamedTemporaryFile\\nfrom tensorflow.keras.preprocessing.image import img_to_array, load_img\\n\\ntemp_file = NamedTemporaryFile(delete=False)\\n\\nDEFAULT_CONFIDENCE_THRESHOLD = 0.5\\nDEMO_IMAGE = \\"test_images/demo.jpg\\"\\nMODEL = \\"model/MobileNetSSD_deploy.caffemodel\\"\\nPROTOTXT = \\"model/MobileNetSSD_deploy.prototxt.txt\\"\\n\\nCLASSES = [\\n    \\"background\\",\\n    \\"aeroplane\\",\\n    \\"bicycle\\",\\n    \\"bird\\",\\n    \\"boat\\",\\n    \\"bottle\\",\\n    \\"bus\\",\\n    \\"car\\",\\n    \\"cat\\",\\n    \\"chair\\",\\n    \\"cow\\",\\n    \\"diningtable\\",\\n    \\"dog\\",\\n    \\"horse\\",\\n    \\"motorbike\\",\\n    \\"person\\",\\n    \\"pottedplant\\",\\n    \\"sheep\\",\\n    \\"sofa\\",\\n    \\"train\\",\\n    \\"tvmonitor\\",\\n]\\nCOLORS = np.random.uniform(0, 255, size=(len(CLASSES), 3))\\n\\n@st.cache\\ndef process_image(image):\\n    blob = cv2.dnn.blobFromImage(\\n        cv2.resize(image, (300, 300)), 0.007843, (300, 300), 127.5\\n    )\\n    net = cv2.dnn.readNetFromCaffe(PROTOTXT, MODEL)\\n    net.setInput(blob)\\n    detections = net.forward()\\n    return detections\\n\\n@st.cache\\ndef annotate_image(\\n    image, detections, confidence_threshold=DEFAULT_CONFIDENCE_THRESHOLD\\n):\\n    # loop over the detections\\n    (h, w) = image.shape[:2]\\n    labels = []\\n    for i in np.arange(0, detections.shape[2]):\\n        confidence = detections[0, 0, i, 2]\\n\\n        if confidence > confidence_threshold:\\n            # extract the index of the class label from the `detections`,\\n            # then compute the (x, y)-coordinates of the bounding box for\\n            # the object\\n            idx = int(detections[0, 0, i, 1])\\n            box = detections[0, 0, i, 3:7] * np.array([w, h, w, h])\\n            (startX, startY, endX, endY) = box.astype(\\"int\\")\\n\\n            # display the prediction\\n            label = f\\"{CLASSES[idx]}: {round(confidence * 100, 2)}%\\"\\n            labels.append(label)\\n            cv2.rectangle(image, (startX, startY), (endX, endY), COLORS[idx], 2)\\n            y = startY - 15 if startY - 15 > 15 else startY + 15\\n            cv2.putText(\\n                image, label, (startX, y), cv2.FONT_HERSHEY_SIMPLEX, 0.5, COLORS[idx], 2\\n            )\\n    return image, labels\\n\\ndef main():\\n  selected_box = st.sidebar.selectbox(\\n    \'Choose one of the following\',\\n    (\'Welcome\', \'Object Detection\')\\n    )\\n    \\n  if selected_box == \'Welcome\':\\n      welcome()\\n  if selected_box == \'Object Detection\':\\n      object_detection() \\n\\ndef welcome():\\n  st.title(\'Object Detection using Streamlit\')\\n  st.subheader(\'A simple app for object detection\')\\n  st.image(\'test_images/demo.jpg\',use_column_width=True)\\n\\ndef object_detection():\\n  \\n  st.title(\\"Object detection with MobileNet SSD\\")\\n\\n  confidence_threshold = st.sidebar.slider(\\n    \\"Confidence threshold\\", 0.0, 1.0, DEFAULT_CONFIDENCE_THRESHOLD, 0.05)\\n\\n  st.sidebar.multiselect(\\"Select object classes to include\\",\\n  options=CLASSES,\\n  default=CLASSES\\n  )\\n\\n  img_file_buffer = st.file_uploader(\\"Upload an image\\", type=[\\"png\\", \\"jpg\\", \\"jpeg\\"])\\n\\n  if img_file_buffer is not None:\\n      temp_file.write(img_file_buffer.getvalue())\\n      image = load_img(temp_file.name)\\n      image = img_to_array(image)\\n      image = image/255.0\\n\\n  else:\\n      demo_image = DEMO_IMAGE\\n      image = np.array(Image.open(demo_image))\\n\\n  detections = process_image(image)\\n  image, labels = annotate_image(image, detections, confidence_threshold)\\n\\n  st.image(\\n      image, caption=f\\"Processed image\\", use_column_width=True,\\n  )\\n\\n  st.write(labels)\\n\\nmain()\\n```\\n\\n*You can play with the live app* [*here](https://share.streamlit.io/sparsh-ai/streamlit-489fbbb7/app.py). Source code is available [here](https://github.com/sparsh-ai/streamlit-5a407279/tree/master) on Github.*"},{"id":"/2020/01/19/mobilenet-ssd-caffe-pre-trained-model","metadata":{"permalink":"/ai-kb/blog/2020/01/19/mobilenet-ssd-caffe-pre-trained-model","source":"@site/blog/2020-01-19-mobilenet-ssd-caffe-pre-trained-model.mdx","title":"MobileNet SSD Caffe Pre-trained model","description":"You can play with the live app here. Souce code is available here on Github.","date":"2020-01-19T00:00:00.000Z","formattedDate":"January 19, 2020","tags":[{"label":"app","permalink":"/ai-kb/blog/tags/app"},{"label":"vision","permalink":"/ai-kb/blog/tags/vision"},{"label":"streamlit","permalink":"/ai-kb/blog/tags/streamlit"}],"readingTime":0.74,"truncated":false,"authors":[{"name":"Sparsh Agarwal","title":"Principal Developer","url":"https://github.com/sparsh-ai","imageURL":"https://avatars.githubusercontent.com/u/62965911?v=4","key":"sparsh"}],"frontMatter":{"title":"MobileNet SSD Caffe Pre-trained model","authors":["sparsh"],"tags":["app","vision","streamlit"]},"prevItem":{"title":"Object detection with YOLO3","permalink":"/ai-kb/blog/2021/01/23/object-detection-with-yolo3"}},"content":"*You can play with the live app [here](https://share.streamlit.io/sparsh-ai/streamlit-5a407279/app.py). Souce code is available* [here](https://github.com/sparsh-ai/streamlit-489fbbb7) *on Github.*\\n\\n## Live app\\n\\n![/img/content-blog-raw-mobilenet-ssd-caffe-pre-trained-model-untitled.png](/img/content-blog-raw-mobilenet-ssd-caffe-pre-trained-model-untitled.png)\\n\\n## Code\\n\\n```python\\n#------------------------------------------------------#\\n# Import libraries\\n#------------------------------------------------------#\\n\\nimport datetime\\nimport urllib\\nimport time\\nimport cv2 as cv\\nimport streamlit as st\\n\\nfrom plugins import Motion_Detection\\nfrom utils import GUI, AppManager, DataManager\\n\\n#------------------------------------------------------#\\n#------------------------------------------------------#\\n\\ndef imageWebApp(guiParam):\\n    \\"\\"\\"\\n    \\"\\"\\"\\n    # Load the image according to the selected option\\n    conf = DataManager(guiParam)\\n    image = conf.load_image_or_video()\\n    \\n    # GUI\\n    switchProcessing = st.button(\'* Start Processing *\')\\n\\n    # Apply the selected plugin on the image\\n    bboxed_frame, output = AppManager(guiParam).process(image, True)\\n\\n    # Display results\\n    st.image(bboxed_frame, channels=\\"BGR\\",  use_column_width=True)\\n\\ndef main():\\n    \\"\\"\\"\\n    \\"\\"\\"\\n    # Get the parameter entered by the user from the GUI\\n    guiParam = GUI().getGuiParameters()\\n\\n    # Check if the application if it is Empty\\n    if guiParam[\'appType\'] == \'Image Applications\':\\n        if guiParam[\\"selectedApp\\"] is not \'Empty\':\\n            imageWebApp(guiParam)\\n\\n    else:\\n        raise st.ScriptRunner.StopException\\n\\n#------------------------------------------------------#\\n#------------------------------------------------------#\\n\\nif __name__ == \\"__main__\\":\\n    main()\\n```"}]}')}}]);