"use strict";(self.webpackChunkdocs=self.webpackChunkdocs||[]).push([[9957],{3905:function(e,t,n){n.d(t,{Zo:function(){return d},kt:function(){return p}});var a=n(67294);function o(e,t,n){return t in e?Object.defineProperty(e,t,{value:n,enumerable:!0,configurable:!0,writable:!0}):e[t]=n,e}function i(e,t){var n=Object.keys(e);if(Object.getOwnPropertySymbols){var a=Object.getOwnPropertySymbols(e);t&&(a=a.filter((function(t){return Object.getOwnPropertyDescriptor(e,t).enumerable}))),n.push.apply(n,a)}return n}function r(e){for(var t=1;t<arguments.length;t++){var n=null!=arguments[t]?arguments[t]:{};t%2?i(Object(n),!0).forEach((function(t){o(e,t,n[t])})):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(n)):i(Object(n)).forEach((function(t){Object.defineProperty(e,t,Object.getOwnPropertyDescriptor(n,t))}))}return e}function l(e,t){if(null==e)return{};var n,a,o=function(e,t){if(null==e)return{};var n,a,o={},i=Object.keys(e);for(a=0;a<i.length;a++)n=i[a],t.indexOf(n)>=0||(o[n]=e[n]);return o}(e,t);if(Object.getOwnPropertySymbols){var i=Object.getOwnPropertySymbols(e);for(a=0;a<i.length;a++)n=i[a],t.indexOf(n)>=0||Object.prototype.propertyIsEnumerable.call(e,n)&&(o[n]=e[n])}return o}var c=a.createContext({}),s=function(e){var t=a.useContext(c),n=t;return e&&(n="function"==typeof e?e(t):r(r({},t),e)),n},d=function(e){var t=s(e.components);return a.createElement(c.Provider,{value:t},e.children)},g={inlineCode:"code",wrapper:function(e){var t=e.children;return a.createElement(a.Fragment,{},t)}},m=a.forwardRef((function(e,t){var n=e.components,o=e.mdxType,i=e.originalType,c=e.parentName,d=l(e,["components","mdxType","originalType","parentName"]),m=s(n),p=o,f=m["".concat(c,".").concat(p)]||m[p]||g[p]||i;return n?a.createElement(f,r(r({ref:t},d),{},{components:n})):a.createElement(f,r({ref:t},d))}));function p(e,t){var n=arguments,o=t&&t.mdxType;if("string"==typeof e||o){var i=n.length,r=new Array(i);r[0]=m;var l={};for(var c in t)hasOwnProperty.call(t,c)&&(l[c]=t[c]);l.originalType=e,l.mdxType="string"==typeof e?e:o,r[1]=l;for(var s=2;s<i;s++)r[s]=n[s];return a.createElement.apply(null,r)}return a.createElement.apply(null,n)}m.displayName="MDXCreateElement"},35698:function(e,t,n){n.r(t),n.d(t,{assets:function(){return d},contentTitle:function(){return c},default:function(){return p},frontMatter:function(){return l},metadata:function(){return s},toc:function(){return g}});var a=n(87462),o=n(63366),i=(n(67294),n(3905)),r=["components"],l={title:"Detectron 2",authors:"sparsh",tags:["tool","vision"]},c=void 0,s={permalink:"/ai-kb/blog/2021/10/01/detectron-2",source:"@site/blog/2021-10-01-detectron-2.mdx",title:"Detectron 2",description:"/img/content-blog-raw-blog-detectron-2-untitled.png",date:"2021-10-01T00:00:00.000Z",formattedDate:"October 1, 2021",tags:[{label:"tool",permalink:"/ai-kb/blog/tags/tool"},{label:"vision",permalink:"/ai-kb/blog/tags/vision"}],readingTime:6.13,truncated:!1,authors:[{name:"Sparsh Agarwal",title:"Principal Developer",url:"https://github.com/sparsh-ai",imageURL:"https://avatars.githubusercontent.com/u/62965911?v=4",key:"sparsh"}],frontMatter:{title:"Detectron 2",authors:"sparsh",tags:["tool","vision"]},prevItem:{title:"Clinical Decision Making",permalink:"/ai-kb/blog/2021/10/01/clinical-decision-making"},nextItem:{title:"Distributed Training of Recommender Systems",permalink:"/ai-kb/blog/2021/10/01/distributed-training-of-recommender-systems"}},d={authorsImageUrls:[void 0]},g=[{value:"Load the data",id:"load-the-data",level:3},{value:"Convert dataset into Detectron2&#39;s standard format",id:"convert-dataset-into-detectron2s-standard-format",level:3},{value:"Model configuration and training",id:"model-configuration-and-training",level:3},{value:"Inference and Visualization",id:"inference-and-visualization",level:3},{value:"Load the data",id:"load-the-data-1",level:3},{value:"Convert dataset into Detectron2&#39;s standard format",id:"convert-dataset-into-detectron2s-standard-format-1",level:3},{value:"Model configuration and training",id:"model-configuration-and-training-1",level:3},{value:"Inference and Visualization",id:"inference-and-visualization-1",level:3},{value:"Real-time Webcam inference",id:"real-time-webcam-inference",level:3},{value:"Behind the scenes",id:"behind-the-scenes",level:3},{value:"References",id:"references",level:3}],m={toc:g};function p(e){var t=e.components,l=(0,o.Z)(e,r);return(0,i.kt)("wrapper",(0,a.Z)({},m,l,{components:t,mdxType:"MDXLayout"}),(0,i.kt)("p",null,(0,i.kt)("img",{loading:"lazy",alt:"/img/content-blog-raw-blog-detectron-2-untitled.png",src:n(67715).Z,width:"1347",height:"901"})),(0,i.kt)("h1",{id:"introduction"},"Introduction"),(0,i.kt)("p",null,"Detectron 2 is a next-generation open-source object detection system from Facebook AI Research. With the repo you can use and train the various state-of-the-art models for detection tasks such as bounding-box detection, instance and semantic segmentation, and person keypoint detection."),(0,i.kt)("p",null,"The following is the directory tree of detectron 2:"),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre"},"detectron2\n\u251c\u2500checkpoint  <- checkpointer and model catalog handlers\n\u251c\u2500config      <- default configs and handlers\n\u251c\u2500data        <- dataset handlers and data loaders\n\u251c\u2500engine      <- predictor and trainer engines\n\u251c\u2500evaluation  <- evaluator for each dataset\n\u251c\u2500export      <- converter of detectron2 models to caffe2 (ONNX)\n\u251c\u2500layers      <- custom layers e.g. deformable conv.\n\u251c\u2500model_zoo   <- pre-trained model links and handler\n\u251c\u2500modeling   \n\u2502  \u251c\u2500meta_arch <- meta architecture e.g. R-CNN, RetinaNet\n\u2502  \u251c\u2500backbone  <- backbone network e.g. ResNet, FPN\n\u2502  \u251c\u2500proposal_generator <- region proposal network\n\u2502  \u2514\u2500roi_heads <- head networks for pooled ROIs e.g. box, mask heads\n\u251c\u2500solver       <- optimizer and scheduler builders\n\u251c\u2500structures   <- structure classes e.g. Boxes, Instances, etc\n\u2514\u2500utils        <- utility modules e.g. visualizer, logger, etc\n")),(0,i.kt)("h1",{id:"installation"},"Installation"),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-python"},"%%time\n!pip install -U torch==1.4+cu100 torchvision==0.5+cu100 -f https://download.pytorch.org/whl/torch_stable.html;\n!pip install cython pyyaml==5.1;\n!pip install -U 'git+https://github.com/cocodataset/cocoapi.git#subdirectory=PythonAPI';\n!pip install detectron2 -f https://dl.fbaipublicfiles.com/detectron2/wheels/cu100/index.html;\n\nfrom detectron2 import model_zoo\nfrom detectron2.engine import DefaultPredictor\nfrom detectron2.config import get_cfg\nfrom detectron2.utils.visualizer import Visualizer\nfrom detectron2.data import MetadataCatalog\n")),(0,i.kt)("h1",{id:"inference-on-pre-trained-models"},"Inference on pre-trained models"),(0,i.kt)("p",null,(0,i.kt)("img",{loading:"lazy",alt:"Original image",src:n(79533).Z,width:"620",height:"479"})),(0,i.kt)("p",null,"Original image"),(0,i.kt)("p",null,(0,i.kt)("img",{loading:"lazy",alt:"Object detection with Faster-RCNN-101",src:n(28160).Z,width:"744",height:"574"})),(0,i.kt)("p",null,"Object detection with Faster-RCNN-101"),(0,i.kt)("p",null,(0,i.kt)("img",{loading:"lazy",alt:"Instance segmentation with Mask-RCNN-50",src:n(38472).Z,width:"744",height:"574"})),(0,i.kt)("p",null,"Instance segmentation with Mask-RCNN-50"),(0,i.kt)("p",null,(0,i.kt)("img",{loading:"lazy",alt:"Keypoint estimation with Keypoint-RCNN-50",src:n(76785).Z,width:"744",height:"574"})),(0,i.kt)("p",null,"Keypoint estimation with Keypoint-RCNN-50"),(0,i.kt)("p",null,(0,i.kt)("img",{loading:"lazy",alt:"Panoptic segmentation with Panoptic-FPN-101",src:n(31623).Z,width:"744",height:"574"})),(0,i.kt)("p",null,"Panoptic segmentation with Panoptic-FPN-101"),(0,i.kt)("p",null,(0,i.kt)("img",{loading:"lazy",alt:"Default Mask R-CNN (top) vs. Mask R-CNN with PointRend (bottom) comparison",src:n(47761).Z,width:"744",height:"1148"})),(0,i.kt)("p",null,"Default Mask R-CNN (top) vs. Mask R-CNN with PointRend (bottom) comparison"),(0,i.kt)("h1",{id:"fine-tuning-balloons-dataset"},"Fine-tuning Balloons Dataset"),(0,i.kt)("h3",{id:"load-the-data"},"Load the data"),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre"},"# download, decompress the data\n!wget https://github.com/matterport/Mask_RCNN/releases/download/v2.1/balloon_dataset.zip\n!unzip balloon_dataset.zip > /dev/null\n")),(0,i.kt)("h3",{id:"convert-dataset-into-detectron2s-standard-format"},"Convert dataset into Detectron2's standard format"),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre"},'from detectron2.structures import BoxMode\n# write a function that loads the dataset into detectron2\'s standard format\ndef get_balloon_dicts(img_dir):\n    json_file = os.path.join(img_dir, "via_region_data.json")\n    with open(json_file) as f:\n        imgs_anns = json.load(f)\n\n    dataset_dicts = []\n    for _, v in imgs_anns.items():\n        record = {}\n        \n        filename = os.path.join(img_dir, v["filename"])\n        height, width = cv2.imread(filename).shape[:2]\n        \n        record["file_name"] = filename\n        record["height"] = height\n        record["width"] = width\n      \n        annos = v["regions"]\n        objs = []\n        for _, anno in annos.items():\n            assert not anno["region_attributes"]\n            anno = anno["shape_attributes"]\n            px = anno["all_points_x"]\n            py = anno["all_points_y"]\n            poly = [(x + 0.5, y + 0.5) for x, y in zip(px, py)]\n            poly = list(itertools.chain.from_iterable(poly))\n\n            obj = {\n                "bbox": [np.min(px), np.min(py), np.max(px), np.max(py)],\n                "bbox_mode": BoxMode.XYXY_ABS,\n                "segmentation": [poly],\n                "category_id": 0,\n                "iscrowd": 0\n            }\n            objs.append(obj)\n        record["annotations"] = objs\n        dataset_dicts.append(record)\n    return dataset_dicts\n\nfrom detectron2.data import DatasetCatalog, MetadataCatalog\nfor d in ["train", "val"]:\n    DatasetCatalog.register("balloon/" + d, lambda d=d: get_balloon_dicts("balloon/" + d))\n    MetadataCatalog.get("balloon/" + d).set(thing_classes=["balloon"])\nballoon_metadata = MetadataCatalog.get("balloon/train")\n')),(0,i.kt)("h3",{id:"model-configuration-and-training"},"Model configuration and training"),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre"},'from detectron2.engine import DefaultTrainer\nfrom detectron2.config import get_cfg\n\ncfg = get_cfg()\ncfg.merge_from_file(model_zoo.get_config_file("COCO-InstanceSegmentation/mask_rcnn_R_50_FPN_3x.yaml"))\ncfg.DATASETS.TRAIN = ("balloon/train",)\ncfg.DATASETS.TEST = ()   # no metrics implemented for this dataset\ncfg.DATALOADER.NUM_WORKERS = 2\ncfg.MODEL.WEIGHTS = model_zoo.get_checkpoint_url("COCO-InstanceSegmentation/mask_rcnn_R_50_FPN_3x.yaml")\ncfg.SOLVER.IMS_PER_BATCH = 2\ncfg.SOLVER.BASE_LR = 0.00025\ncfg.SOLVER.MAX_ITER = 300    # 300 iterations seems good enough, but you can certainly train longer\ncfg.MODEL.ROI_HEADS.BATCH_SIZE_PER_IMAGE = 128   # faster, and good enough for this toy dataset\ncfg.MODEL.ROI_HEADS.NUM_CLASSES = 1  # only has one class (ballon)\n\nos.makedirs(cfg.OUTPUT_DIR, exist_ok=True)\ntrainer = DefaultTrainer(cfg) \ntrainer.resume_or_load(resume=False)\ntrainer.train()\n')),(0,i.kt)("h3",{id:"inference-and-visualization"},"Inference and Visualization"),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre"},'from detectron2.utils.visualizer import ColorMode\n\n# load weights\ncfg.MODEL.WEIGHTS = os.path.join(cfg.OUTPUT_DIR, "model_final.pth")\ncfg.MODEL.ROI_HEADS.SCORE_THRESH_TEST = 0.7   # set the testing threshold for this model\n# Set training data-set path\ncfg.DATASETS.TEST = ("balloon/val", )\n# Create predictor (model for inference)\npredictor = DefaultPredictor(cfg)\n\ndataset_dicts = get_balloon_dicts("balloon/val")\nfor d in random.sample(dataset_dicts, 3):    \n    im = cv2.imread(d["file_name"])\n    outputs = predictor(im)\n    v = Visualizer(im[:, :, ::-1],\n                   metadata=balloon_metadata, \n                   scale=0.8, \n                   instance_mode=ColorMode.IMAGE_BW   # remove the colors of unsegmented pixels\n    )\n    v = v.draw_instance_predictions(outputs["instances"].to("cpu"))\n    cv2_imshow(v.get_image()[:, :, ::-1])\n')),(0,i.kt)("p",null,(0,i.kt)("img",{loading:"lazy",alt:"/img/content-blog-raw-blog-detectron-2-untitled-7.png",src:n(93884).Z,width:"819",height:"540"})),(0,i.kt)("p",null,(0,i.kt)("img",{loading:"lazy",alt:"/img/content-blog-raw-blog-detectron-2-untitled-8.png",src:n(1955).Z,width:"819",height:"614"})),(0,i.kt)("p",null,(0,i.kt)("img",{loading:"lazy",alt:"/img/content-blog-raw-blog-detectron-2-untitled-9.png",src:n(79429).Z,width:"548",height:"819"})),(0,i.kt)("h1",{id:"fine-tuning-chip-dataset"},"Fine-tuning Chip Dataset"),(0,i.kt)("h3",{id:"load-the-data-1"},"Load the data"),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre"},"#get the dataset\n!pip install -q kaggle\n!pip install -q kaggle-cli\nos.environ['KAGGLE_USERNAME'] = \"sparshag\" \nos.environ['KAGGLE_KEY'] = \"1b1f894d1fa6febe9676681b44ad807b\"\n!kaggle datasets download -d tannergi/microcontroller-detection\n!unzip microcontroller-detection.zip\n")),(0,i.kt)("h3",{id:"convert-dataset-into-detectron2s-standard-format-1"},"Convert dataset into Detectron2's standard format"),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre"},"# Registering the dataset\nfrom detectron2.structures import BoxMode\ndef get_microcontroller_dicts(csv_file, img_dir):\n    df = pd.read_csv(csv_file)\n    df['filename'] = df['filename'].map(lambda x: img_dir+x)\n\n    classes = ['Raspberry_Pi_3', 'Arduino_Nano', 'ESP8266', 'Heltec_ESP32_Lora']\n\n    df['class_int'] = df['class'].map(lambda x: classes.index(x))\n\n    dataset_dicts = []\n    for filename in df['filename'].unique().tolist():\n        record = {}\n        \n        height, width = cv2.imread(filename).shape[:2]\n        \n        record[\"file_name\"] = filename\n        record[\"height\"] = height\n        record[\"width\"] = width\n\n        objs = []\n        for index, row in df[(df['filename']==filename)].iterrows():\n          obj= {\n              'bbox': [row['xmin'], row['ymin'], row['xmax'], row['ymax']],\n              'bbox_mode': BoxMode.XYXY_ABS,\n              'category_id': row['class_int'],\n              \"iscrowd\": 0\n          }\n          objs.append(obj)\n        record[\"annotations\"] = objs\n        dataset_dicts.append(record)\n    return dataset_dicts\n\nclasses = ['Raspberry_Pi_3', 'Arduino_Nano', 'ESP8266', 'Heltec_ESP32_Lora']\nfor d in [\"train\", \"test\"]:\n  DatasetCatalog.register('microcontroller/' + d, lambda d=d: get_microcontroller_dicts('Microcontroller Detection/' + d + '_labels.csv', 'Microcontroller Detection/' + d+'/'))\n  MetadataCatalog.get('microcontroller/' + d).set(thing_classes=classes)\nmicrocontroller_metadata = MetadataCatalog.get('microcontroller/train')\n")),(0,i.kt)("h3",{id:"model-configuration-and-training-1"},"Model configuration and training"),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre"},'# Train the model\ncfg = get_cfg()\ncfg.merge_from_file(model_zoo.get_config_file("COCO-Detection/faster_rcnn_R_101_FPN_3x.yaml"))\ncfg.DATASETS.TRAIN = (\'microcontroller/train\',)\ncfg.DATASETS.TEST = ()   # no metrics implemented for this dataset\ncfg.DATALOADER.NUM_WORKERS = 2\ncfg.MODEL.WEIGHTS = model_zoo.get_checkpoint_url("COCO-Detection/faster_rcnn_R_101_FPN_3x.yaml")\ncfg.SOLVER.IMS_PER_BATCH = 2\ncfg.SOLVER.MAX_ITER = 1000\ncfg.MODEL.ROI_HEADS.NUM_CLASSES = 4\n\nos.makedirs(cfg.OUTPUT_DIR, exist_ok=True)\ntrainer = DefaultTrainer(cfg) \ntrainer.resume_or_load(resume=False)\ntrainer.train()\n')),(0,i.kt)("p",null,(0,i.kt)("img",{loading:"lazy",alt:"/img/content-blog-raw-blog-detectron-2-untitled-10.png",src:n(20419).Z,width:"765",height:"578"})),(0,i.kt)("p",null,(0,i.kt)("img",{loading:"lazy",alt:"/img/content-blog-raw-blog-detectron-2-untitled-11.png",src:n(24164).Z,width:"640",height:"480"})),(0,i.kt)("h3",{id:"inference-and-visualization-1"},"Inference and Visualization"),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre"},'cfg.MODEL.WEIGHTS = os.path.join(cfg.OUTPUT_DIR, "model_final.pth")\ncfg.MODEL.ROI_HEADS.SCORE_THRESH_TEST = 0.8   # set the testing threshold for this model\ncfg.DATASETS.TEST = (\'microcontroller/test\', )\npredictor = DefaultPredictor(cfg)\n\ndf_test = pd.read_csv(\'Microcontroller Detection/test_labels.csv\')\n\ndataset_dicts = DatasetCatalog.get(\'microcontroller/test\')\nfor d in random.sample(dataset_dicts, 3):    \n    im = cv2.imread(d["file_name"])\n    outputs = predictor(im)\n    v = Visualizer(im[:, :, ::-1], \n                   metadata=microcontroller_metadata, \n                   scale=0.8\n                   )\n    v = v.draw_instance_predictions(outputs["instances"].to("cpu"))\n    cv2_imshow(v.get_image()[:, :, ::-1])\n')),(0,i.kt)("h3",{id:"real-time-webcam-inference"},"Real-time Webcam inference"),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre"},"from IPython.display import display, Javascript\nfrom google.colab.output import eval_js\nfrom base64 import b64decode\n\ndef take_photo(filename='photo.jpg', quality=0.8):\n  js = Javascript('''\n    async function takePhoto(quality) {\n      const div = document.createElement('div');\n      const capture = document.createElement('button');\n      capture.textContent = 'Capture';\n      div.appendChild(capture);\n\n      const video = document.createElement('video');\n      video.style.display = 'block';\n      const stream = await navigator.mediaDevices.getUserMedia({video: true});\n\n      document.body.appendChild(div);\n      div.appendChild(video);\n      video.srcObject = stream;\n      await video.play();\n\n      // Resize the output to fit the video element.\n      google.colab.output.setIframeHeight(document.documentElement.scrollHeight, true);\n\n      // Wait for Capture to be clicked.\n      await new Promise((resolve) => capture.onclick = resolve);\n\n      const canvas = document.createElement('canvas');\n      canvas.width = video.videoWidth;\n      canvas.height = video.videoHeight;\n      canvas.getContext('2d').drawImage(video, 0, 0);\n      stream.getVideoTracks()[0].stop();\n      div.remove();\n      return canvas.toDataURL('image/jpeg', quality);\n    }\n    ''')\n  display(js)\n  data = eval_js('takePhoto({})'.format(quality))\n  binary = b64decode(data.split(',')[1])\n  with open(filename, 'wb') as f:\n    f.write(binary)\n  return filename\n\nfrom IPython.display import Image\ntry:\n  filename = take_photo()\n  print('Saved to {}'.format(filename))\n  \n  # Show the image which was just taken.\n  display(Image(filename))\nexcept Exception as err:\n  # Errors will be thrown if the user does not have a webcam or if they do not\n  # grant the page permission to access it.\n  print(str(err))\n")),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre"},'model_path = \'/content/output/model_final.pth\'\nconfig_path= model_zoo.get_config_file("COCO-Detection/faster_rcnn_R_101_FPN_3x.yaml")\n\n# Create config\ncfg = get_cfg()\ncfg.merge_from_file(config_path)\ncfg.MODEL.ROI_HEADS.SCORE_THRESH_TEST = 0.1\ncfg.MODEL.WEIGHTS = model_path\n\npredictor = DefaultPredictor(cfg)\n\nim = cv2.imread(\'photo.jpg\')\noutputs = predictor(im)\n\nv = Visualizer(im[:, :, ::-1], MetadataCatalog.get(cfg.DATASETS.TRAIN[0]), scale=1.2)\nv = v.draw_instance_predictions(outputs["instances"].to("cpu"))\ncv2_imshow(v.get_image()[:, :, ::-1])\n')),(0,i.kt)("h1",{id:"fine-tuning-on-face-dataset"},"Fine-tuning on Face dataset"),(0,i.kt)("p",null,"The process is same. Here is the output."),(0,i.kt)("p",null,(0,i.kt)("img",{loading:"lazy",alt:"/img/content-blog-raw-blog-detectron-2-untitled-12.png",src:n(20263).Z,width:"1353",height:"550"})),(0,i.kt)("p",null,(0,i.kt)("img",{loading:"lazy",alt:"/img/content-blog-raw-blog-detectron-2-untitled-13.png",src:n(66317).Z,width:"700",height:"400"})),(0,i.kt)("p",null,(0,i.kt)("img",{loading:"lazy",alt:"/img/content-blog-raw-blog-detectron-2-untitled-14.png",src:n(78347).Z,width:"700",height:"400"})),(0,i.kt)("h3",{id:"behind-the-scenes"},"Behind the scenes"),(0,i.kt)("p",null,(0,i.kt)("img",{loading:"lazy",alt:"/img/content-blog-raw-blog-detectron-2-untitled-15.png",src:n(51942).Z,width:"1071",height:"563"})),(0,i.kt)("h3",{id:"references"},"References"),(0,i.kt)("ul",null,(0,i.kt)("li",{parentName:"ul"},(0,i.kt)("a",{parentName:"li",href:"https://medium.com/deepvisionguru/how-to-embed-detectron2-in-your-computer-vision-project-817f29149461"},"How to embed Detectron2 in your computer vision project - blogpost")),(0,i.kt)("li",{parentName:"ul"},(0,i.kt)("a",{parentName:"li",href:"https://gilberttanner.com/blog/detectron2-train-a-instance-segmentation-model"},"Detectron2 Train a Instance Segmentation Model by Gilbert Tanner")),(0,i.kt)("li",{parentName:"ul"},(0,i.kt)("a",{parentName:"li",href:"https://www.dlology.com/blog/how-to-train-detectron2-with-custom-coco-datasets/"},"How to train Detectron2 with Custom COCO Datasets - DLology")),(0,i.kt)("li",{parentName:"ul"},(0,i.kt)("a",{parentName:"li",href:"https://towardsdatascience.com/character-recognition-and-segmentation-for-custom-data-using-detectron2-599de82b393c"},"Character Recognition and Segmentation For Custom Data Using Detectron2 - blogpost")),(0,i.kt)("li",{parentName:"ul"},(0,i.kt)("a",{parentName:"li",href:"https://www.celantur.com/blog/panoptic-segmentation-in-detectron2/"},"Training models with Panoptic Segmentation in Detectron2")),(0,i.kt)("li",{parentName:"ul"},(0,i.kt)("a",{parentName:"li",href:"https://www.kaggle.com/lewisgmorris/image-segmentation-using-detectron2"},"Image segmentation using Detectron2 - Kaggle")),(0,i.kt)("li",{parentName:"ul"},(0,i.kt)("a",{parentName:"li",href:"https://towardsdatascience.com/a-beginners-guide-to-object-detection-and-computer-vision-with-facebook-s-detectron2-700b6273390e"},"A Beginner\u2019s Guide To Object Detection And Computer Vision With Facebook\u2019s Detectron2")),(0,i.kt)("li",{parentName:"ul"},(0,i.kt)("a",{parentName:"li",href:"https://www.curiousily.com/posts/face-detection-on-custom-dataset-with-detectron2-in-python/"},"Face Detection on Custom Dataset with Detectron2 and PyTorch using Python")),(0,i.kt)("li",{parentName:"ul"},(0,i.kt)("a",{parentName:"li",href:"https://www.notion.so/Detectron-2-d31ac9c14a8d4d9888882df14a4e0eee"},"My Experiment Notion")),(0,i.kt)("li",{parentName:"ul"},(0,i.kt)("a",{parentName:"li",href:"https://colab.research.google.com/drive/16jcaJoc6bCFAQ96jDe2HwtXj7BMD_-m5"},"Official Colab")),(0,i.kt)("li",{parentName:"ul"},(0,i.kt)("a",{parentName:"li",href:"https://research.fb.com/wp-content/uploads/2019/12/4.-detectron2.pdf"},"Official Slide")),(0,i.kt)("li",{parentName:"ul"},(0,i.kt)("a",{parentName:"li",href:"https://github.com/facebookresearch/detectron2"},"Official Git"))))}p.isMDXComponent=!0},79533:function(e,t,n){t.Z=n.p+"assets/images/content-blog-raw-blog-detectron-2-untitled-1-f67813931f498e5451c89d34fc53c18a.png"},20419:function(e,t,n){t.Z=n.p+"assets/images/content-blog-raw-blog-detectron-2-untitled-10-8d5237190905a0e4fadc0b0ed691aef3.png"},24164:function(e,t,n){t.Z=n.p+"assets/images/content-blog-raw-blog-detectron-2-untitled-11-4da902dcc9cb45b630b4918e12ffd40b.png"},20263:function(e,t,n){t.Z=n.p+"assets/images/content-blog-raw-blog-detectron-2-untitled-12-8624038661cf0ab9a13ffd35bcea2092.png"},66317:function(e,t,n){t.Z=n.p+"assets/images/content-blog-raw-blog-detectron-2-untitled-13-9ef0b07eeb23c36fea216ff6d8776168.png"},78347:function(e,t,n){t.Z=n.p+"assets/images/content-blog-raw-blog-detectron-2-untitled-14-4eb13d5f04434cff475a78207b96071c.png"},51942:function(e,t,n){t.Z=n.p+"assets/images/content-blog-raw-blog-detectron-2-untitled-15-3bcd41db9aeb9c74ee059c1848fbc157.png"},28160:function(e,t,n){t.Z=n.p+"assets/images/content-blog-raw-blog-detectron-2-untitled-2-2d53fa263a2f78ba53e25191a5174f6a.png"},38472:function(e,t,n){t.Z=n.p+"assets/images/content-blog-raw-blog-detectron-2-untitled-3-843f187808ce30f5f39ee16632bcc07e.png"},76785:function(e,t,n){t.Z=n.p+"assets/images/content-blog-raw-blog-detectron-2-untitled-4-dc55d3ff169a8b338c25147d005a7357.png"},31623:function(e,t,n){t.Z=n.p+"assets/images/content-blog-raw-blog-detectron-2-untitled-5-400fd835e75e14ba61644847b378b47f.png"},47761:function(e,t,n){t.Z=n.p+"assets/images/content-blog-raw-blog-detectron-2-untitled-6-9497a38e9bed29af766a327244331cee.png"},93884:function(e,t,n){t.Z=n.p+"assets/images/content-blog-raw-blog-detectron-2-untitled-7-2e7f9a8cba8cc49ee8ab648cf18bc33b.png"},1955:function(e,t,n){t.Z=n.p+"assets/images/content-blog-raw-blog-detectron-2-untitled-8-9d5d0a41fa5a618be93561523b131930.png"},79429:function(e,t,n){t.Z=n.p+"assets/images/content-blog-raw-blog-detectron-2-untitled-9-67a9478fd72c9f64dc0f058a5395edb8.png"},67715:function(e,t,n){t.Z=n.p+"assets/images/content-blog-raw-blog-detectron-2-untitled-1b615c35cb13ff3ee0b6a399bde1a3c7.png"}}]);