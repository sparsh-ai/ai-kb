{"cells": [{"cell_type": "markdown", "metadata": {}, "source": ["# Question Answering\n", "\n", "In this notebook, we'll deploy and use a question answering model\n", "from the [transformers](https://huggingface.co/transformers/) library\n", "which uses PyTorch.\n", "\n", "Question Answering is useful when you want to query a large amount of\n", "text for specific information. Maybe you're interested in extracting the\n", "date a certain event happened. You can construct a question (or query) in\n", "natural language to retrive this information: e.g. 'When did Company X\n", "release Product Y?\". Similar to extractive summarization we saw in the\n", "last notebook, Question Answering will return a verbatim slice of the\n", "text as the answer. It won't generate new words to answer the question.\n", "\n", "**Note**: When running this notebook on SageMaker Studio, you should make\n", "sure the 'SageMaker JumpStart PyTorch 1.0' image/kernel is used. When\n", "running this notebook on SageMaker Notebook Instance, you should make\n", "sure the 'sagemaker-soln' kernel is used."]}, {"cell_type": "markdown", "metadata": {}, "source": ["This solution relies on a config file to run the provisioned AWS resources. Run the cell below to generate that file."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["import boto3\n", "import os\n", "import json\n", "\n", "client = boto3.client('servicecatalog')\n", "cwd = os.getcwd().split('/')\n", "i= cwd.index('S3Downloads')\n", "pp_name = cwd[i + 1]\n", "pp = client.describe_provisioned_product(Name=pp_name)\n", "record_id = pp['ProvisionedProductDetail']['LastSuccessfulProvisioningRecordId']\n", "record = client.describe_record(Id=record_id)\n", "\n", "keys = [ x['OutputKey'] for x in record['RecordOutputs'] if 'OutputKey' and 'OutputValue' in x]\n", "values = [ x['OutputValue'] for x in record['RecordOutputs'] if 'OutputKey' and 'OutputValue' in x]\n", "stack_output = dict(zip(keys, values))\n", "\n", "with open(f'/root/S3Downloads/{pp_name}/stack_outputs.json', 'w') as f:\n", "    json.dump(stack_output, f)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["We start by importing a variety of packages that will be used throughout\n", "the notebook. One of the most important packages is the Amazon SageMaker\n", "Python SDK (i.e. `import sagemaker`). We also import modules from our own\n", "custom (and editable) package that can be found at `../package`."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["import boto3\n", "import sagemaker\n", "from sagemaker.pytorch import PyTorchModel\n", "from sagemaker.local import LocalSession\n", "import sys\n", "\n", "sys.path.insert(0, '../package')\n", "from package import config, utils"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Up next, we define the current folder and create a SageMaker client (from\n", "`boto3`). We can use the SageMaker client to call SageMaker APIs\n", "directly, as an alternative to using the Amazon SageMaker SDK. We'll use\n", "it at the end of the notebook to delete certain resources that are\n", "created in this notebook."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["current_folder = utils.get_current_folder(globals())\n", "sagemaker_client = boto3.client('sagemaker')"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Our question answering system needs a machine learning model. In this\n", "section, we'll deploy a model to an Amazon SageMaker Endpoint and then\n", "invoke the endpoint from the notebook. We'll use a pre-trained model from\n", "the [transformers](https://huggingface.co/transformers/) library instead\n", "of training a model from scratch, specifically the BERT Large model that\n", "has been pre-trained on the SQuAD dataset.\n", "\n", "We'll use the unique solution prefix to name the model and endpoint."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["model_name = \"{}-question-answering\".format(config.SOLUTION_PREFIX)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Up next, we need to define the Amazon SageMaker Model which references\n", "the source code and the specifies which container to use. Our pre-trained\n", "model is from the transformers library which uses PyTorch. As a result,\n", "we should use the PyTorchModel from the Amazon SageMaker Python SDK.\n", "Using PyTorchModel and setting the framework_version argument, means that\n", "our deployed model will run inside a container that has PyTorch\n", "pre-installed. Other requirements can be installed by defining a\n", "requirements.txt file at the specified source_dir location. We use the\n", "entry_point argument to reference the code (within source_dir) that\n", "should be run for model inference: functions called model_fn, input_fn,\n", "predict_fn and output_fn are expected to be defined. And lastly, you can\n", "pass `model_data` from a training job, but we are going to load the\n", "pre-trained model in the source code running on the endpoint. We still\n", "need to provide `model_data`, so we pass an empty archive."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["model = PyTorchModel(\n", "    name=model_name,\n", "    model_data=f'{config.SOURCE_S3_PATH}/models/empty.tar.gz',\n", "    entry_point='entry_point.py',\n", "    source_dir='../containers/question_answering',\n", "    role=config.IAM_ROLE,\n", "    framework_version='1.5.0',\n", "    py_version='py3',\n", "    code_location='s3://' + config.S3_BUCKET + '/code',\n", "    env={\n", "        'MODEL_ASSETS_S3_BUCKET': config.SOURCE_S3_BUCKET,\n", "        'MODEL_ASSETS_S3_PREFIX': f\"{config.SOURCE_S3_PREFIX}/models/question_answering/\"\n", "    }\n", ")"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Using this Amazon SageMaker Model, we can deploy a HTTPS endpoint on a\n", "dedicated instance. We choose to deploy the endpoint on a single\n", "ml.p3.2xlarge instance (or ml.g4dn.2xlarge if unavailable in this\n", "region). Our question answering model is transfomer that\n", "benefits from GPU optimization, and a ml.p3.2xlarge has a high\n", "performance NVIDIA V100 GPU that can reduce inference latency on each\n", "request. You can expect this deployment step to take around 5 minutes.\n", "After approximately 15 dashes, you can expect to see an exclamation mark\n", "which indicates a successful deployment."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["from sagemaker.serializers import JSONSerializer\n", "from sagemaker.deserializers import JSONDeserializer\n", "\n", "predictor = model.deploy(\n", "    endpoint_name=model_name,\n", "    instance_type=config.HOSTING_INSTANCE_TYPE,\n", "    initial_instance_count=1,\n", "    serializer=JSONSerializer(),\n", "    deserializer=JSONDeserializer()\n", ")"]}, {"cell_type": "markdown", "metadata": {}, "source": ["When you're trying to update the model for development purposes, but\n", "experiencing issues because the model/endpoint-config/endpoint already\n", "exists, you can delete the existing model/endpoint-config/endpoint by\n", "uncommenting and running the following commands:"]}, {"cell_type": "code", "execution_count": null, "metadata": {"lines_to_next_cell": 2}, "outputs": [], "source": ["# sagemaker_client.delete_endpoint(EndpointName=model_name)\n", "# sagemaker_client.delete_endpoint_config(EndpointConfigName=model_name)\n", "# sagemaker_client.delete_model(ModelName=model_name)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["When calling our new endpoint from the notebook, we use a Amazon\n", "SageMaker SDK\n", "[`Predictor`](https://sagemaker.readthedocs.io/en/stable/predictors.html).\n", "A `Predictor` is used to send data to an endpoint (as part of a request),\n", "and interpret the response. Our `model.deploy` command returned a\n", "`Predictor` but, by default, it will send and receive numpy arrays. Our\n", "endpoint expects to receive (and also sends) JSON formatted objects, so\n", "we modify the `Predictor` to use JSON instead of the PyTorch endpoint\n", "default of numpy arrays. JSON is used here because it is a standard\n", "endpoint format and the endpoint response can contain nested data\n", "structures."]}, {"cell_type": "markdown", "metadata": {}, "source": ["With our model successfully deployed and our predictor configured, we can\n", "try out the question answering model out on example inputs. All we need\n", "to do is construct a dictionary object with two keys. `context` is the\n", "text that we wish to retrieve information from. `question` is the natural\n", "language query which specifices what information we're interested in\n", "extracting. We call `predict` on our predictor and we should get a\n", "response from the endpoint that contains the most likely answers."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["data = {'question': 'what is my name?', 'context': \"my name is thom\"}\n", "response = predictor.predict(data=data)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["We have the responce and we can print out the most likely answers that\n", "has been extracted from the text above. You'll see each answer has a\n", "confidence score used for ranking (but this score shouldn't be\n", "interpreted as a true probability). In addition to the verbatim answer,\n", "you also get the start and end character indexes of the answer from the\n", "original context."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["print(response['answers'])"]}, {"cell_type": "markdown", "metadata": {}, "source": ["You can try more examples above, but note that this model has been\n", "pretrained on the SQuAD dataset. You may need to fine-tune this model\n", "with your own question answering data to obtain better results."]}, {"cell_type": "markdown", "metadata": {}, "source": ["## Clean Up\n", "\n", "When you've finished with the summarization endpoint (and associated\n", "endpoint-config), make sure that you delete it to avoid accidental\n", "charges."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["sagemaker_client.delete_endpoint(EndpointName=model_name)\n", "sagemaker_client.delete_endpoint_config(EndpointConfigName=model_name)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## Next Stage\n", "\n", "We've just looked at how you can query document for specific information.\n", "Up next we'll look at a technique that can be used to extract the key\n", "entities from a document, called Entity Recognition.\n", "\n", "[Click here to continue.](./3_entity_recognition.ipynb)"]}], "metadata": {"jupytext": {"cell_metadata_filter": "-all", "main_language": "python", "notebook_metadata_filter": "-all"}, "kernelspec": {"display_name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:us-east-1:081325390199:image/1.8.1-cpu-py36", "language": "python", "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:us-east-1:081325390199:image/1.8.1-cpu-py36"}}, "nbformat": 4, "nbformat_minor": 4}